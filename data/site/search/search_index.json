{"config":{"lang":["fr"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Formation Deep Learning pour BTS SIO","text":""},{"location":"#bienvenue-dans-ce-parcours-dapprentissage","title":"\ud83d\ude80 Bienvenue dans ce parcours d'apprentissage","text":"<p>Cette formation intensive vous initie au Deep Learning \u00e0 travers une approche pratique et progressive, sp\u00e9cialement con\u00e7ue pour les \u00e9tudiants de BTS SIO. Vous d\u00e9couvrirez les fondamentaux des r\u00e9seaux de neurones, explorerez diff\u00e9rentes architectures sp\u00e9cialis\u00e9es, et d\u00e9velopperez un chatbot p\u00e9dagogique int\u00e9grant l'API Mistral AI.</p>"},{"location":"#quest-ce-que-le-deep-learning","title":"\ud83e\udde0 Qu'est-ce que le Deep Learning ?","text":"<p>Le Deep Learning est une branche du Machine Learning qui utilise des r\u00e9seaux de neurones \u00e0 multiples couches pour extraire automatiquement des caract\u00e9ristiques hi\u00e9rarchiques \u00e0 partir des donn\u00e9es. Contrairement au Machine Learning classique qui n\u00e9cessite une extraction manuelle des caract\u00e9ristiques, le Deep Learning automatise ce processus, le rendant particuli\u00e8rement efficace pour des t\u00e2ches complexes comme la vision par ordinateur et le traitement du langage naturel.</p> <p></p> <ul> <li> <p>Apr\u00e8s avoir explor\u00e9 le paysage de l'Intelligence Artificielle et situ\u00e9 le Deep Learning dans cet \u00e9cosyst\u00e8me, examinons maintenant son impact sur les diff\u00e9rentes m\u00e9thodes d'apprentissage automatique. Contrairement \u00e0 ce qu'on pourrait penser, le Deep Learning n'est pas un type d'apprentissage isol\u00e9, mais une approche r\u00e9volutionnaire qui s'applique aux trois paradigmes fondamentaux : l'apprentissage supervis\u00e9, non supervis\u00e9 et par renforcement. </p> </li> <li> <p>Comme l'illustre le sch\u00e9ma suivant, le Deep Learning agit comme un amplificateur qui transforme radicalement les capacit\u00e9s de chacune de ces m\u00e9thodes, permettant de r\u00e9soudre des probl\u00e8mes jusqu'alors hors de port\u00e9e des approches traditionnelles.</p> </li> </ul> <p></p>"},{"location":"#organisation-du-parcours","title":"\ud83d\udcda Organisation du parcours","text":""},{"location":"#prerequis-techniques","title":"\ud83d\udee0\ufe0f Pr\u00e9requis techniques","text":"<p>Pour suivre efficacement cette formation, vous devez :</p> <ul> <li>Poss\u00e9der des bases en programmation Python</li> <li>Disposer d'un compte Google pour acc\u00e9der \u00e0 Google Colab</li> <li>Avoir une curiosit\u00e9 pour l'intelligence artificielle</li> </ul>"},{"location":"#documentation-de-reference","title":"\ud83d\udccc Documentation de r\u00e9f\u00e9rence","text":"<ul> <li>Glossaire du Deep Learning - Les termes essentiels expliqu\u00e9s simplement</li> <li>Comp\u00e9tences BTS SIO d\u00e9velopp\u00e9es - Consultez les comp\u00e9tences professionnelles vis\u00e9es</li> </ul>"},{"location":"#commencer-votre-parcours","title":"\ud83d\ude80 Commencer votre parcours","text":"<p>Pr\u00eat \u00e0 vous lancer dans l'univers du Deep Learning ? Deux options s'offrent \u00e0 vous :</p> <p>\ud83e\udd16 D\u00e9couvrir le projet chatbot \ud83c\udfc1 Commencer le Module 1</p>"},{"location":"carte-progression/","title":"Carte de progression","text":""},{"location":"carte-progression/#gps-pedagogique-votre-itineraire-dapprentissage-du-deep-learning","title":"GPS p\u00e9dagogique : votre itin\u00e9raire d'apprentissage du Deep Learning","text":"<p>Cette carte de progression vous permettra de visualiser clairement les objectifs, les activit\u00e9s et les comp\u00e9tences d\u00e9velopp\u00e9es \u00e0 chaque \u00e9tape de votre formation en Deep Learning.</p>"},{"location":"carte-progression/#les-4-modules-du-parcours","title":"Les 4 modules du parcours","text":"<pre><code>flowchart LR\n    M1[Module 1  Fondamentaux du DL] --&gt;  M2[Module 2 Architectures sp\u00e9cialis\u00e9es]\n    M2[Module 2  Architectures sp\u00e9cialis\u00e9es] --&gt; M3[Module 3 D\u00e9veloppement d'applications]\n    M3[Module 3  D\u00e9veloppement d'applications] --&gt; M4[Module 4 Projet chatbot p\u00e9dagogique]\n</code></pre>"},{"location":"carte-progression/#module-1-fondamentaux-du-deep-learning","title":"Module 1 : Fondamentaux du Deep Learning","text":"<p>Concepts cl\u00e9s : - Structure et fonctionnement d'un neurone artificiel - R\u00e9seaux de neurones multicouches - Forward et backpropagation - Fonctions d'activation (ReLU, Sigmoid, Softmax) - Diff\u00e9rences fondamentales entre Machine Learning classique et Deep Learning</p> <p>Activit\u00e9s pratiques : - Manipulation d'un r\u00e9seau de neurones sur donn\u00e9es MNIST - Comparaison directe ML vs DL sur le m\u00eame jeu de donn\u00e9es - Visualisation des couches internes d'un r\u00e9seau</p> <p>Auto-\u00e9valuation : - QCM sur les concepts fondamentaux - Sch\u00e9ma conceptuel \u00e0 compl\u00e9ter - Analyse critique des r\u00e9sultats obtenus</p> <p>Livrables : - Notebook \"Hello World du Deep Learning\" compl\u00e9t\u00e9 - Sch\u00e9ma annot\u00e9 d'un r\u00e9seau de neurones</p>"},{"location":"carte-progression/#module-2-architectures-specialisees","title":"Module 2 : Architectures sp\u00e9cialis\u00e9es","text":"<p>Concepts cl\u00e9s pour les CNN : - Convolution et filtres - Pooling et r\u00e9duction de dimension - Feature maps et leur interpr\u00e9tation - Transfer learning avec mod\u00e8les pr\u00e9-entra\u00een\u00e9s</p> <p>Concepts cl\u00e9s pour les RNN : - Traitement de s\u00e9quences et donn\u00e9es temporelles - Probl\u00e8me de la disparition du gradient - Cellules LSTM et GRU - Applications au traitement du langage naturel</p> <p>Auto-\u00e9valuation : - QCM sur les architectures CNN et RNN - Analyse de performance des mod\u00e8les - Questions \u00e0 r\u00e9ponse courte sur l'int\u00e9gration pratique</p> <p>Activit\u00e9s pratiques : - Impl\u00e9mentation d'un CNN pour la classification d'images - D\u00e9veloppement d'un RNN pour l'analyse de sentiment - Optimisation d'un mod\u00e8le de pr\u00e9vision des ventes</p>"},{"location":"carte-progression/#module-3-developpement-dapplications-pratiques","title":"Module 3 : D\u00e9veloppement d'applications pratiques","text":"<p>Concepts cl\u00e9s : - TensorFlow/Keras : mod\u00e8les fonctionnels et s\u00e9quentiels - Optimisation des hyperparam\u00e8tres - Techniques de r\u00e9gularisation (dropout, batch normalization) - API REST pour servir des mod\u00e8les - Int\u00e9gration de mod\u00e8les de langage (API Mistral)</p> <p>Auto-\u00e9valuation : - QCM sur les frameworks et l'optimisation - Exercice pratique de d\u00e9veloppement d'API - Cas concret d'int\u00e9gration de mod\u00e8les</p> <p>Activit\u00e9s pratiques : - Utilisation de mod\u00e8les pr\u00e9-entra\u00een\u00e9s - Optimisation des performances d'inf\u00e9rence - Premier test d'int\u00e9gration avec l'API Mistral - Conception du prototype de chatbot</p>"},{"location":"carte-progression/#module-4-projet-integrateur-chatbot-pedagogique","title":"Module 4 : Projet int\u00e9grateur - Chatbot p\u00e9dagogique","text":"<p>Concepts cl\u00e9s : - Prompt engineering pour mod\u00e8les de langage - Gestion du contexte conversationnel - Structures de donn\u00e9es pour bases de connaissances - Optimisation de l'exp\u00e9rience utilisateur - Techniques de d\u00e9ploiement</p> <p>Auto-\u00e9valuation : - QCM sur la conception et l'architecture du chatbot - Exercice pratique sur la gestion du contexte - Analyse de performance des optimisations</p> <p>Activit\u00e9s pratiques : - D\u00e9veloppement d'une interface conversationnelle - Int\u00e9gration avanc\u00e9e avec l'API Mistral - Structuration d'une base de connaissances - Tests et optimisation de l'exp\u00e9rience utilisateur</p>"},{"location":"carte-progression/#ce-que-vous-saurez-faire-apres-chaque-module","title":"Ce que vous saurez faire apr\u00e8s chaque module","text":""},{"location":"carte-progression/#apres-le-module-1","title":"Apr\u00e8s le Module 1","text":"<ul> <li>Expliquer le fonctionnement d'un r\u00e9seau de neurones de base</li> <li>Distinguer ML classique et Deep Learning dans des cas concrets</li> <li>Impl\u00e9menter un r\u00e9seau simple pour la classification d'images</li> <li>Interpr\u00e9ter les m\u00e9triques d'entra\u00eenement (pr\u00e9cision, perte)</li> </ul>"},{"location":"carte-progression/#apres-le-module-2","title":"Apr\u00e8s le Module 2","text":"<ul> <li>Impl\u00e9menter et adapter un CNN pour la vision par ordinateur</li> <li>D\u00e9velopper un RNN pour des t\u00e2ches de traitement de texte</li> <li>Visualiser et interpr\u00e9ter les feature maps d'un CNN</li> <li>Am\u00e9liorer un mod\u00e8le existant avec diff\u00e9rentes techniques</li> </ul>"},{"location":"carte-progression/#apres-le-module-3","title":"Apr\u00e8s le Module 3","text":"<ul> <li>Utiliser efficacement TensorFlow/Keras pour cr\u00e9er des mod\u00e8les</li> <li>Appliquer des techniques d'optimisation des performances</li> <li>Int\u00e9grer l'API Mistral dans une application simple</li> <li>Concevoir l'architecture d'un chatbot p\u00e9dagogique</li> </ul>"},{"location":"carte-progression/#apres-le-module-4","title":"Apr\u00e8s le Module 4","text":"<ul> <li>D\u00e9velopper un chatbot p\u00e9dagogique complet et fonctionnel</li> <li>Cr\u00e9er et g\u00e9rer une base de connaissances structur\u00e9e</li> <li>Optimiser l'exp\u00e9rience utilisateur d'un syst\u00e8me conversationnel</li> <li>Pr\u00e9senter et d\u00e9fendre un projet technique</li> </ul>"},{"location":"carte-progression/#concepts-cles-du-deep-learning-a-travers-le-parcours","title":"Concepts cl\u00e9s du Deep Learning \u00e0 travers le parcours","text":"<ul> <li>Neurones artificiels et r\u00e9seaux \u2192 Module 1</li> <li>Descente de gradient et r\u00e9tropropagation \u2192 Module 1</li> <li>Convolution et vision par ordinateur \u2192 Module 2</li> <li>M\u00e9moire r\u00e9currente et s\u00e9quences \u2192 Module 2</li> <li>Optimisation et hyperparam\u00e8tres \u2192 Module 3</li> <li>Mod\u00e8les de langage et g\u00e9n\u00e9ration de texte \u2192 Module 3, 4</li> <li>Syst\u00e8mes conversationnels \u2192 Module 4</li> <li>Architectures d'applications IA \u2192 Module 4</li> </ul>"},{"location":"carte-progression/#ressources-essentielles","title":"Ressources essentielles","text":"<ul> <li>Documentation TensorFlow/Keras - tensorflow.org/tutorials</li> <li>API Mistral - docs.mistral.ai</li> <li>Hugging Face - huggingface.co/docs</li> <li>FastAPI - fastapi.tiangolo.com</li> </ul>"},{"location":"carte-progression/#auto-evaluation-et-progression","title":"Auto-\u00e9valuation et progression","text":"<p>Pour suivre efficacement votre progression : - Compl\u00e9tez chaque QCM \u00e0 la fin du module correspondant - Analysez vos r\u00e9sultats pour identifier vos points forts et points \u00e0 am\u00e9liorer - R\u00e9visez les concepts pour lesquels vous avez obtenu un score inf\u00e9rieur \u00e0 70% - N'h\u00e9sitez pas \u00e0 refaire les QCM apr\u00e8s avoir approfondi les sujets concern\u00e9</p> <p>Retour \u00e0 l'accueil Commencer le Module 1</p>"},{"location":"feedback/","title":"Formulaire de feedback - QCM d'auto-\u00e9valuation","text":"<p>Ce formulaire vous permet de nous faire part de vos commentaires sur les QCM d'auto-\u00e9valuation. Vos retours nous aideront \u00e0 am\u00e9liorer la qualit\u00e9 et la pertinence des questions.</p>"},{"location":"feedback/#instructions","title":"Instructions","text":"<ul> <li>Remplissez ce formulaire apr\u00e8s avoir compl\u00e9t\u00e9 un QCM d'auto-\u00e9valuation</li> <li>Soyez aussi pr\u00e9cis que possible dans vos commentaires</li> <li>Envoyez le formulaire compl\u00e9t\u00e9 \u00e0 l'adresse email indiqu\u00e9e ci-dessous</li> </ul>"},{"location":"feedback/#identification-du-qcm","title":"Identification du QCM","text":"<p>Module concern\u00e9 : - [ ] Module 1 : Fondamentaux du Deep Learning - [ ] Module 2 : Architectures sp\u00e9cialis\u00e9es (CNN et RNN) - [ ] Module 3 : D\u00e9veloppement d'applications pratiques - [ ] Module 4 : Projet int\u00e9grateur - Chatbot p\u00e9dagogique</p> <p>Num\u00e9ro ou titre de la question concern\u00e9e (si sp\u00e9cifique) : _______</p>"},{"location":"feedback/#type-de-feedback","title":"Type de feedback","text":"<p>Nature du feedback : - [ ] Erreur factuelle - [ ] Probl\u00e8me de formulation - [ ] Ambigu\u00eft\u00e9 dans la question ou les r\u00e9ponses - [ ] Difficult\u00e9 inadapt\u00e9e (trop facile ou trop difficile) - [ ] Probl\u00e8me technique dans l'exercice pratique - [ ] Suggestion d'am\u00e9lioration - [ ] Autre : _______</p>"},{"location":"feedback/#description-detaillee","title":"Description d\u00e9taill\u00e9e","text":"<p>D\u00e9crivez pr\u00e9cis\u00e9ment le probl\u00e8me rencontr\u00e9 ou votre suggestion : <pre><code>[Votre description ici]\n</code></pre></p> <p>Si applicable, quelle serait votre proposition d'am\u00e9lioration ? <pre><code>[Votre proposition ici]\n</code></pre></p>"},{"location":"feedback/#evaluation-globale-du-qcm","title":"\u00c9valuation globale du QCM","text":"<p>Difficult\u00e9 g\u00e9n\u00e9rale du QCM : - [ ] Trop facile - [ ] Adapt\u00e9 - [ ] Trop difficile</p> <p>Pertinence par rapport au contenu du module : - [ ] Peu pertinent - [ ] Moyennement pertinent - [ ] Tr\u00e8s pertinent</p> <p>Utilit\u00e9 pour votre apprentissage : - [ ] Peu utile - [ ] Moyennement utile - [ ] Tr\u00e8s utile</p> <p>Score obtenu au QCM : _ / _</p>"},{"location":"feedback/#commentaires-additionnels","title":"Commentaires additionnels","text":"<pre><code>[Vos commentaires additionnels ici]\n</code></pre> <p>Merci pour votre contribution \u00e0 l'am\u00e9lioration de notre mat\u00e9riel p\u00e9dagogique !</p>"},{"location":"suivi-progression/","title":"Suivi progression","text":""},{"location":"suivi-progression/#tableau-de-bord","title":"Tableau de bord","text":"<p>Ce tableau de bord vous permet de suivre votre progression \u00e0 travers les diff\u00e9rents modules et activit\u00e9s de la formation Deep Learning. Cochez les cases au fur et \u00e0 mesure que vous compl\u00e9tez chaque partie.</p>"},{"location":"suivi-progression/#module-1-fondamentaux-du-deep-learning","title":"Module 1 : Fondamentaux du Deep Learning","text":"Section Activit\u00e9 Statut Date de compl\u00e9tion Introduction pratique D\u00e9monstrations d'applications \u2b1c Premier contact avec un r\u00e9seau de neurones \u2b1c Exp\u00e9rimentations guid\u00e9es \u2b1c Concepts fondamentaux Atelier \"Bo\u00eete noire\" \u2b1c D\u00e9fi de g\u00e9n\u00e9ralisation \u2b1c Exploration d'un neurone et d'un r\u00e9seau \u2b1c Mini-projet individuel Modification et am\u00e9lioration d'un r\u00e9seau \u2b1c Documentation des r\u00e9sultats \u2b1c Auto-\u00e9valuation QCM sur les concepts fondamentaux \u2b1c Sch\u00e9ma conceptuel compl\u00e9t\u00e9 \u2b1c"},{"location":"suivi-progression/#module-2-architectures-specialisees","title":"Module 2 : Architectures sp\u00e9cialis\u00e9es","text":"Section Activit\u00e9 Statut Date de compl\u00e9tion R\u00e9seaux convolutifs (CNN) Principes des CNN \u2b1c Impl\u00e9mentation d'un CNN pour MNIST \u2b1c Visualisation des filtres et feature maps \u2b1c Int\u00e9gration dans une application web \u2b1c R\u00e9seaux r\u00e9currents (RNN) Principes des RNN/LSTM \u2b1c Impl\u00e9mentation d'un mod\u00e8le d'analyse de sentiment \u2b1c Exp\u00e9rimentation avec l'API Mistral AI \u2b1c Challenge d'am\u00e9lioration Diagnostic d'un mod\u00e8le sous-optimal \u2b1c Exp\u00e9rimentation avec diff\u00e9rentes architectures \u2b1c Documentation des am\u00e9liorations \u2b1c Auto-\u00e9valuation QCM sur les architectures sp\u00e9cialis\u00e9es \u2b1c Analyse critique des performances \u2b1c"},{"location":"suivi-progression/#module-3-developpement-dapplications-pratiques","title":"Module 3 : D\u00e9veloppement d'applications pratiques","text":"Section Activit\u00e9 Statut Date de compl\u00e9tion Frameworks pour d\u00e9butants Installation et configuration de TensorFlow/Keras \u2b1c Utilisation de mod\u00e8les pr\u00e9-entra\u00een\u00e9s \u2b1c D\u00e9veloppement d'une API simple \u2b1c Am\u00e9lioration des performances Techniques d'optimisation \u2b1c Bonnes pratiques \u2b1c TP pratique d'am\u00e9lioration \u2b1c Pr\u00e9paration au projet final \u00c9tude du cahier des charges \u2b1c Analyse de cas r\u00e9els \u2b1c Prototype avec API Mistral \u2b1c Auto-\u00e9valuation QCM sur les frameworks et l'optimisation \u2b1c Exercice pratique d'int\u00e9gration \u2b1c Cas concret d'application \u2b1c"},{"location":"suivi-progression/#module-4-projet-integrateur-chatbot-pedagogique","title":"Module 4 : Projet int\u00e9grateur - Chatbot p\u00e9dagogique","text":"Section Activit\u00e9 Statut Date de compl\u00e9tion D\u00e9veloppement du chatbot Interface conversationnelle \u2b1c Int\u00e9gration avec API Mistral AI \u2b1c Base de connaissances \u2b1c Fonctionnalit\u00e9s p\u00e9dagogiques \u2b1c Finalisation et tests Tests fonctionnels \u2b1c Optimisation des performances \u2b1c Documentation technique \u2b1c Guide utilisateur \u2b1c Pr\u00e9sentation Pr\u00e9paration de la d\u00e9monstration \u2b1c Pr\u00e9sentation finale \u2b1c Auto-\u00e9valuation QCM sur le d\u00e9veloppement de chatbots \u2b1c Exercice pratique de gestion de contexte \u2b1c Analyse de performance \u2b1c"},{"location":"suivi-progression/#livrables-soumis","title":"Livrables soumis","text":"Livrable Module Statut Date de soumission Note Fiche d'observations \"Hello World\" 1 \u2b1c Tableau comparatif ML vs DL 1 \u2b1c Sch\u00e9ma annot\u00e9 d'un r\u00e9seau de neurones 1 \u2b1c QCM d'auto-\u00e9valuation Module 1 1 \u2b1c Rapport du mini-projet 1 \u2b1c Application CNN fonctionnelle 2 \u2b1c Mod\u00e8le RNN pour analyse de sentiment 2 \u2b1c QCM d'auto-\u00e9valuation Module 2 2 \u2b1c Rapport d'analyse comparative 2 \u2b1c Mod\u00e8le optimis\u00e9 et documentation 3 \u2b1c QCM d'auto-\u00e9valuation Module 3 3 \u2b1c Document de conception du chatbot 3 \u2b1c Code source du chatbot 4 \u2b1c Base de connaissances 4 \u2b1c Documentation technique 4 \u2b1c Guide utilisateur 4 \u2b1c QCM d'auto-\u00e9valuation Module 4 4 \u2b1c Pr\u00e9sentation finale 4 \u2b1c"},{"location":"suivi-progression/#graphique-de-progression","title":"Graphique de progression","text":"<p>Pour visualiser votre progression globale, calculez le pourcentage d'activit\u00e9s compl\u00e9t\u00e9es pour chaque module :</p> <ul> <li>Module 1 : _ / 10 activit\u00e9s compl\u00e9t\u00e9es (_%)</li> <li>Module 2 : _ / 11 activit\u00e9s compl\u00e9t\u00e9es (_%)</li> <li>Module 3 : _ / 12 activit\u00e9s compl\u00e9t\u00e9es (_%)</li> <li>Module 4 : _ / 13 activit\u00e9s compl\u00e9t\u00e9es (_%)</li> </ul> <p>Progression globale : _ / 46 activit\u00e9s compl\u00e9t\u00e9es (_%)</p>"},{"location":"suivi-progression/#suivi-des-qcm-dauto-evaluation","title":"Suivi des QCM d'auto-\u00e9valuation","text":"Module Score obtenu Score maximum Pourcentage Date Module 1 15 Module 2 16 Module 3 40 Module 4 40"},{"location":"suivi-progression/#instructions-dutilisation","title":"Instructions d'utilisation","text":"<ol> <li>T\u00e9l\u00e9chargez ou imprimez cette page pour votre suivi personnel</li> <li>Cochez les cases (remplacez \u2b1c par \u2705) au fur et \u00e0 mesure de votre progression</li> <li>Notez la date de compl\u00e9tion pour chaque activit\u00e9</li> <li>Calculez r\u00e9guli\u00e8rement votre pourcentage de progression</li> <li>Partagez votre progression avec votre formateur lors des points d'\u00e9tape</li> </ol>"},{"location":"suivi-progression/#notes-personnelles-et-reflexions","title":"Notes personnelles et r\u00e9flexions","text":"<p>Utilisez cet espace pour noter vos observations, difficult\u00e9s rencontr\u00e9es et points forts identifi\u00e9s au cours de votre formation.</p> <p>Retour \u00e0 l'accueil</p>"},{"location":"evaluation/","title":"\u00c9valuation du parcours Deep Learning","text":""},{"location":"evaluation/#presentation-du-systeme-devaluation","title":"Pr\u00e9sentation du syst\u00e8me d'\u00e9valuation","text":"<p>Cette section d\u00e9taille les modalit\u00e9s d'\u00e9valuation du parcours sur le Deep Learning et du projet de chatbot p\u00e9dagogique. Elle vous permettra de comprendre clairement les attentes, les crit\u00e8res d'\u00e9valuation et les livrables requis.</p>"},{"location":"evaluation/#objectifs-de-levaluation","title":"Objectifs de l'\u00e9valuation","text":"<p>L'\u00e9valuation de ce parcours vise plusieurs objectifs :</p> <ol> <li>Mesurer votre compr\u00e9hension des concepts fondamentaux du Deep Learning</li> <li>\u00c9valuer votre capacit\u00e9 \u00e0 appliquer ces concepts dans un projet concret</li> <li>Valoriser le travail d'\u00e9quipe et la r\u00e9partition efficace des t\u00e2ches</li> <li>Pr\u00e9parer aux situations professionnelles en simulant un projet r\u00e9el d'entreprise</li> <li>Fournir un feedback constructif pour votre progression personnelle</li> </ol>"},{"location":"evaluation/#repartition-globale-de-levaluation","title":"R\u00e9partition globale de l'\u00e9valuation","text":"<p>L'\u00e9valuation globale du parcours se d\u00e9compose comme suit :</p> Composante Pond\u00e9ration Description Participation active 10% Engagement dans les activit\u00e9s, pertinence des contributions Mini-projets 30% Qualit\u00e9 des livrables des s\u00e9ances 2 et 3 Projet final - Produit 30% Fonctionnalit\u00e9 et qualit\u00e9 technique du chatbot Projet final - Processus 15% Organisation, m\u00e9thodologie, r\u00e9partition des t\u00e2ches Projet final - Pr\u00e9sentation 15% Qualit\u00e9 de la pr\u00e9sentation et de la documentation"},{"location":"evaluation/#documents-devaluation-disponibles","title":"Documents d'\u00e9valuation disponibles","text":""},{"location":"evaluation/#criteres-devaluation-detailles","title":"Crit\u00e8res d'\u00e9valuation d\u00e9taill\u00e9s","text":"<p>Ce document pr\u00e9sente en d\u00e9tail tous les crit\u00e8res utilis\u00e9s pour l'\u00e9valuation de chaque composante du parcours :      - Description pr\u00e9cise de chaque crit\u00e8re      - Bar\u00e8me de notation et pond\u00e9ration      - Exemples de livrables attendus pour chaque niveau de performance      - Conseils pour maximiser votre score</p>"},{"location":"evaluation/#grille-de-repartition-des-taches","title":"Grille de r\u00e9partition des t\u00e2ches","text":"<p>Cette grille vous aide \u00e0 organiser efficacement le travail au sein de votre \u00e9quipe :</p> <pre><code> - Identification des r\u00f4les et responsabilit\u00e9s\n - Planning des jalons interm\u00e9diaires\n - Suivi de l'avancement des t\u00e2ches\n - Gestion des risques et plan de contingence\n</code></pre>"},{"location":"evaluation/#fiche-devaluation-finale","title":"Fiche d'\u00e9valuation finale","text":"<p>La fiche utilis\u00e9e lors de l'\u00e9valuation finale du projet de chatbot p\u00e9dagogique :</p> <pre><code> - Crit\u00e8res sp\u00e9cifiques pour chaque aspect du projet\n - Bar\u00e8me de notation d\u00e9taill\u00e9\n - Espace pour les commentaires et feedback\n - Auto-\u00e9valuation pr\u00e9alable \u00e0 remplir par l'\u00e9quipe\n</code></pre>"},{"location":"evaluation/#checklist-dauto-evaluation","title":"Checklist d'auto-\u00e9valuation","text":"<p>Cette checklist vous permet de v\u00e9rifier que votre projet r\u00e9pond \u00e0 tous les crit\u00e8res avant la soumission finale:</p> <ul> <li>Liste compl\u00e8te des fonctionnalit\u00e9s \u00e0 impl\u00e9menter</li> <li>Points techniques \u00e0 v\u00e9rifier</li> <li>Aspects de documentation \u00e0 ne pas oublier</li> <li>Conseils pour la pr\u00e9sentation finale</li> </ul>"},{"location":"evaluation/#mini-projets-evalues","title":"Mini-projets \u00e9valu\u00e9s","text":"<p>Les mini-projets des s\u00e9ances 2 et 3 font partie int\u00e9grante de l'\u00e9valuation continue :</p>"},{"location":"evaluation/#mini-projet-cnn-10","title":"Mini-projet CNN (10%)","text":"<pre><code> - Impl\u00e9mentation d'un r\u00e9seau convolutif pour la classification d'images\n - Visualisation et interpr\u00e9tation des filtres et feature maps\n - Int\u00e9gration dans une application web simple\n - \u00c9valuation des performances sur diff\u00e9rents jeux de donn\u00e9es\n</code></pre>"},{"location":"evaluation/#mini-projet-rnn-10","title":"Mini-projet RNN (10%)","text":"<pre><code> - Impl\u00e9mentation d'un mod\u00e8le LSTM pour l'analyse de sentiment\n - Exp\u00e9rimentation avec l'API Mistral AI pour le traitement du langage\n - Comparaison entre approche par r\u00e9seau de neurones et API de mod\u00e8le de langage\n - Documentation des r\u00e9sultats et limitations\n</code></pre>"},{"location":"evaluation/#projet-damelioration-10","title":"Projet d'am\u00e9lioration (10%)","text":"<pre><code> - Diagnostic des probl\u00e8mes d'un mod\u00e8le de pr\u00e9vision des ventes sous-optimal\n - Application de techniques d'am\u00e9lioration cibl\u00e9es\n - Mesure et analyse comparative des performances\n - Documentation des exp\u00e9rimentations et conclusions\n</code></pre>"},{"location":"evaluation/#evaluation-du-projet-final","title":"\u00c9valuation du projet final","text":"<p>Le projet final de chatbot p\u00e9dagogique constitue la partie la plus importante de l'\u00e9valuation (60% au total) :</p>"},{"location":"evaluation/#produit-final-30","title":"Produit final (30%)","text":"<pre><code> - Interface conversationnelle fonctionnelle et intuitive\n - Int\u00e9gration avanc\u00e9e avec l'API Mistral AI\n - Base de connaissances compl\u00e8te et structur\u00e9e sur le Deep Learning\n - Fonctionnalit\u00e9s p\u00e9dagogiques (explications, exemples, exercices)\n - Performances techniques (temps de r\u00e9ponse, gestion des erreurs)\n</code></pre>"},{"location":"evaluation/#processus-de-developpement-15","title":"Processus de d\u00e9veloppement (15%)","text":"<pre><code> - Organisation de l'\u00e9quipe et r\u00e9partition des t\u00e2ches\n - Respect des jalons interm\u00e9diaires\n - Tests fonctionnels et validation des performances\n - Adaptation aux difficult\u00e9s techniques rencontr\u00e9es\n</code></pre>"},{"location":"evaluation/#presentation-et-documentation-15","title":"Pr\u00e9sentation et documentation (15%)","text":"<pre><code> - Pr\u00e9sentation claire et d\u00e9monstration convaincante\n - Documentation technique d\u00e9taill\u00e9e\n - Guide utilisateur complet\n - Explication des choix techniques et architecture\n</code></pre>"},{"location":"evaluation/#calendrier-devaluation","title":"Calendrier d'\u00e9valuation","text":"S\u00e9ance \u00c9valuation Livrables attendus S\u00e9ance 2 Mini-projets CNN et RNN Mod\u00e8les fonctionnels et rapports d'analyse S\u00e9ance 3 Projet d'am\u00e9lioration Mod\u00e8le optimis\u00e9 et documentation S\u00e9ance 3 Pr\u00e9paration du projet Document de conception du chatbot S\u00e9ance 4 D\u00e9veloppement Chatbot fonctionnel et documentation S\u00e9ance 4 Pr\u00e9sentation finale D\u00e9monstration et d\u00e9fense du projet"},{"location":"evaluation/#conseils-pour-reussir","title":"Conseils pour r\u00e9ussir","text":"<ol> <li>Commencez par l'essentiel - Assurez-vous que les fonctionnalit\u00e9s de base sont solides avant d'ajouter des \u00e9l\u00e9ments avanc\u00e9s</li> <li>Documentez au fur et \u00e0 mesure - Ne laissez pas la documentation pour la fin</li> <li>Testez r\u00e9guli\u00e8rement - Identifiez et corrigez les probl\u00e8mes t\u00f4t</li> <li>R\u00e9partissez \u00e9quitablement les t\u00e2ches - Utilisez la grille de r\u00e9partition pour organiser le travail</li> <li>Pr\u00e9parez soigneusement votre d\u00e9monstration - Pr\u00e9voyez un sc\u00e9nario qui met en valeur les points forts de votre solution</li> <li>G\u00e9rez efficacement le temps - Respectez les jalons interm\u00e9diaires pour \u00e9viter le stress de derni\u00e8re minute</li> <li>Communiquez avec l'enseignant - Demandez de l'aide si vous rencontrez des difficult\u00e9s</li> </ol>"},{"location":"evaluation/#auto-evaluation","title":"Auto-\u00e9valuation","text":"<p>Avant chaque remise, nous vous encourageons \u00e0 r\u00e9aliser une auto-\u00e9valuation \u00e0 l'aide des grilles fournies. Cette pratique vous permettra de :</p> <pre><code> - Identifier les points forts et axes d'am\u00e9lioration de votre travail\n - V\u00e9rifier que tous les crit\u00e8res sont bien pris en compte\n - Prioriser les aspects \u00e0 finaliser ou am\u00e9liorer\n - Pr\u00e9parer votre argumentaire pour la pr\u00e9sentation\n</code></pre>"},{"location":"evaluation/#ressources-supplementaires","title":"Ressources suppl\u00e9mentaires","text":"<ul> <li>Guide de bonnes pratiques pour la documentation technique</li> <li>Conseils pour une pr\u00e9sentation efficace</li> <li>Comp\u00e9tences recherch\u00e9es en stage BTS SIO</li> </ul> <p>L'\u00e9valuation est con\u00e7ue comme un outil p\u00e9dagogique pour vous guider et vous motiver tout au long de ce parcours d'apprentissage. Gardez \u00e0 l'esprit que l'objectif principal est d'acqu\u00e9rir des comp\u00e9tences pratiques en Deep Learning que vous pourrez valoriser dans votre parcours professionnel.</p> <p>[</p>"},{"location":"evaluation/checklist-auto-evaluation/","title":"Checklist d'auto-\u00e9valuation simplifi\u00e9e","text":"<p>Cette checklist vous aidera \u00e0 \u00e9valuer votre travail sur le projet de chatbot p\u00e9dagogique. Elle est organis\u00e9e par fonctionnalit\u00e9s essentielles pour vous permettre de v\u00e9rifier votre progression.</p>"},{"location":"evaluation/checklist-auto-evaluation/#interface-utilisateur","title":"Interface utilisateur","text":""},{"location":"evaluation/checklist-auto-evaluation/#interface-conversationnelle","title":"Interface conversationnelle","text":"<ul> <li> Zone de saisie et d'affichage des messages</li> <li> Indication visuelle lors du chargement des r\u00e9ponses</li> <li> Affichage clair des \u00e9changes (messages utilisateur vs assistant)</li> <li> Interface responsive (s'adapte aux diff\u00e9rentes tailles d'\u00e9cran)</li> </ul>"},{"location":"evaluation/checklist-auto-evaluation/#fonctionnalites-du-chatbot","title":"Fonctionnalit\u00e9s du chatbot","text":""},{"location":"evaluation/checklist-auto-evaluation/#integration-avec-lapi-mistral","title":"Int\u00e9gration avec l'API Mistral","text":"<ul> <li> Configuration correcte de l'API Mistral</li> <li> Gestion du contexte de conversation</li> <li> Traitement correct des r\u00e9ponses de l'API</li> <li> Gestion des erreurs (perte de connexion, limites de l'API)</li> </ul>"},{"location":"evaluation/checklist-auto-evaluation/#base-de-connaissances","title":"Base de connaissances","text":"<ul> <li> Structure pour stocker les concepts de Deep Learning</li> <li> D\u00e9finitions des concepts principaux du cours</li> <li> Exemples pour illustrer les concepts</li> <li> R\u00e9f\u00e9rences aux modules du cours</li> </ul>"},{"location":"evaluation/checklist-auto-evaluation/#qualite-pedagogique","title":"Qualit\u00e9 p\u00e9dagogique","text":"<ul> <li> Les explications sont claires et adapt\u00e9es</li> <li> Le chatbot propose des exemples pertinents</li> <li> Les r\u00e9ponses sont structur\u00e9es logiquement</li> <li> Le chatbot peut adapter son niveau d'explication</li> </ul>"},{"location":"evaluation/checklist-auto-evaluation/#documentation","title":"Documentation","text":""},{"location":"evaluation/checklist-auto-evaluation/#guide-utilisateur","title":"Guide utilisateur","text":"<ul> <li> Instructions d'installation</li> <li> Guide d'utilisation du chatbot</li> <li> Exemples de questions \u00e0 poser</li> <li> Limites connues du syst\u00e8me</li> </ul>"},{"location":"evaluation/checklist-auto-evaluation/#documentation-technique","title":"Documentation technique","text":"<ul> <li> Structure du code et de l'application</li> <li> Description de l'architecture</li> <li> Explication des choix techniques</li> <li> Instructions pour les d\u00e9veloppeurs</li> </ul>"},{"location":"evaluation/checklist-auto-evaluation/#tests","title":"Tests","text":""},{"location":"evaluation/checklist-auto-evaluation/#verification-des-fonctionnalites","title":"V\u00e9rification des fonctionnalit\u00e9s","text":"<ul> <li> Le chatbot r\u00e9pond correctement aux questions sur le Deep Learning</li> <li> Les messages s'affichent sans erreur</li> <li> L'application fonctionne sur diff\u00e9rents navigateurs</li> <li> Les longues conversations sont g\u00e9r\u00e9es correctement</li> </ul>"},{"location":"evaluation/checklist-auto-evaluation/#scenarios-de-test","title":"Sc\u00e9narios de test","text":"<ul> <li> Questions sur les concepts fondamentaux</li> <li> Questions sur les CNN et RNN</li> <li> Questions de suivi dans une conversation</li> <li> Requ\u00eates non li\u00e9es au Deep Learning</li> </ul>"},{"location":"evaluation/checklist-auto-evaluation/#conseils-pour-utiliser-cette-checklist","title":"Conseils pour utiliser cette checklist","text":"<ol> <li>Avant le d\u00e9veloppement : Consultez cette liste pour comprendre les attentes</li> <li>Pendant le d\u00e9veloppement : V\u00e9rifiez r\u00e9guli\u00e8rement votre progression</li> <li>Avant la soumission finale : Assurez-vous que tous les points essentiels sont coch\u00e9s</li> </ol>"},{"location":"evaluation/checklist-auto-evaluation/#priorisation-des-taches","title":"Priorisation des t\u00e2ches","text":"<p>Si vous manquez de temps, concentrez-vous sur ces \u00e9l\u00e9ments prioritaires :</p>"},{"location":"evaluation/checklist-auto-evaluation/#fonctionnalites-essentielles-priorite-haute","title":"Fonctionnalit\u00e9s essentielles (priorit\u00e9 haute)","text":"<ul> <li>Interface conversationnelle fonctionnelle</li> <li>Int\u00e9gration basique avec l'API Mistral</li> <li>Base de connaissances avec concepts fondamentaux</li> <li>Documentation utilisateur minimale</li> </ul>"},{"location":"evaluation/checklist-auto-evaluation/#ameliorations-priorite-moyenne","title":"Am\u00e9liorations (priorit\u00e9 moyenne)","text":"<ul> <li>Gestion avanc\u00e9e du contexte conversationnel</li> <li>Adaptation au niveau de l'utilisateur</li> <li>Documentation technique compl\u00e8te</li> <li>Tests approfondis</li> </ul>"},{"location":"evaluation/checklist-auto-evaluation/#fonctionnalites-bonus-priorite-basse","title":"Fonctionnalit\u00e9s bonus (priorit\u00e9 basse)","text":"<ul> <li>Fonctionnalit\u00e9s p\u00e9dagogiques avanc\u00e9es (quiz, exercices)</li> <li>Personnalisation de l'interface</li> <li>Analyse des r\u00e9ponses de l'utilisateur</li> <li>Multilingue ou fonctionnalit\u00e9s suppl\u00e9mentaires</li> </ul>"},{"location":"evaluation/checklist-auto-evaluation/#evaluation-finale","title":"\u00c9valuation finale","text":"<p>Avant de soumettre votre projet, comptez le nombre d'\u00e9l\u00e9ments que vous avez pu impl\u00e9menter :</p> <ul> <li>Interface utilisateur : ___ / 4</li> <li>Fonctionnalit\u00e9s API : ___ / 4</li> <li>Base de connaissances : ___ / 4</li> <li>Qualit\u00e9 p\u00e9dagogique : ___ / 4</li> <li>Documentation : ___ / 4</li> <li>Tests : ___ / 4</li> </ul> <p>Total : ___ / 24</p> <p>Ce score vous donne une id\u00e9e de la compl\u00e9tude de votre projet, mais rappelez-vous que la qualit\u00e9 de l'impl\u00e9mentation est aussi importante que la quantit\u00e9 de fonctionnalit\u00e9s.</p>"},{"location":"evaluation/criteres-evaluation/","title":"Crit\u00e8res d'\u00e9valuation","text":"<p>Ce document pr\u00e9sente les crit\u00e8res d'\u00e9valuation  du projet de chatbot p\u00e9dagogique sur le Deep Learning.</p>"},{"location":"evaluation/criteres-evaluation/#repartition-globale","title":"R\u00e9partition globale","text":"Composante % Description Participation aux activit\u00e9s 20% Engagement dans les activit\u00e9s et exercices pratiques Mini-projets 30% R\u00e9alisation des mini-projets CNN et RNN Projet final - Chatbot 50% Conception et d\u00e9veloppement du chatbot p\u00e9dagogique"},{"location":"evaluation/criteres-evaluation/#detail-des-criteres","title":"D\u00e9tail des crit\u00e8res","text":""},{"location":"evaluation/criteres-evaluation/#1-participation-aux-activites-20","title":"1. Participation aux activit\u00e9s (20%)","text":"<ul> <li>Pr\u00e9sence et engagement : Participation active aux s\u00e9ances</li> <li>R\u00e9alisation des exercices : Compl\u00e9tion des notebooks et exercices pratiques</li> <li>Collaboration : Travail d'\u00e9quipe efficace</li> </ul>"},{"location":"evaluation/criteres-evaluation/#2-mini-projets-30","title":"2. Mini-projets (30%)","text":""},{"location":"evaluation/criteres-evaluation/#mini-projet-cnn-15","title":"Mini-projet CNN (15%)","text":"<ul> <li>Mise en \u0153uvre fonctionnelle du mod\u00e8le CNN</li> <li>Compr\u00e9hension des concepts (filtres, feature maps)</li> <li>Exp\u00e9rimentation et analyse des r\u00e9sultats</li> </ul>"},{"location":"evaluation/criteres-evaluation/#mini-projet-rnn-15","title":"Mini-projet RNN (15%)","text":"<ul> <li>Impl\u00e9mentation du mod\u00e8le pour l'analyse de sentiment</li> <li>Utilisation de l'API Mistral AI</li> <li>Documentation des r\u00e9sultats et observations</li> </ul>"},{"location":"evaluation/criteres-evaluation/#3-projet-final-chatbot-pedagogique-50","title":"3. Projet final - Chatbot p\u00e9dagogique (50%)","text":"<ul> <li>Fonctionnalit\u00e9s de base (20%)</li> <li>Interface conversationnelle fonctionnelle</li> <li>Int\u00e9gration r\u00e9ussie avec l'API Mistral AI</li> <li> <p>Capacit\u00e9 \u00e0 r\u00e9pondre aux questions sur le Deep Learning</p> </li> <li> <p>Base de connaissances (15%)</p> </li> <li>Structure claire et coh\u00e9rente</li> <li>Couverture des concepts principaux vus en cours</li> <li> <p>Adaptation au niveau de l'utilisateur</p> </li> <li> <p>Documentation et pr\u00e9sentation (15%)</p> </li> <li>Documentation du projet et de son fonctionnement</li> <li>Pr\u00e9sentation claire du projet</li> <li>D\u00e9monstration fonctionnelle</li> </ul>"},{"location":"evaluation/criteres-evaluation/#modalites-devaluation","title":"Modalit\u00e9s d'\u00e9valuation","text":""},{"location":"evaluation/criteres-evaluation/#evaluation-continue","title":"\u00c9valuation continue","text":"<p>Tout au long du parcours, votre progression sera \u00e9valu\u00e9e via : - La compl\u00e9tion des notebooks et exercices - La participation aux discussions et activit\u00e9s - La qualit\u00e9 des mini-projets rendus</p>"},{"location":"evaluation/criteres-evaluation/#projet-final","title":"Projet final","text":"<p>Le projet final sera \u00e9valu\u00e9 sur : - Le respect des fonctionnalit\u00e9s demand\u00e9es - La qualit\u00e9 technique de l'impl\u00e9mentation - La pertinence p\u00e9dagogique du chatbot - La clart\u00e9 de la pr\u00e9sentation</p>"},{"location":"evaluation/criteres-evaluation/#conseils-pour-reussir","title":"Conseils pour r\u00e9ussir","text":"<ol> <li>Pratiquez r\u00e9guli\u00e8rement : Compl\u00e9tez tous les exercices et exp\u00e9rimentez par vous-m\u00eame</li> <li>Commencez t\u00f4t le d\u00e9veloppement du chatbot</li> <li>Testez abondamment votre solution avec diff\u00e9rents types de questions</li> <li>Documentez votre travail au fur et \u00e0 mesure</li> </ol>"},{"location":"evaluation/criteres-evaluation/#bareme-indicatif","title":"Bar\u00e8me indicatif","text":"Note Appr\u00e9ciation 16-20 Excellent : Toutes les fonctionnalit\u00e9s impl\u00e9ment\u00e9es avec qualit\u00e9, base de connaissances riche, pr\u00e9sentation claire 12-15 Bon : Fonctionnalit\u00e9s essentielles pr\u00e9sentes, quelques limitations, bonne pr\u00e9sentation 9-11 Satisfaisant : Base fonctionnelle mais incompl\u00e8te 5-8 Insuffisant : Fonctionnalit\u00e9s de base incompl\u00e8tes 0-4 Tr\u00e8s insuffisant : Projet non fonctionnel"},{"location":"evaluation/grille-repartition-taches/","title":"Grille de r\u00e9partition des t\u00e2ches","text":""},{"location":"evaluation/grille-repartition-taches/#organisation-du-travail-dequipe-pour-le-projet-chatbot-pedagogique","title":"Organisation du travail d'\u00e9quipe pour le projet chatbot p\u00e9dagogique","text":"<p>Cette grille  vous aidera \u00e0 organiser efficacement le travail au sein de votre \u00e9quipe pour le d\u00e9veloppement du chatbot p\u00e9dagogique.</p>"},{"location":"evaluation/grille-repartition-taches/#composition-de-lequipe","title":"Composition de l'\u00e9quipe","text":"Nom et pr\u00e9nom R\u00f4le principal Contact <p>\u00c0 compl\u00e9ter avec les informations de votre \u00e9quipe (2 personnes maximum par \u00e9quipe)</p>"},{"location":"evaluation/grille-repartition-taches/#domaines-de-responsabilite","title":"Domaines de responsabilit\u00e9","text":"<p>Pour une r\u00e9partition \u00e9quilibr\u00e9e du travail, voici une suggestion de r\u00e9partition :</p>"},{"location":"evaluation/grille-repartition-taches/#repartition-par-composant","title":"R\u00e9partition par composant","text":"Composant Membre 1 Membre 2 Interface utilisateur \u25a1 \u25a1 Backend et API \u25a1 \u25a1 Base de connaissances \u25a1 \u25a1 Documentation \u25a1 \u25a1 <p>Cochez pour indiquer qui est responsable de chaque composant</p>"},{"location":"evaluation/grille-repartition-taches/#repartition-par-taches","title":"R\u00e9partition par t\u00e2ches","text":"T\u00e2che Responsable \u00c9ch\u00e9ance Statut Configuration initiale du projet Int\u00e9gration de l'API Mistral Cr\u00e9ation de l'interface web Structuration de la base de connaissances D\u00e9veloppement des fonctionnalit\u00e9s p\u00e9dagogiques Tests fonctionnels Documentation Pr\u00e9paration de la pr\u00e9sentation"},{"location":"evaluation/grille-repartition-taches/#planning-simplifie","title":"Planning simplifi\u00e9","text":"Temps Activit\u00e9 Objectif Semaine 1 Conception et planification D\u00e9finir l'architecture et les responsabilit\u00e9s Semaine 2 D\u00e9veloppement des composants de base Interface et int\u00e9gration API fonctionnelles Semaine 3 Enrichissement de la base de connaissances Am\u00e9liorer la qualit\u00e9 des r\u00e9ponses Semaine 4 Finalisation et tests Pr\u00e9parer la pr\u00e9sentation finale"},{"location":"evaluation/grille-repartition-taches/#suivi-des-progres","title":"Suivi des progr\u00e8s","text":""},{"location":"evaluation/grille-repartition-taches/#points-detape","title":"Points d'\u00e9tape","text":"Date Objectifs atteints Difficult\u00e9s rencontr\u00e9es Prochaines \u00e9tapes <p>Pr\u00e9voyez un point d'\u00e9tape hebdomadaire minimum</p>"},{"location":"evaluation/grille-repartition-taches/#defis-anticipes-et-solutions","title":"D\u00e9fis anticip\u00e9s et solutions","text":"D\u00e9fi potentiel Solution envisag\u00e9e Difficult\u00e9s avec l'API Mistral Probl\u00e8mes d'int\u00e9gration frontend/backend Qualit\u00e9 insuffisante des r\u00e9ponses Manque de temps pour certaines fonctionnalit\u00e9s"},{"location":"evaluation/grille-repartition-taches/#ressources-partagees","title":"Ressources partag\u00e9es","text":"<p>Listez ici les ressources partag\u00e9es entre les membres de l'\u00e9quipe :</p> <ul> <li>D\u00e9p\u00f4t GitHub : _______</li> <li>Documentation de r\u00e9f\u00e9rence : _______</li> <li>Outils de communication : _______</li> <li>Autres ressources : _______</li> </ul>"},{"location":"evaluation/grille-repartition-taches/#engagement-de-lequipe","title":"Engagement de l'\u00e9quipe","text":"<p>Nous nous engageons \u00e0 : - Communiquer r\u00e9guli\u00e8rement sur notre progression - Respecter les \u00e9ch\u00e9ances fix\u00e9es - Demander de l'aide en cas de difficult\u00e9s - Contribuer \u00e9quitablement au projet</p> <p>Cette grille est un outil de travail qui peut \u00e9voluer selon vos besoins. L'important est qu'elle vous aide \u00e0 structurer votre collaboration de mani\u00e8re efficace.</p>"},{"location":"module1/","title":"\ud83e\udde0 Module 1 : Fondamentaux du Deep Learning","text":""},{"location":"module1/#objectifs-du-module","title":"\u2705 Objectifs du module","text":"<p>\u00c0 l'issue de ce module, vous serez capable de :</p> <ul> <li>Manipuler concr\u00e8tement un r\u00e9seau de neurones simple</li> <li>Comprendre les diff\u00e9rences entre Machine Learning classique et Deep Learning</li> <li>Expliquer le fonctionnement de base d'un r\u00e9seau de neurones</li> <li>Appliquer des techniques d'am\u00e9lioration d'un mod\u00e8le de Deep Learning</li> </ul>"},{"location":"module1/#programme-4h","title":"\ud83d\udcca Programme (4h)","text":"<p>Ce module se d\u00e9roule en quatre phases distinctes, chacune con\u00e7ue pour vous faire d\u00e9couvrir le Deep Learning par la pratique plut\u00f4t que par la th\u00e9orie.</p>"},{"location":"module1/#phase-1-introduction-pratique-1h","title":"\ud83d\udd0d Phase 1 : Introduction pratique (1h)","text":"<p>D\u00e9couvrez le Deep Learning \u00e0 travers des exemples concrets, sans vous pr\u00e9occuper de la th\u00e9orie pour le moment.</p> <ul> <li>\ud83c\udfae D\u00e9monstrations d'applications concr\u00e8tes (GitHub Copilot, reconnaissance d'objets...)</li> <li>\ud83d\udd04 Premier contact avec un r\u00e9seau de neurones simple</li> <li>\ud83c\udfc6 Challenge d'exp\u00e9rimentation guid\u00e9e sur un mod\u00e8le MNIST</li> </ul>"},{"location":"module1/#phase-2-concepts-fondamentaux-1h30","title":"\ud83e\udde9 Phase 2 : Concepts fondamentaux (1h30)","text":"<p>Comparez les approches du Machine Learning classique et du Deep Learning pour comprendre leurs diff\u00e9rences fondamentales.</p> <ul> <li>\ud83d\udd2c Atelier \"Bo\u00eete noire\" : exploration parall\u00e8le des deux approches</li> <li>\ud83d\udd04 D\u00e9fi de g\u00e9n\u00e9ralisation sur des donn\u00e9es modifi\u00e9es</li> <li>\ud83d\udd0d Exploration interactive d'un neurone et d'un r\u00e9seau simple</li> </ul>"},{"location":"module1/#phase-3-mini-projet-individuel-1h10","title":"\ud83d\udee0\ufe0f Phase 3 : Mini-projet individuel (1h10)","text":"<p>Mettez en pratique vos connaissances en modifiant et am\u00e9liorant un r\u00e9seau de neurones.</p> <ul> <li>\u2699\ufe0f Modification des hyperparam\u00e8tres</li> <li>\ud83e\uddea Test avec diff\u00e9rentes architectures</li> <li>\ud83d\udcca Analyse de l'impact des changements sur les performances</li> <li>\ud83d\udcdd Documentation des r\u00e9sultats dans un rapport synth\u00e9tique</li> </ul>"},{"location":"module1/#auto-evaluation-et-synthese-20-min","title":"\ud83d\udcdd Auto-\u00e9valuation et synth\u00e8se (20 min)","text":"<p>Cette \u00e9tape finale du module vous permettra de consolider vos connaissances et d'\u00e9valuer votre compr\u00e9hension.</p>"},{"location":"module1/#carte-heuristique-des-fondamentaux-du-deep-learning","title":"\ud83e\udde0 Carte heuristique des fondamentaux du Deep Learning","text":"<p>\ud83d\udcca Visualisation compl\u00e8te</p> <p>Pour une exploration interactive et d\u00e9taill\u00e9e de tous les concepts du Deep Learning, consultez la version interactive de la carte mentale.</p>"},{"location":"module1/#qcm-dauto-evaluation","title":"\u2705 QCM d'auto-\u00e9valuation","text":"<p>Testez vos connaissances en Machine Learning</p> <p>Ce QCM couvre l'ensemble des concepts fondamentaux abord\u00e9s dans ce module:</p> <ul> <li>15 questions sur les fondamentaux du Deep Learning</li> <li>\u00c9valuation de votre compr\u00e9hension des diff\u00e9rentes architectures</li> <li>Explication d\u00e9taill\u00e9e des r\u00e9ponses pour renforcer votre apprentissage</li> </ul> <p>Commencer le QCM</p>"},{"location":"module1/#synthese-personnelle","title":"\ud83d\udcdd Synth\u00e8se personnelle","text":"<p>Intelligence Artificielle - R\u00e9flexion globale</p> <p>Avant de conclure ce module, prenez quelques minutes pour r\u00e9fl\u00e9chir \u00e0 votre apprentissage:</p> <ol> <li>Identifiez les 3 concepts qui vous ont sembl\u00e9 les plus importants</li> <li>Comparez les approches de Machine Learning classique et de Deep Learning</li> <li>R\u00e9fl\u00e9chissez aux applications potentielles dans votre domaine professionnel</li> </ol> <p>Cette r\u00e9flexion personnelle contribuera significativement \u00e0 ancrer vos apprentissages.</p>"},{"location":"module1/#livrables-attendus","title":"Livrables attendus","text":"<p>\u00c0 l'issue de ce module, vous devrez avoir produit :</p> <ul> <li>\ud83d\udccb Phase 1 : La fiche d'observations compl\u00e9t\u00e9e sur le \"Hello World du Deep Learning\"</li> <li>\ud83d\udccb Phase 2 : fiche d'observations - Concepts fondamentaux du Deep Learning</li> <li>\ud83d\udccb Phase 3 : fiche d'observations - Mini-projet d'am\u00e9lioration</li> </ul>"},{"location":"module1/#ressources-complementaires","title":"Ressources compl\u00e9mentaires","text":"<ul> <li>\ud83d\udcd5 Glossaire du Deep Learning - Les termes essentiels expliqu\u00e9s simplement</li> <li>\ud83d\udcda Guide d'utilisation de Google Colab - Pour vous aider \u00e0 utiliser cet outil</li> </ul>"},{"location":"module1/#competences-bts-sio-developpees","title":"Comp\u00e9tences BTS SIO d\u00e9velopp\u00e9es","text":"<p>Ce module vous permet d'acqu\u00e9rir plusieurs comp\u00e9tences du r\u00e9f\u00e9rentiel BTS SIO :</p> Comp\u00e9tence Description Activit\u00e9s associ\u00e9es B1.3 Gestion des donn\u00e9es Manipulation des datasets d'images B2.2 Conception et d\u00e9veloppement Am\u00e9lioration des mod\u00e8les de Deep Learning B2.3 Conception et d\u00e9veloppement d'IHM Analyse des interfaces de notebooks interactifs B3.2 V\u00e9rification et validation \u00c9valuation de la performance des mod\u00e8les"},{"location":"module1/#pret-a-commencer","title":"Pr\u00eat \u00e0 commencer ?","text":"<p>Conseil</p> <p>Avant de commencer, assurez-vous d'avoir un compte Google pour utiliser Colab et d'avoir parcouru le guide d'utilisation.</p> <p>Plongez dans le monde fascinant du Deep Learning en commen\u00e7ant par la premi\u00e8re phase d'introduction pratique !</p> <p>Commencer par l'introduction pratique \u00c9valuer vos connaissances</p>"},{"location":"module1/concepts-fondamentaux/","title":"\ud83e\udde9 Phase 2 : D\u00e9couverte des concepts par l'exp\u00e9rimentation","text":""},{"location":"module1/concepts-fondamentaux/#objectifs-de-la-phase","title":"\ud83c\udfaf Objectifs de la phase","text":"<p>Dans cette phase, vous allez :</p> <ul> <li>Comparer exp\u00e9rimentalement le Machine Learning classique et le Deep Learning</li> <li>Observer les diff\u00e9rences fondamentales en termes de pr\u00e9paration des donn\u00e9es et de performances</li> <li>D\u00e9couvrir l'anatomie d'un r\u00e9seau de neurones en manipulant directement ses composants</li> <li>Comprendre par la pratique comment l'information circule dans un r\u00e9seau de neurones</li> </ul>"},{"location":"module1/concepts-fondamentaux/#fiche-dobservations-a-completer","title":"\ud83d\udccb Fiche d'observations \u00e0 compl\u00e9ter","text":"<p>IMPORTANT : Tout au long de cette phase, vous devrez compl\u00e9ter la Fiche d'observations disponible ci-dessous. Ce document sera votre livrable principal et vous aidera \u00e0 structurer votre apprentissage.</p> <p>\ud83d\udce5 T\u00e9l\u00e9chargez et consultez la \ud83d\udccb fiche d'observations d\u00e8s maintenant pour comprendre les \u00e9l\u00e9ments \u00e0 observer et \u00e0 documenter pendant les activit\u00e9s.</p>"},{"location":"module1/concepts-fondamentaux/#comparaison-pratique-machine-learning-vs-deep-learning-30-min","title":"Comparaison pratique : Machine Learning vs Deep Learning (30 min)","text":""},{"location":"module1/concepts-fondamentaux/#objectif","title":"Objectif","text":"<p>Comprendre par l'observation directe les diff\u00e9rences fondamentales entre le Machine Learning classique et le Deep Learning, en les appliquant au m\u00eame jeu de donn\u00e9es.</p>"},{"location":"module1/concepts-fondamentaux/#instructions-pour-une-pratique-individuelle","title":"Instructions pour une pratique individuelle","text":"<ol> <li> <p>Ouvrez deux notebooks Google Colab dans des onglets s\u00e9par\u00e9s :</p> <ul> <li>Machine Learning classique (Random Forest)</li> <li>Deep Learning (CNN)    -  </li> <li>Compl\u00e9ter la \ud83d\udccb fiche d'observations</li> </ul> </li> <li> <p>Suivez les instructions dans chaque notebook et ex\u00e9cutez les cellules dans l'ordre indiqu\u00e9.</p> </li> <li> <p>Pour la fiche d'observations : Pendant que vous explorez les deux approches, notez dans la section \"Partie 1\" de votre fiche :</p> <ul> <li>Comment chaque approche traite les donn\u00e9es MNIST (chiffres manuscrits)</li> <li>Les diff\u00e9rences dans la pr\u00e9paration des donn\u00e9es</li> <li>La complexit\u00e9 d'impl\u00e9mentation de chaque approche</li> <li>Le temps d'entra\u00eenement respectif</li> <li>Les performances sur donn\u00e9es normales et bruit\u00e9es</li> </ul> </li> </ol>"},{"location":"module1/concepts-fondamentaux/#points-cles-a-identifier-par-vous-meme","title":"Points cl\u00e9s \u00e0 identifier par vous-m\u00eame","text":"<p>\u00c0 travers cette exp\u00e9rimentation, identifiez ces concepts fondamentaux :</p> <ul> <li>Comment les caract\u00e9ristiques (features) sont trait\u00e9es dans chaque approche</li> <li>Le r\u00f4le de la repr\u00e9sentation des donn\u00e9es</li> <li>La capacit\u00e9 d'abstraction des diff\u00e9rents mod\u00e8les</li> <li>Les compromis entre temps d'entra\u00eenement et performance</li> </ul>"},{"location":"module1/concepts-fondamentaux/#exploration-pratique-anatomie-dun-reseau-de-neurones-45-min","title":"Exploration pratique : Anatomie d'un r\u00e9seau de neurones (45 min)","text":"<p>Dans cette partie, vous allez explorer individuellement le fonctionnement interne d'un r\u00e9seau de neurones.</p>"},{"location":"module1/concepts-fondamentaux/#materiel-pour-la-pratique-individuelle","title":"Mat\u00e9riel pour la pratique individuelle","text":"<ul> <li>Notebook interactif \"Anatomie d'un r\u00e9seau de neurones\"</li> </ul>"},{"location":"module1/concepts-fondamentaux/#instructions-etape-par-etape","title":"Instructions \u00e9tape par \u00e9tape","text":""},{"location":"module1/concepts-fondamentaux/#partie-1-exploration-dun-neurone-unique-15-min","title":"Partie 1 : Exploration d'un neurone unique (15 min)","text":"<p>Dans cette partie, vous allez manipuler un neurone artificiel unique pour comprendre son fonctionnement de base.</p> <ol> <li>Ouvrez le notebook \"Anatomie d'un r\u00e9seau de neurones\" dans Google Colab</li> <li>Ex\u00e9cutez les cellules d'importation des biblioth\u00e8ques et de configuration</li> <li>Localisez la section \"Neurone unique\" et ex\u00e9cutez la cellule d'initialisation</li> <li> <p>Exp\u00e9rimentez avec les contr\u00f4les interactifs pour :</p> </li> <li> <p>Modifier les valeurs d'entr\u00e9e (x\u2081, x\u2082)</p> </li> <li>Ajuster les poids (w\u2081, w\u2082)</li> <li>Changer la valeur du biais (b)</li> <li>Observer l'effet sur la sortie du neurone</li> </ol> <p>Questions \u00e0 explorer par vous-m\u00eame :</p> <ul> <li>Que se passe-t-il si tous les poids sont \u00e0 z\u00e9ro ?</li> <li>Comment pouvez-vous configurer le neurone pour qu'il s'active uniquement si les deux entr\u00e9es sont \u00e9lev\u00e9es ?</li> <li>Quel est l'effet du biais sur le \"seuil\" d'activation ?</li> <li>Comment la fonction d'activation ReLU transforme-t-elle la sortie ?</li> </ul>"},{"location":"module1/concepts-fondamentaux/#partie-2-de-lunique-au-reseau-15-min","title":"Partie 2 : De l'unique au r\u00e9seau (15 min)","text":"<p>Passez maintenant \u00e0 un petit r\u00e9seau de neurones pour comprendre comment l'information circule \u00e0 travers les couches.</p> <ol> <li>Localisez la section \"R\u00e9seau simple\" et ex\u00e9cutez les cellules d'initialisation</li> <li> <p>Explorez le r\u00e9seau compos\u00e9 de :</p> </li> <li> <p>Une couche d'entr\u00e9e (2 neurones)</p> </li> <li>Une couche cach\u00e9e (3 neurones)</li> <li> <p>Une couche de sortie (1 neurone)</p> </li> <li> <p>R\u00e9alisez les exp\u00e9riences suivantes par vous-m\u00eame :</p> </li> <li> <p>Observez comment le signal se propage \u00e0 travers les couches</p> </li> <li>Suivez le parcours d'une information sp\u00e9cifique (valeur d'entr\u00e9e)</li> <li>Identifiez les \"motifs d'activation\" qui se forment pour diff\u00e9rentes entr\u00e9es</li> <li>Testez diff\u00e9rentes fonctions d'activation (ReLU, Sigmoid, Tanh)</li> </ol> <p>Exercice pratique :  Essayez de configurer manuellement les poids pour que le r\u00e9seau r\u00e9alise la fonction logique XOR (entr\u00e9es : [0,0]\u21920, [0,1]\u21921, [1,0]\u21921, [1,1]\u21920).</p>"},{"location":"module1/concepts-fondamentaux/#partie-3-visualisation-de-lentrainement-10-min","title":"Partie 3 : Visualisation de l'entra\u00eenement (10 min)","text":"<p>Dans cette partie, vous allez observer comment un r\u00e9seau apprend au fil du temps.</p> <ol> <li>Localisez la section \"Entra\u00eenement\" et ex\u00e9cutez la cellule d'initialisation</li> <li>Lancez la visualisation de l'entra\u00eenement en temps r\u00e9el</li> <li> <p>Observez :</p> </li> <li> <p>L'\u00e9volution des poids \u00e0 chaque it\u00e9ration</p> </li> <li>Comment la \"fronti\u00e8re de d\u00e9cision\" se modifie</li> <li> <p>La diminution de l'erreur au fil des \u00e9poques</p> </li> <li> <p>Essayez de modifier par vous-m\u00eame :</p> </li> <li> <p>Le taux d'apprentissage (learning rate)</p> </li> <li>La complexit\u00e9 du probl\u00e8me (type de donn\u00e9es)</li> <li>L'architecture du r\u00e9seau (nombre de neurones)</li> </ol>"},{"location":"module1/concepts-fondamentaux/#partie-4-synthese-et-verbalisation-5-min","title":"Partie 4 : Synth\u00e8se et verbalisation (5 min)","text":"<ol> <li>Compl\u00e9tez le sch\u00e9ma du r\u00e9seau de neurones fourni en fin de notebook</li> <li> <p>Identifiez et nommez correctement :</p> </li> <li> <p>Les entr\u00e9es et sorties</p> </li> <li>Les poids et biais</li> <li>Les fonctions d'activation</li> <li>Les couches cach\u00e9es</li> <li> <p>R\u00e9digez un court paragraphe (5-7 lignes) expliquant avec vos propres mots :</p> </li> <li> <p>Comment un r\u00e9seau de neurones traite l'information</p> </li> <li>Comment il peut apprendre \u00e0 partir d'exemples</li> </ol>"},{"location":"module1/concepts-fondamentaux/#defi-de-generalisation-10-min","title":"D\u00e9fi de g\u00e9n\u00e9ralisation (10 min)","text":"<p>Pour approfondir votre compr\u00e9hension, r\u00e9alisez ce d\u00e9fi suppl\u00e9mentaire :</p> <ol> <li>Retournez aux notebooks de la premi\u00e8re partie (ML classique et Deep Learning)</li> <li>Localisez la section \"D\u00e9fi de g\u00e9n\u00e9ralisation\" dans chaque notebook</li> <li> <p>Ex\u00e9cutez les cellules qui permettent de tester les mod\u00e8les sur :</p> </li> <li> <p>Des images avec du bruit ajout\u00e9</p> </li> <li>Des images avec rotation l\u00e9g\u00e8re</li> <li>Notez les performances des deux approches sur ces donn\u00e9es modifi\u00e9es</li> <li> <p>Analysez par vous-m\u00eame :</p> </li> <li> <p>Lequel des mod\u00e8les g\u00e9n\u00e9ralise le mieux aux nouvelles donn\u00e9es ?</p> </li> <li>Pourquoi existe-t-il cette diff\u00e9rence ?</li> <li>Quels avantages et inconv\u00e9nients pr\u00e9sentent chaque approche ?</li> </ol>"},{"location":"module1/concepts-fondamentaux/#ressources-complementaires","title":"Ressources compl\u00e9mentaires","text":"<ul> <li>Guide d'utilisation de Google Colab - Pour vous aider \u00e0 utiliser cet environnement</li> <li>Glossaire du Deep Learning - D\u00e9finitions des termes techniques rencontr\u00e9s</li> <li>TensorFlow Playground - Interface interactive pour exp\u00e9rimenter avec des r\u00e9seaux de neurones simples</li> </ul> <p>Retour au Module 1 Continuer vers le mini-projet</p>"},{"location":"module1/introduction-pratique/","title":"\ud83d\udd0d Phase 1: Introduction pratique au Deep Learning","text":""},{"location":"module1/introduction-pratique/#objectifs-de-cette-section","title":"\ud83c\udfaf Objectifs de cette section","text":"<p>Dans cette premi\u00e8re approche du Deep Learning, vous allez :</p> <ul> <li>D\u00e9couvrir des applications concr\u00e8tes et impressionnantes du Deep Learning</li> <li>Manipuler votre premier r\u00e9seau de neurones sans pr\u00e9requis th\u00e9oriques</li> <li>Exp\u00e9rimenter l'impact des modifications sur les performances d'un mod\u00e8le</li> <li>D\u00e9velopper une intuition sur le fonctionnement des r\u00e9seaux de neurones</li> </ul>"},{"location":"module1/introduction-pratique/#approche-pedagogique-dabord-la-pratique-ensuite-la-theorie","title":"\ud83d\udca1 Approche p\u00e9dagogique : d'abord la pratique, ensuite la th\u00e9orie","text":"<p>Contrairement \u00e0 l'approche traditionnelle qui commence par la th\u00e9orie, nous allons d'abord vous faire manipuler des mod\u00e8les de Deep Learning pour \u00e9veiller votre curiosit\u00e9 et vous donner une intuition pratique. Les concepts th\u00e9oriques seront introduits progressivement, en s'appuyant sur votre exp\u00e9rience directe.</p>"},{"location":"module1/introduction-pratique/#partie-1-applications-du-deep-learning-15-min","title":"\ud83c\udfae Partie 1 : Applications du Deep Learning (15 min)","text":""},{"location":"module1/introduction-pratique/#demonstration-1-github-copilot","title":"D\u00e9monstration 1 : GitHub Copilot","text":"<p>GitHub Copilot est un assistant de programmation bas\u00e9 sur un mod\u00e8le de Deep Learning. Il analyse le contexte de votre code et sugg\u00e8re des compl\u00e9ments pertinents.</p> <p>Comment \u00e7a fonctionne :</p> <ul> <li>Entra\u00een\u00e9 sur des millions de d\u00e9p\u00f4ts GitHub publics</li> <li>Utilise un mod\u00e8le de langage bas\u00e9 sur des architectures avanc\u00e9es</li> <li>Analyse le contexte (code existant, commentaires, noms de fonctions)</li> <li>G\u00e9n\u00e8re des suggestions pertinentes en temps r\u00e9el</li> </ul> <p>Exemple pratique :</p> <ul> <li>\u00c9criture d'une fonction \u00e0 partir d'un simple commentaire</li> <li>Compl\u00e9tion de code automatique</li> <li>G\u00e9n\u00e9ration de tests unitaires</li> </ul>"},{"location":"module1/introduction-pratique/#demonstration-2-reconnaissance-dobjets-en-temps-reel","title":"D\u00e9monstration 2 : Reconnaissance d'objets en temps r\u00e9el","text":"<p>La reconnaissance d'objets est l'une des applications les plus visibles du Deep Learning. Nous utiliserons l'application Teachable Machine pour une d\u00e9monstration en direct.</p> <p>Points \u00e0 observer :</p> <ul> <li>D\u00e9tection en temps r\u00e9el d'objets pr\u00e9sents dans la salle</li> <li>Niveau de confiance (pourcentage) pour chaque pr\u00e9diction</li> <li>Robustesse face aux variations (angle, \u00e9clairage)</li> </ul> <p>Comment \u00e7a fonctionne :</p> <ul> <li>Utilise des r\u00e9seaux de neurones convolutifs (CNN)</li> <li>D\u00e9tecte des caract\u00e9ristiques visuelles \u00e0 diff\u00e9rents niveaux d'abstraction</li> <li>Identifie et localise les objets dans l'image</li> </ul>"},{"location":"module1/introduction-pratique/#demonstration-3-generation-de-texte","title":"D\u00e9monstration 3 : G\u00e9n\u00e9ration de texte","text":"<p>Les mod\u00e8les de langage comme GPT ou Mistral peuvent g\u00e9n\u00e9rer du texte coh\u00e9rent et contextuellement pertinent sur pratiquement n'importe quel sujet.</p> <p>Exp\u00e9rimentation :</p> <ul> <li>Essai de diff\u00e9rentes amorces (technique, cr\u00e9ative, formelle)</li> <li>Observation de l'adaptation au style et au contexte</li> <li>Analyse de la coh\u00e9rence des textes g\u00e9n\u00e9r\u00e9s</li> </ul> <p>Applications professionnelles :</p> <ul> <li>G\u00e9n\u00e9ration automatique de descriptions de produits</li> <li>Cr\u00e9ation d'assistants virtuels pour guider les utilisateurs</li> <li>Production de r\u00e9sum\u00e9s de documents techniques</li> <li>Suggestions de r\u00e9ponses dans une application de service client</li> </ul>"},{"location":"module1/introduction-pratique/#partie-2-premier-contact-avec-un-reseau-de-neurones-30-min","title":"\ud83d\udd04 Partie 2 : Premier contact avec un r\u00e9seau de neurones (30 min)","text":""},{"location":"module1/introduction-pratique/#instructions-detaillees","title":"Instructions d\u00e9taill\u00e9es","text":""},{"location":"module1/introduction-pratique/#1-creation-dun-notebook-dans-google-colab","title":"1. Cr\u00e9ation d'un notebook dans Google Colab","text":"<p>Google Colab est un environnement Jupyter Notebook h\u00e9berg\u00e9 qui permet d'ex\u00e9cuter du code Python dans le cloud, sans installation locale.</p> <ol> <li>Ouvrez Google Colab</li> <li>Connectez-vous avec votre compte Google</li> <li>Cliquez sur \"Fichier\" &gt; \"Nouveau notebook\"</li> </ol>"},{"location":"module1/introduction-pratique/#2-exploration-du-hello-world-du-deep-learning","title":"2. Exploration du \"Hello World du Deep Learning\"","text":"<p>Le \"Hello World\" du Deep Learning est la reconnaissance de chiffres manuscrits avec le dataset MNIST. Vous allez impl\u00e9menter un r\u00e9seau de neurones simple capable de reconna\u00eetre des chiffres \u00e9crits \u00e0 la main.</p> <p>Suivez ces \u00e9tapes :</p> <ul> <li>Copier-coller les cellules depuis le notebook de r\u00e9f\u00e9rence</li> <li>Ex\u00e9cuter chaque cellule en cliquant sur le bouton \u25b6\ufe0f ou avec Ctrl+Entr\u00e9e</li> <li>Observer et analyser les r\u00e9sultats \u00e0 chaque \u00e9tape</li> <li>Compl\u00e9ter la fiche d'observations</li> </ul>"},{"location":"module1/introduction-pratique/#3-structure-du-notebook","title":"3. Structure du notebook","text":"<p>Le notebook de reconnaissance de chiffres manuscrits est organis\u00e9 en 9 sections progressives guidant votre apprentissage du Deep Learning :</p> <ol> <li> <p>Introduction    Pr\u00e9sentation du probl\u00e8me de reconnaissance de chiffres manuscrits et des objectifs d'apprentissage.</p> </li> <li> <p>Configuration    Installation et importation des biblioth\u00e8ques essentielles (TensorFlow, Keras, NumPy, Matplotlib).</p> </li> <li> <p>Chargement des donn\u00e9es    Pr\u00e9paration du dataset MNIST contenant 70 000 images de chiffres manuscrits de 0 \u00e0 9.</p> </li> <li> <p>Cr\u00e9ation du mod\u00e8le    Construction de l'architecture du r\u00e9seau de neurones avec ses diff\u00e9rentes couches.</p> </li> <li> <p>Entra\u00eenement    Configuration et lancement du processus d'apprentissage pour ajuster les poids du r\u00e9seau.</p> </li> <li> <p>Visualisation    Graphiques montrant l'\u00e9volution de la pr\u00e9cision et de la perte pendant l'entra\u00eenement.</p> </li> <li> <p>Pr\u00e9dictions    Test du mod\u00e8le entra\u00een\u00e9 sur des exemples pour \u00e9valuer ses performances.</p> </li> <li> <p>Dessin interactif    Interface permettant de dessiner vos propres chiffres et de les faire reconna\u00eetre par le mod\u00e8le.</p> </li> <li> <p>Exp\u00e9rimentation    Suggestions pour modifier le mod\u00e8le et observer les effets sur les performances.</p> </li> </ol>"},{"location":"module1/introduction-pratique/#4-experimentations-guidees","title":"4. Exp\u00e9rimentations guid\u00e9es","text":"<p>Apr\u00e8s avoir ex\u00e9cut\u00e9 le notebook de base, essayez ces modifications pour observer leur impact :</p> <ol> <li> <p>Modification de l'architecture :</p> <ul> <li>Augmenter/diminuer le nombre de neurones dans chaque couche</li> <li>Ajouter ou supprimer des couches dans le r\u00e9seau</li> <li>Essayer d'ajouter une couche Dropout (qui d\u00e9sactive al\u00e9atoirement certains neurones pendant l'entra\u00eenement)</li> </ul> </li> <li> <p>Ajustement des param\u00e8tres d'entra\u00eenement :</p> <ul> <li>Changer le nombre de cycles d'entra\u00eenement (\u00e9poques)</li> <li>Modifier le nombre d'exemples trait\u00e9s \u00e0 la fois (taille du batch)</li> <li>Tester diff\u00e9rentes m\u00e9thodes d'apprentissage (optimiseurs)</li> </ul> </li> <li> <p>Test avec vos propres dessins :</p> <ul> <li>Utiliser l'interface de dessin pour tester des chiffres manuscrits</li> <li>Observer comment le mod\u00e8le r\u00e9agit \u00e0 diff\u00e9rents styles d'\u00e9criture</li> <li>Analyser les pr\u00e9dictions erron\u00e9es et tenter de comprendre pourquoi</li> </ul> </li> </ol>"},{"location":"module1/introduction-pratique/#partie-3-reflexion-et-documentation-15-min","title":"\ud83d\udccb Partie 3 : R\u00e9flexion et documentation (15 min)","text":"<p>Apr\u00e8s vos exp\u00e9rimentations, prenez le temps de r\u00e9fl\u00e9chir \u00e0 ce que vous avez observ\u00e9 :</p> <ol> <li> <p>Compl\u00e9tez la fiche d'observations :</p> <ul> <li>Notez les performances initiales du mod\u00e8le</li> <li>Documentez l'impact de vos modifications</li> <li>Analysez les cas o\u00f9 le mod\u00e8le \u00e9choue</li> </ul> </li> <li> <p>Questions de r\u00e9flexion :</p> <ul> <li>Qu'est-ce qui semble avoir le plus d'impact sur les performances ?</li> <li>Quelles sont les limites du mod\u00e8le que vous avez observ\u00e9es ?</li> <li>Quelles applications pratiques pourriez-vous envisager avec cette technologie ?</li> </ul> </li> <li> <p>Partage d'exp\u00e9rience :</p> <ul> <li>\u00c9changez avec vos camarades sur vos observations</li> <li>Comparez les r\u00e9sultats de vos diff\u00e9rentes modifications</li> <li>Discutez des surprises ou des difficult\u00e9s rencontr\u00e9es</li> </ul> </li> </ol>"},{"location":"module1/introduction-pratique/#conclusion-et-transition","title":"\u2705 Conclusion et transition","text":"<p>Cette introduction pratique vous a permis de manipuler directement un r\u00e9seau de neurones sans vous pr\u00e9occuper imm\u00e9diatement des concepts th\u00e9oriques sous-jacents. Vous avez pu observer comment un mod\u00e8le apprend \u00e0 reconna\u00eetre des chiffres manuscrits et comment diverses modifications peuvent affecter ses performances.</p> <p>Dans la prochaine section, nous approfondirons les concepts fondamentaux du Deep Learning en nous appuyant sur votre exp\u00e9rience pratique. Nous comparerons \u00e9galement le Deep Learning avec les approches classiques du Machine Learning pour mieux comprendre ses particularit\u00e9s et ses avantages.</p>"},{"location":"module1/introduction-pratique/#ressources-complementaires","title":"\ud83d\udcda Ressources compl\u00e9mentaires","text":"<ul> <li>Guide d'utilisation de Google Colab - Pour vous aider \u00e0 utiliser cet environnement</li> <li>Glossaire du Deep Learning - D\u00e9finitions des termes techniques rencontr\u00e9s</li> <li>TensorFlow Playground - Interface interactive pour exp\u00e9rimenter avec des r\u00e9seaux de neurones simples</li> </ul> <p>Retour au Module 1 Continuer vers les Concepts fondamentaux</p>"},{"location":"module1/mini-projet/","title":"\ud83d\udee0\ufe0f Phase 3 : Mini-projet individuel (1h)","text":""},{"location":"module1/mini-projet/#objectifs","title":"\ud83c\udfaf Objectifs","text":"<p>Ce mini-projet individuel vous permettra de :</p> <ul> <li>Appliquer les connaissances acquises sur les r\u00e9seaux de neurones</li> <li>Exp\u00e9rimenter avec diff\u00e9rentes architectures et hyperparam\u00e8tres </li> <li>Comprendre l'impact des modifications sur les performances</li> <li>Documenter vos observations dans une fiche structur\u00e9e</li> </ul>"},{"location":"module1/mini-projet/#fiche-dobservations-a-completer","title":"\ud83d\udccb Fiche d'observations \u00e0 compl\u00e9ter","text":"<p>IMPORTANT : Tout au long de ce mini-projet, vous devrez compl\u00e9ter la Fiche d'observations disponible ci-dessous. Ce document sera votre livrable principal.</p> <p>\ud83d\udce5 T\u00e9l\u00e9chargez et consultez la \ud83d\udccb fiche d'observations d\u00e8s maintenant pour comprendre les \u00e9l\u00e9ments \u00e0 observer et \u00e0 documenter pendant le mini-projet.</p>"},{"location":"module1/mini-projet/#deroulement-du-mini-projet","title":"\ud83d\udcdd D\u00e9roulement du mini-projet","text":""},{"location":"module1/mini-projet/#etape-1-modele-de-base-15-min","title":"\u00c9tape 1 : Mod\u00e8le de base (15 min)","text":"<ol> <li>Cr\u00e9ez un nouveau notebook dans Google Colab</li> <li>Copiez-collez le code du mod\u00e8le de base ci-dessous</li> <li>Ex\u00e9cutez le code pour voir la performance initiale</li> </ol> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\nfrom tensorflow.keras.utils import to_categorical\n\n# Charger les donn\u00e9es\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\n# Pr\u00e9traiter les donn\u00e9es\nX_train = X_train.reshape(-1, 28, 28, 1) / 255.0\nX_test = X_test.reshape(-1, 28, 28, 1) / 255.0\ny_train_cat = to_categorical(y_train, 10)\ny_test_cat = to_categorical(y_test, 10)\n\n# Cr\u00e9er le mod\u00e8le de base\nmodel = Sequential([\n    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n    MaxPooling2D(pool_size=(2, 2)),\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dense(10, activation='softmax')\n])\n\n# Compiler le mod\u00e8le\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Afficher le r\u00e9sum\u00e9 du mod\u00e8le\nmodel.summary()\n\n# Entra\u00eener le mod\u00e8le\nhistory = model.fit(\n    X_train, y_train_cat,\n    epochs=3,  # Peu d'\u00e9poques pour aller vite\n    batch_size=128,\n    validation_split=0.2,\n    verbose=1\n)\n\n# \u00c9valuer le mod\u00e8le\ntest_loss, test_acc = model.evaluate(X_test, y_test_cat)\nprint(f\"Pr\u00e9cision sur les donn\u00e9es de test : {test_acc*100:.2f}%\")\n\n# Visualiser l'\u00e9volution de l'apprentissage\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Entra\u00eenement')\nplt.plot(history.history['val_accuracy'], label='Validation')\nplt.title('Pr\u00e9cision du mod\u00e8le')\nplt.xlabel('\u00c9poque')\nplt.ylabel('Pr\u00e9cision')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Entra\u00eenement')\nplt.plot(history.history['val_loss'], label='Validation')\nplt.title('Perte (loss)')\nplt.xlabel('\u00c9poque')\nplt.ylabel('Perte')\nplt.legend()\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"module1/mini-projet/#etape-2-ameliorations-du-modele-30-min","title":"\u00c9tape 2 : Am\u00e9liorations du mod\u00e8le (30 min)","text":"<p>Choisissez au moins 2 modifications parmi les propositions suivantes et notez vos observations sur votre fiche :</p>"},{"location":"module1/mini-projet/#modification-a-ajouter-une-couche-de-convolution","title":"Modification A : Ajouter une couche de convolution","text":"<pre><code>model = Sequential([\n    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n    MaxPooling2D(pool_size=(2, 2)),\n    Conv2D(64, kernel_size=(3, 3), activation='relu'),  # Couche ajout\u00e9e\n    MaxPooling2D(pool_size=(2, 2)),  # Pooling ajout\u00e9\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dense(10, activation='softmax')\n])\n</code></pre>"},{"location":"module1/mini-projet/#modification-b-ajouter-plus-de-neurones","title":"Modification B : Ajouter plus de neurones","text":"<pre><code>model = Sequential([\n    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n    MaxPooling2D(pool_size=(2, 2)),\n    Flatten(),\n    Dense(256, activation='relu'),  # 256 au lieu de 128\n    Dense(10, activation='softmax')\n])\n</code></pre>"},{"location":"module1/mini-projet/#modification-c-ajouter-du-dropout-pour-reduire-le-surapprentissage","title":"Modification C : Ajouter du Dropout pour r\u00e9duire le surapprentissage","text":"<pre><code>from tensorflow.keras.layers import Dropout\n\nmodel = Sequential([\n    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n    MaxPooling2D(pool_size=(2, 2)),\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.5),  # Ajout d'une couche de Dropout\n    Dense(10, activation='softmax')\n])\n</code></pre>"},{"location":"module1/mini-projet/#modification-d-changer-loptimiseur","title":"Modification D : Changer l'optimiseur","text":"<pre><code>from tensorflow.keras.optimizers import SGD\n\n# Compiler le mod\u00e8le avec SGD au lieu d'Adam\nmodel.compile(\n    optimizer=SGD(learning_rate=0.01),  # Utilisation de SGD\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n</code></pre>"},{"location":"module1/mini-projet/#modification-e-augmenter-le-nombre-depoques-dentrainement","title":"Modification E : Augmenter le nombre d'\u00e9poques d'entra\u00eenement","text":"<pre><code># Entra\u00eener le mod\u00e8le plus longtemps\nhistory = model.fit(\n    X_train, y_train_cat,\n    epochs=5,  # 5 au lieu de 3\n    batch_size=128,\n    validation_split=0.2,\n    verbose=1\n)\n</code></pre>"},{"location":"module1/mini-projet/#etape-3-analyse-des-resultats-15-min","title":"\u00c9tape 3 : Analyse des r\u00e9sultats (15 min)","text":"<p>Pour analyser l'impact de vos modifications, ajoutez ce code \u00e0 votre notebook :</p> <pre><code># Visualiser quelques pr\u00e9dictions\ndef plot_predictions(model, X, y_true, n=10):\n    predictions = model.predict(X[:n])\n    pred_classes = np.argmax(predictions, axis=1)\n    true_classes = np.argmax(y_true[:n], axis=1)\n\n    plt.figure(figsize=(15, 3))\n    for i in range(n):\n        plt.subplot(1, n, i+1)\n        plt.imshow(X[i].reshape(28, 28), cmap='gray')\n\n        if pred_classes[i] == true_classes[i]:\n            color = 'green'\n        else:\n            color = 'red'\n\n        plt.title(f\"Vrai: {true_classes[i]}\\nPr\u00e9dit: {pred_classes[i]}\", color=color)\n        plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n\n# Tester avec des donn\u00e9es normales\nplot_predictions(model, X_test, y_test_cat)\n\n# Tester avec des donn\u00e9es bruit\u00e9es\nX_test_noisy = X_test + np.random.normal(0, 0.1, X_test.shape)\nX_test_noisy = np.clip(X_test_noisy, 0, 1)\nplot_predictions(model, X_test_noisy, y_test_cat)\n</code></pre>"},{"location":"module1/mini-projet/#elements-a-documenter-dans-votre-fiche-dobservations","title":"\ud83d\udcca \u00c9l\u00e9ments \u00e0 documenter dans votre fiche d'observations","text":"<p>Sur votre fiche d'observations (\u00e0 t\u00e9l\u00e9charger au d\u00e9but du TP), vous devrez remplir :</p> <ol> <li>Mod\u00e8le de base</li> <li>Architecture (nombre de couches et de neurones)</li> <li>Performance obtenue (pr\u00e9cision sur les donn\u00e9es de test)</li> <li> <p>Analyse des courbes d'entra\u00eenement</p> </li> <li> <p>Modifications effectu\u00e9es</p> </li> <li>Description de chaque modification</li> <li> <p>Justification de votre choix</p> </li> <li> <p>R\u00e9sultats obtenus</p> </li> <li>Performance apr\u00e8s chaque modification</li> <li>Comparaison avec le mod\u00e8le de base</li> <li> <p>Comportement avec les donn\u00e9es bruit\u00e9es</p> </li> <li> <p>Analyse et observations</p> </li> <li>Impact de chaque modification</li> <li>Types d'erreurs observ\u00e9es</li> <li>Votre interpr\u00e9tation des r\u00e9sultats</li> </ol>"},{"location":"module1/mini-projet/#conseils","title":"\ud83d\udca1 Conseils","text":"<ul> <li>Testez les modifications une par une</li> <li>Prenez des notes sur chaque modification dans votre fiche d'observations</li> <li>Observez attentivement les courbes d'apprentissage et les pr\u00e9dictions</li> </ul> <p>Retour au Module 1 Continuer vers l'Auto-\u00e9valuation</p>"},{"location":"module1/qcm-evaluation-module1/","title":"\ud83d\udcdd QCM d'auto-\u00e9valuation - Module 1 : Fondamentaux du Deep Learning","text":"<p>Ce QCM vous permettra d'\u00e9valuer votre compr\u00e9hension des concepts fondamentaux du Deep Learning vus durant cette premi\u00e8re s\u00e9ance.</p>"},{"location":"module1/qcm-evaluation-module1/#instructions","title":"\u2705 Instructions","text":"<ul> <li>Cochez la ou les r\u00e9ponses correctes pour chaque question</li> <li>Certaines questions peuvent avoir plusieurs r\u00e9ponses correctes</li> <li>Pour les questions \u00e0 choix multiples, 0,5 point est attribu\u00e9 par r\u00e9ponse correcte (maximum 1 point par question)</li> <li>\u00c0 la fin du questionnaire, calculez votre score gr\u00e2ce au corrig\u00e9 fourni</li> <li>Dur\u00e9e recommand\u00e9e : 20 minutes</li> </ul>"},{"location":"module1/qcm-evaluation-module1/#partie-1-introduction-pratique","title":"\ud83d\udd0d Partie 1 : Introduction pratique","text":""},{"location":"module1/qcm-evaluation-module1/#1-dans-le-hello-world-du-deep-learning-avec-mnist-que-representent-les-donnees-dentree","title":"1. Dans le \"Hello World\" du Deep Learning avec MNIST, que repr\u00e9sentent les donn\u00e9es d'entr\u00e9e ?","text":"<ul> <li> Des \u00e9chantillons de texte manuscrit</li> <li> Des images de chiffres manuscrits de 0 \u00e0 9</li> <li> Des enregistrements audio de chiffres prononc\u00e9s</li> <li> Des coordonn\u00e9es de points repr\u00e9sentant des chiffres</li> </ul>"},{"location":"module1/qcm-evaluation-module1/#2-lors-de-la-normalisation-des-donnees-dimage-mnist-pourquoi-divise-t-on-les-valeurs-des-pixels-par-255","title":"2. Lors de la normalisation des donn\u00e9es d'image MNIST, pourquoi divise-t-on les valeurs des pixels par 255 ?","text":"<ul> <li> Pour compresser les images et \u00e9conomiser de la m\u00e9moire</li> <li> Pour ramener toutes les valeurs entre 0 et 1</li> <li> Pour augmenter la vitesse de traitement</li> <li> Pour convertir les images en noir et blanc</li> </ul>"},{"location":"module1/qcm-evaluation-module1/#3-parmi-ces-applications-laquelle-nest-pas-un-exemple-typique-de-deep-learning-presente-dans-lintroduction-pratique","title":"3. Parmi ces applications, laquelle n'est PAS un exemple typique de Deep Learning pr\u00e9sent\u00e9 dans l'introduction pratique ?","text":"<ul> <li> GitHub Copilot pour la compl\u00e9tion de code</li> <li> Reconnaissance d'objets en temps r\u00e9el </li> <li> G\u00e9n\u00e9ration de texte contextuel</li> <li> Analyse statistique de donn\u00e9es tabulaires</li> </ul>"},{"location":"module1/qcm-evaluation-module1/#4-lors-de-lexperimentation-avec-le-modele-mnist-quel-parametre-a-le-plus-dinfluence-sur-le-temps-dentrainement","title":"4. Lors de l'exp\u00e9rimentation avec le mod\u00e8le MNIST, quel param\u00e8tre a le plus d'influence sur le temps d'entra\u00eenement ?","text":"<ul> <li> Le nombre d'\u00e9poques</li> <li> La taille du batch</li> <li> Le type de fonction d'activation</li> <li> Le nombre de classes de sortie</li> </ul>"},{"location":"module1/qcm-evaluation-module1/#5-quels-sont-les-avantages-observes-du-deep-learning-dans-votre-premiere-experience-pratique-plusieurs-reponses-possibles","title":"5. Quels sont les avantages observ\u00e9s du Deep Learning dans votre premi\u00e8re exp\u00e9rience pratique ? (plusieurs r\u00e9ponses possibles)","text":"<ul> <li> Capacit\u00e9 \u00e0 traiter directement des images brutes</li> <li> Pas besoin de pr\u00e9traitement des donn\u00e9es</li> <li> Apprentissage automatique des caract\u00e9ristiques importantes</li> <li> Reconnaissance robuste malgr\u00e9 des variations dans les entr\u00e9es</li> <li> Facilit\u00e9 d'impl\u00e9mentation et d'entra\u00eenement</li> </ul>"},{"location":"module1/qcm-evaluation-module1/#partie-2-concepts-fondamentaux","title":"\ud83e\udde9 Partie 2 : Concepts fondamentaux","text":""},{"location":"module1/qcm-evaluation-module1/#6-quelle-est-la-principale-difference-entre-le-machine-learning-classique-et-le-deep-learning-concernant-les-caracteristiques-features","title":"6. Quelle est la principale diff\u00e9rence entre le Machine Learning classique et le Deep Learning concernant les caract\u00e9ristiques (features) ?","text":"<ul> <li> Le Machine Learning classique fonctionne avec moins de donn\u00e9es</li> <li> Le Deep Learning extrait automatiquement les caract\u00e9ristiques pertinentes</li> <li> Le Machine Learning classique ne n\u00e9cessite pas de phase d'entra\u00eenement</li> <li> Le Deep Learning demande moins de puissance de calcul</li> </ul>"},{"location":"module1/qcm-evaluation-module1/#7-quels-sont-les-composants-fondamentaux-dun-reseau-de-neurones-plusieurs-reponses-possibles","title":"7. Quels sont les composants fondamentaux d'un r\u00e9seau de neurones ? (plusieurs r\u00e9ponses possibles)","text":"<ul> <li> Neurones</li> <li> Poids et connexions</li> <li> Fonctions d'activation</li> <li> Instructions conditionnelles</li> <li> Biais</li> </ul>"},{"location":"module1/qcm-evaluation-module1/#8-dans-un-reseau-de-neurones-quest-ce-quune-couche-cachee","title":"8. Dans un r\u00e9seau de neurones, qu'est-ce qu'une \"couche cach\u00e9e\" ?","text":"<ul> <li> Une couche qui n'est pas visible dans le code</li> <li> Une couche situ\u00e9e entre la couche d'entr\u00e9e et la couche de sortie</li> <li> Une couche qui ne s'active que dans certaines conditions</li> <li> Une couche utilis\u00e9e uniquement pendant la phase de test</li> </ul>"},{"location":"module1/qcm-evaluation-module1/#9-en-observant-le-schema-dun-neurone-artificiel-quelles-operations-mathematiques-sont-appliquees-dans-lordre-correct","title":"9. En observant le sch\u00e9ma d'un neurone artificiel, quelles op\u00e9rations math\u00e9matiques sont appliqu\u00e9es dans l'ordre correct ?","text":"<ul> <li> Multiplication \u2192 Addition \u2192 Fonction d'activation</li> <li> Addition \u2192 Multiplication \u2192 Fonction d'activation</li> <li> Fonction d'activation \u2192 Multiplication \u2192 Addition</li> <li> Multiplication \u2192 Fonction d'activation \u2192 Addition</li> </ul>"},{"location":"module1/qcm-evaluation-module1/#10-lors-de-la-comparaison-entre-machine-learning-classique-et-deep-learning-sur-des-donnees-bruitees-quavez-vous-observe","title":"10. Lors de la comparaison entre Machine Learning classique et Deep Learning sur des donn\u00e9es bruit\u00e9es, qu'avez-vous observ\u00e9 ?","text":"<ul> <li> Les deux approches ont des performances similaires</li> <li> Le Machine Learning classique est plus robuste au bruit</li> <li> Le Deep Learning maintient g\u00e9n\u00e9ralement de meilleures performances</li> <li> Les deux approches \u00e9chouent compl\u00e8tement avec des donn\u00e9es bruit\u00e9es</li> </ul>"},{"location":"module1/qcm-evaluation-module1/#partie-3-mini-projet-individuel","title":"\ud83d\udee0\ufe0f Partie 3 : Mini-projet individuel","text":""},{"location":"module1/qcm-evaluation-module1/#11-dans-le-mini-projet-quelle-modification-a-generalement-le-plus-dimpact-positif-sur-les-performances-du-modele-cnn","title":"11. Dans le mini-projet, quelle modification a g\u00e9n\u00e9ralement le plus d'impact positif sur les performances du mod\u00e8le CNN ?","text":"<ul> <li> Ajouter une couche de convolution suppl\u00e9mentaire</li> <li> Augmenter simplement le nombre de neurones dans les couches existantes</li> <li> Changer l'optimiseur d'Adam \u00e0 SGD</li> <li> R\u00e9duire le nombre d'\u00e9poques d'entra\u00eenement</li> </ul>"},{"location":"module1/qcm-evaluation-module1/#12-quel-est-leffet-principal-de-lajout-dune-couche-de-dropout-dans-un-modele-de-deep-learning","title":"12. Quel est l'effet principal de l'ajout d'une couche de Dropout dans un mod\u00e8le de Deep Learning ?","text":"<ul> <li> Acc\u00e9l\u00e9ration de l'entra\u00eenement</li> <li> R\u00e9duction du surapprentissage (overfitting)</li> <li> Am\u00e9lioration des performances sur les donn\u00e9es complexes</li> <li> Simplification de l'architecture du r\u00e9seau</li> </ul>"},{"location":"module1/qcm-evaluation-module1/#13-analysez-ce-graphique-dentrainement-quelle-affirmation-est-correcte","title":"13. Analysez ce graphique d'entra\u00eenement. Quelle affirmation est correcte ?","text":"<pre><code>Pr\u00e9cision\n^\n|\n|      ****     *******\n|    **               ******\n|   *                        ******\n|  *\n| *\n|*\n+---------------------------------&gt; \u00c9poques\n  \u2014 Entra\u00eenement   .... Validation\n</code></pre> <ul> <li> Le mod\u00e8le n'apprend pas correctement</li> <li> Le mod\u00e8le souffre de surapprentissage</li> <li> Le mod\u00e8le souffre de sous-apprentissage</li> <li> Le mod\u00e8le g\u00e9n\u00e9ralise parfaitement</li> </ul>"},{"location":"module1/qcm-evaluation-module1/#14-lors-du-test-du-modele-sur-des-donnees-bruitees-quelle-modification-tend-a-ameliorer-le-plus-la-robustesse","title":"14. Lors du test du mod\u00e8le sur des donn\u00e9es bruit\u00e9es, quelle modification tend \u00e0 am\u00e9liorer le plus la robustesse ?","text":"<ul> <li> Augmentation du nombre d'\u00e9poques</li> <li> Ajout de couches de Dropout</li> <li> R\u00e9duction du nombre de neurones</li> <li> Changement de la fonction d'activation</li> </ul>"},{"location":"module1/qcm-evaluation-module1/#15-quelle-relation-decrit-le-mieux-le-lien-entre-les-trois-phases-du-module-1","title":"15. Quelle relation d\u00e9crit le mieux le lien entre les trois phases du Module 1 ?","text":"<ul> <li> Chaque phase est ind\u00e9pendante et peut \u00eatre \u00e9tudi\u00e9e s\u00e9par\u00e9ment</li> <li> La Phase 1 fournit l'exp\u00e9rience pratique, la Phase 2 explique les concepts, et la Phase 3 applique ces connaissances</li> <li> Les phases doivent obligatoirement \u00eatre suivies dans l'ordre pour comprendre le Deep Learning</li> <li> Les phases repr\u00e9sentent trois approches alternatives pour apprendre le Deep Learning</li> </ul>"},{"location":"module1/qcm-evaluation-module1/#auto-evaluation","title":"Auto-\u00e9valuation","text":"<p>Une fois le QCM compl\u00e9t\u00e9, v\u00e9rifiez vos r\u00e9ponses avec le corrig\u00e9 ci-dessous et calculez votre score.</p>"},{"location":"module1/qcm-evaluation-module1/#corrige-avec-explications","title":"Corrig\u00e9 avec explications","text":"<ol> <li> <p>b - Des images de chiffres manuscrits de 0 \u00e0 9 Le dataset MNIST contient 70 000 images en niveaux de gris de chiffres manuscrits, format standard pour d\u00e9buter en Deep Learning.</p> </li> <li> <p>b - Pour ramener toutes les valeurs entre 0 et 1 La normalisation des valeurs de pixels (qui sont initialement entre 0 et 255) permet de stabiliser l'entra\u00eenement et d'acc\u00e9l\u00e9rer la convergence du mod\u00e8le.</p> </li> <li> <p>d - Analyse statistique de donn\u00e9es tabulaires C'est typiquement un cas o\u00f9 le Machine Learning classique est plus appropri\u00e9 que le Deep Learning. Les autres options sont des applications typiques de Deep Learning pr\u00e9sent\u00e9es dans l'introduction.</p> </li> <li> <p>a - Le nombre d'\u00e9poques Une \u00e9poque repr\u00e9sente un passage complet sur toutes les donn\u00e9es d'entra\u00eenement. Augmenter le nombre d'\u00e9poques multiplie proportionnellement le temps d'entra\u00eenement.</p> </li> <li> <p>a, c, d - Capacit\u00e9 \u00e0 traiter directement des images brutes, Apprentissage automatique des caract\u00e9ristiques importantes, Reconnaissance robuste malgr\u00e9 des variations dans les entr\u00e9es Le Deep Learning requiert g\u00e9n\u00e9ralement un pr\u00e9traitement (normalisation), donc b est incorrect. Il n'est pas n\u00e9cessairement plus facile \u00e0 impl\u00e9menter que le ML classique, donc e est incorrect.</p> </li> <li> <p>b - Le Deep Learning extrait automatiquement les caract\u00e9ristiques pertinentes C'est la diff\u00e9rence fondamentale : le ML classique n\u00e9cessite une extraction manuelle des caract\u00e9ristiques alors que le DL les apprend automatiquement \u00e0 partir des donn\u00e9es brutes.</p> </li> <li> <p>a, b, c, e - Neurones, Poids et connexions, Fonctions d'activation, Biais Les instructions conditionnelles ne font pas partie de la structure standard d'un r\u00e9seau de neurones.</p> </li> <li> <p>b - Une couche situ\u00e9e entre la couche d'entr\u00e9e et la couche de sortie Les couches cach\u00e9es sont responsables de l'extraction progressive des caract\u00e9ristiques et sont situ\u00e9es entre l'entr\u00e9e et la sortie du r\u00e9seau.</p> </li> <li> <p>a - Multiplication \u2192 Addition \u2192 Fonction d'activation Dans un neurone artificiel, on multiplie d'abord les entr\u00e9es par les poids, puis on additionne ces produits avec le biais, et enfin on applique la fonction d'activation.</p> </li> <li> <p>c - Le Deep Learning maintient g\u00e9n\u00e9ralement de meilleures performances Gr\u00e2ce \u00e0 sa capacit\u00e9 \u00e0 extraire des caract\u00e9ristiques hi\u00e9rarchiques complexes, le Deep Learning est souvent plus robuste aux variations et au bruit dans les donn\u00e9es.</p> </li> <li> <p>a - Ajouter une couche de convolution suppl\u00e9mentaire Cette modification permet au r\u00e9seau d'extraire des caract\u00e9ristiques plus complexes et plus abstraites, am\u00e9liorant g\u00e9n\u00e9ralement les performances sur MNIST.</p> </li> <li> <p>b - R\u00e9duction du surapprentissage (overfitting) Le Dropout d\u00e9sactive al\u00e9atoirement des neurones pendant l'entra\u00eenement, ce qui emp\u00eache le r\u00e9seau de trop s'adapter aux donn\u00e9es d'entra\u00eenement et am\u00e9liore la g\u00e9n\u00e9ralisation.</p> </li> <li> <p>b - Le mod\u00e8le souffre de surapprentissage Le graphique montre que la pr\u00e9cision sur les donn\u00e9es d'entra\u00eenement continue d'augmenter alors que celle sur les donn\u00e9es de validation commence \u00e0 diminuer, signe classique de surapprentissage.</p> </li> <li> <p>b - Ajout de couches de Dropout Le Dropout am\u00e9liore la robustesse du mod\u00e8le en le for\u00e7ant \u00e0 ne pas d\u00e9pendre excessivement de certains neurones, ce qui le rend plus performant sur des donn\u00e9es bruit\u00e9es ou l\u00e9g\u00e8rement diff\u00e9rentes.</p> </li> <li> <p>b - La Phase 1 fournit l'exp\u00e9rience pratique, la Phase 2 explique les concepts, et la Phase 3 applique ces connaissances Cette structure suit l'approche p\u00e9dagogique du module : pratique d'abord, conceptualisation ensuite, et application finale dans un projet.</p> </li> </ol>"},{"location":"module1/qcm-evaluation-module1/#calcul-de-votre-score","title":"Calcul de votre score","text":"<ul> <li>Questions \u00e0 choix unique (1-4, 6, 8-15) : 1 point par r\u00e9ponse correcte</li> <li>Questions \u00e0 choix multiples (5, 7) : 0,5 point par r\u00e9ponse correcte et -0,25 par r\u00e9ponse incorrecte (minimum 0, maximum 1 point par question)</li> </ul> <p>Total des points possibles : 15</p>"},{"location":"module1/qcm-evaluation-module1/#interpretation","title":"Interpr\u00e9tation","text":"<ul> <li>12-15 points : Excellente ma\u00eetrise des concepts fondamentaux du Deep Learning</li> <li>9-11 points : Bonne compr\u00e9hension, quelques points \u00e0 clarifier</li> <li>6-8 points : Compr\u00e9hension de base, r\u00e9vision n\u00e9cessaire de certains concepts</li> <li>0-5 points : R\u00e9vision approfondie recommand\u00e9e avant de poursuivre</li> </ul>"},{"location":"module1/qcm-evaluation-module1/#pour-approfondir","title":"Pour approfondir","text":"<p>Si vous avez obtenu moins de 12 points, nous vous recommandons de revoir les concepts sur lesquels vous avez fait des erreurs. Consultez les ressources suivantes :</p> <ul> <li>Le notebook \"Hello World du Deep Learning\" (Phase 1)</li> <li>La section \"Concepts fondamentaux\" du cours (Phase 2)</li> <li>La \"fiche d'observations du mini-projet d'am\u00e9lioration\" (Phase 3)</li> <li>Le glossaire des termes du Deep Learning</li> </ul>"},{"location":"module1/ressources/Module1-fiche-enseignante-avec-corrections/","title":"\ud83d\udccb Fiche Enseignante - Module 1 : Fondamentaux du Deep Learning","text":""},{"location":"module1/ressources/Module1-fiche-enseignante-avec-corrections/#presentation-generale-du-module","title":"Pr\u00e9sentation g\u00e9n\u00e9rale du module","text":"<p>Dur\u00e9e totale : 4 heures Public cible : \u00c9tudiants BTS SIO (d\u00e9butants en Deep Learning) Pr\u00e9requis : Bases en programmation Python, compte Google pour Colab</p> <p>Approche p\u00e9dagogique : Apprentissage par la pratique d'abord, conceptualisation ensuite</p>"},{"location":"module1/ressources/Module1-fiche-enseignante-avec-corrections/#objectifs-dapprentissage","title":"Objectifs d'apprentissage","text":"<p>\u00c0 l'issue de ce module, les \u00e9tudiants seront capables de : 1. Manipuler un r\u00e9seau de neurones simple via TensorFlow/Keras 2. Distinguer les diff\u00e9rences entre Machine Learning classique et Deep Learning 3. Comprendre les concepts fondamentaux (couches, fonction d'activation, propagation) 4. Modifier et analyser un mod\u00e8le de Deep Learning simple</p>"},{"location":"module1/ressources/Module1-fiche-enseignante-avec-corrections/#organisation-du-module","title":"Organisation du module","text":""},{"location":"module1/ressources/Module1-fiche-enseignante-avec-corrections/#phase-1-introduction-pratique-1h","title":"Phase 1 : Introduction pratique (1h)","text":""},{"location":"module1/ressources/Module1-fiche-enseignante-avec-corrections/#objectifs-specifiques","title":"Objectifs sp\u00e9cifiques","text":"<ul> <li>Cr\u00e9er un premier contact positif avec le Deep Learning</li> <li>Manipuler un r\u00e9seau de neurones sans barri\u00e8re th\u00e9orique pr\u00e9alable</li> <li>Observer concr\u00e8tement le fonctionnement et les performances d'un mod\u00e8le</li> </ul>"},{"location":"module1/ressources/Module1-fiche-enseignante-avec-corrections/#deroulement-et-conseils-danimation","title":"D\u00e9roulement et conseils d'animation","text":"Dur\u00e9e Activit\u00e9 Conseils pour l'enseignant 15 min D\u00e9monstrations applications DL\u2022 GitHub Copilot\u2022 Reconnaissance d'objets\u2022 G\u00e9n\u00e9ration de texte \u2022 Choisir des exemples spectaculaires et accessibles\u2022 Associer les \u00e9tudiants via questions\u2022 \u00c9tablir des liens avec des applications r\u00e9elles 30 min Manipulation guid\u00e9e notebook \"Hello World\"\u2022 Ex\u00e9cution pas \u00e0 pas\u2022 Observation des performances\u2022 Premi\u00e8res exp\u00e9rimentations \u2022 S'assurer que tous les \u00e9tudiants ont acc\u00e8s \u00e0 Colab\u2022 Circuler pour aider aux probl\u00e8mes techniques\u2022 Encourager l'observation et le questionnement 15 min Remplissage fiche d'observations\u2022 Analyse des r\u00e9sultats\u2022 Documentation des observations \u2022 Insister sur l'importance de la documentation\u2022 Encourager la pr\u00e9cision des observations\u2022 Pr\u00e9voir un temps de mise en commun"},{"location":"module1/ressources/Module1-fiche-enseignante-avec-corrections/#elements-de-correction-pour-la-fiche-dobservations-phase-1","title":"\u00c9l\u00e9ments de correction pour la Fiche d'observations - Phase 1","text":"Question \u00c9l\u00e9ments de r\u00e9ponse attendus Version de TensorFlow d\u00e9tect\u00e9e TensorFlow 2.x (la version exacte d\u00e9pend de Colab) GPU disponible ? Oui (g\u00e9n\u00e9ralement dans Colab) Importance du GPU pour le Deep Learning Acc\u00e9l\u00e9ration consid\u00e9rable de l'entra\u00eenement gr\u00e2ce au calcul parall\u00e8le des GPU, permettant de traiter des mod\u00e8les plus grands et plus complexes Nombre d'exemples d'entra\u00eenement 60 000 images Nombre d'exemples de test 10 000 images Dimension des images 28\u00d728 pixels (784 pixels au total) Pourquoi normalise-t-on les valeurs? Pour ramener toutes les valeurs entre 0 et 1, ce qui facilite la convergence du mod\u00e8le et \u00e9vite les probl\u00e8mes num\u00e9riques Difficult\u00e9s potentielles Variabilit\u00e9 des styles d'\u00e9criture, similarit\u00e9 entre certains chiffres (ex: 1 et 7, 3 et 8), qualit\u00e9 variable du trac\u00e9, positionnement non centr\u00e9 Nombre de couches du mod\u00e8le 4-5 couches (entr\u00e9e, convolutions, pooling, dense, sortie) Nombre total de param\u00e8tres Entre 500 000 et 1 500 000 selon l'architecture exacte R\u00f4le des couches de convolution Extraction de caract\u00e9ristiques locales (bords, contours, motifs) ind\u00e9pendamment de leur position R\u00f4le des couches de pooling R\u00e9duction de dimension, invariance aux petites translations, abstraction des caract\u00e9ristiques Pourquoi utiliser 'softmax'? Pour obtenir une distribution de probabilit\u00e9s sur les 10 classes (somme = 1) Nombre d'\u00e9poques 5-10 g\u00e9n\u00e9ralement Pr\u00e9cision sur donn\u00e9es d'entra\u00eenement ~99% Pr\u00e9cision sur donn\u00e9es de validation ~98% Pr\u00e9cision sur l'ensemble de test ~97-98% Signes de surapprentissage \u00c9cart entre pr\u00e9cision d'entra\u00eenement et de validation qui se creuse au fil des \u00e9poques La courbe de pr\u00e9cision est-elle croissante? Oui, avec une augmentation rapide au d\u00e9but puis une stabilisation La courbe de perte est-elle d\u00e9croissante? Oui, avec une diminution rapide au d\u00e9but puis une stabilisation \u00c9cart entre courbes d'entra\u00eenement et validation Faible \u00e0 mod\u00e9r\u00e9, ce qui indique une bonne g\u00e9n\u00e9ralisation Entra\u00eenement suffisant? Oui, si les courbes se stabilisent. Plus d'\u00e9poques n'apporterait que peu d'am\u00e9lioration"},{"location":"module1/ressources/Module1-fiche-enseignante-avec-corrections/#phase-2-concepts-fondamentaux-1h30","title":"Phase 2 : Concepts fondamentaux (1h30)","text":""},{"location":"module1/ressources/Module1-fiche-enseignante-avec-corrections/#objectifs-specifiques_1","title":"Objectifs sp\u00e9cifiques","text":"<ul> <li>Comprendre les diff\u00e9rences entre ML classique et Deep Learning</li> <li>Explorer l'anatomie d'un r\u00e9seau de neurones</li> <li>Saisir les concepts de forward/backward propagation</li> </ul>"},{"location":"module1/ressources/Module1-fiche-enseignante-avec-corrections/#deroulement-et-conseils-danimation_1","title":"D\u00e9roulement et conseils d'animation","text":"Dur\u00e9e Activit\u00e9 Conseils pour l'enseignant 30 min Comparaison ML/DL\u2022 Exploration des deux approches\u2022 Test sur donn\u00e9es normales/bruit\u00e9es \u2022 Insister sur les diff\u00e9rences fondamentales (features engineered vs learned)\u2022 Laisser les \u00e9tudiants d\u00e9couvrir par eux-m\u00eames les forces/faiblesses 45 min Exploration interactive\u2022 Neurone unique\u2022 R\u00e9seau simple\u2022 Visualisation de l'entra\u00eenement \u2022 Utiliser des analogies concr\u00e8tes (ex: neurone comme d\u00e9tecteur de motifs)\u2022 Favoriser la manipulation et l'exp\u00e9rimentation\u2022 Poser des questions guid\u00e9es pour la r\u00e9flexion 15 min Sch\u00e9ma conceptuel\u2022 Compl\u00e9tion collaborative\u2022 Discussion des concepts \u2022 Synth\u00e9tiser les observations\u2022 Formaliser progressivement les concepts\u2022 V\u00e9rifier la compr\u00e9hension par des questions"},{"location":"module1/ressources/Module1-fiche-enseignante-avec-corrections/#elements-de-correction-pour-la-fiche-dobservations-phase-2","title":"\u00c9l\u00e9ments de correction pour la Fiche d'observations - Phase 2","text":"<p>Comparaison Machine Learning vs Deep Learning</p> Aspect observ\u00e9 Machine Learning (Random Forest) Deep Learning (CNN) Pr\u00e9paration des donn\u00e9es N\u00e9cessite un pr\u00e9traitement important et une extraction manuelle de caract\u00e9ristiques Travaille directement sur les donn\u00e9es brutes (pixels) Extraction de caract\u00e9ristiques Manuelle, n\u00e9cessite expertise du domaine Automatique, apprend les caract\u00e9ristiques pertinentes Temps d'entra\u00eenement Relativement rapide (quelques secondes \u00e0 minutes) Plus long (minutes \u00e0 heures), n\u00e9cessite souvent un GPU Pr\u00e9cision globale Bonne (~95-96%) Excellente (~98-99%) Pr\u00e9cision sur donn\u00e9es bruit\u00e9es Faible \u00e0 moyenne, sensible au bruit Bonne, plus robuste aux variations Pr\u00e9cision sur donn\u00e9es avec rotation Tr\u00e8s faible, ne g\u00e8re pas les rotations Mod\u00e9r\u00e9e \u00e0 bonne selon l'entra\u00eenement Facilit\u00e9 d'impl\u00e9mentation Plus simple, moins de param\u00e8tres \u00e0 r\u00e9gler Plus complexe, plus d'hyperparam\u00e8tres Interpr\u00e9tabilit\u00e9 Plus interpr\u00e9table (r\u00e8gles de d\u00e9cision explicites) Moins interpr\u00e9table (\"bo\u00eete noire\") Capacit\u00e9 de g\u00e9n\u00e9ralisation Limit\u00e9e aux caract\u00e9ristiques explicites Meilleure sur des motifs complexes et variations <p>Sch\u00e9ma conceptuel du r\u00e9seau de neurones</p> <ol> <li>Couche d'entr\u00e9e (Input Layer)</li> <li>Premi\u00e8re couche cach\u00e9e (Hidden Layer 1)</li> <li>Deuxi\u00e8me couche cach\u00e9e (Hidden Layer 2)</li> <li>Couche de sortie (Output Layer)</li> <li>Pr\u00e9diction (Prediction)</li> <li>Calcul de l'erreur (Loss Calculation)</li> <li>Donn\u00e9es r\u00e9elles (Ground Truth)</li> </ol> <p>Structure du r\u00e9seau</p> <ol> <li> <p>Type de r\u00e9seau repr\u00e9sent\u00e9: R\u00e9seau de neurones multicouche (MLP) ou perceptron multicouche</p> </li> <li> <p>Nombre de neurones pour MNIST:</p> </li> <li>Couche d'entr\u00e9e: 784 (28\u00d728 pixels)</li> <li>Premi\u00e8re couche cach\u00e9e: 128-512 (variable selon architecture)</li> <li>Deuxi\u00e8me couche cach\u00e9e: 64-256 (variable selon architecture)</li> <li> <p>Couche de sortie: 10 (un neurone par chiffre 0-9)</p> </li> <li> <p>Fonctions d'activation appropri\u00e9es:</p> </li> <li>Couches cach\u00e9es: ReLU (Rectified Linear Unit)</li> <li>Couche de sortie: Softmax (pour obtenir des probabilit\u00e9s)</li> </ol> <p>Processus d'apprentissage</p> <ol> <li>Forward propagation: Les donn\u00e9es d'entr\u00e9e sont propag\u00e9es \u00e0 travers le r\u00e9seau pour produire une pr\u00e9diction</li> <li>Calcul de l'erreur: Comparaison entre la pr\u00e9diction et la valeur r\u00e9elle (ground truth)</li> <li>Backward propagation: L'erreur est propag\u00e9e en arri\u00e8re pour calculer les gradients</li> <li>Mise \u00e0 jour des poids: Les param\u00e8tres du r\u00e9seau sont ajust\u00e9s pour minimiser l'erreur</li> </ol>"},{"location":"module1/ressources/Module1-fiche-enseignante-avec-corrections/#phase-3-mini-projet-individuel-1h","title":"Phase 3 : Mini-projet individuel (1h)","text":""},{"location":"module1/ressources/Module1-fiche-enseignante-avec-corrections/#objectifs-specifiques_2","title":"Objectifs sp\u00e9cifiques","text":"<ul> <li>Appliquer les connaissances acquises</li> <li>D\u00e9velopper une d\u00e9marche d'am\u00e9lioration m\u00e9thodique</li> <li>Analyser l'impact des modifications</li> </ul>"},{"location":"module1/ressources/Module1-fiche-enseignante-avec-corrections/#deroulement-et-conseils-danimation_2","title":"D\u00e9roulement et conseils d'animation","text":"Dur\u00e9e Activit\u00e9 Conseils pour l'enseignant 15 min Pr\u00e9paration mod\u00e8le de base\u2022 Configuration notebook\u2022 Analyse du mod\u00e8le initial \u2022 Fournir le code de base pr\u00eat \u00e0 l'emploi\u2022 Expliquer clairement la structure du projet\u2022 Rappeler les points d'observation importants 30 min Exp\u00e9rimentation modifications\u2022 Impl\u00e9mentation des changements\u2022 Tests et comparaisons \u2022 Sugg\u00e9rer des modifications adapt\u00e9es au niveau\u2022 Encourager la d\u00e9marche scientifique (hypoth\u00e8se\u2192test\u2192analyse)\u2022 Circuler pour guider sans trop diriger 15 min Analyse des r\u00e9sultats\u2022 Documentation des observations\u2022 R\u00e9flexion sur les am\u00e9liorations \u2022 Rappeler l'importance de l'analyse critique\u2022 Encourager la comparaison entre \u00e9tudiants\u2022 Valoriser les d\u00e9marches originales"},{"location":"module1/ressources/Module1-fiche-enseignante-avec-corrections/#elements-de-correction-pour-la-fiche-dobservations-phase-3","title":"\u00c9l\u00e9ments de correction pour la Fiche d'observations - Phase 3","text":"<p>Mod\u00e8le de base</p> \u00c9l\u00e9ment R\u00e9ponse attendue Architecture CNN simple avec 1-2 couches de convolution, 1-2 couches de pooling, 1 couche dense et 1 couche de sortie Nombre de param\u00e8tres ~500 000 pour le mod\u00e8le de base propos\u00e9 Fonction d'activation ReLU pour les couches interm\u00e9diaires, Softmax pour la sortie Optimiseur Adam Pr\u00e9cision du mod\u00e8le de base ~97-98% sur l'ensemble de test <p>Modifications et leurs impacts</p> Modification Impact attendu Ajout d'une couche de convolution Augmentation de la capacit\u00e9 \u00e0 d\u00e9tecter des motifs plus complexes. Am\u00e9lioration potentielle de ~0.5-1% de pr\u00e9cision. Augmentation du nombre de neurones Augmentation de la capacit\u00e9 du mod\u00e8le mais risque de surapprentissage. Am\u00e9lioration variable selon le niveau de r\u00e9gularisation. Ajout de Dropout R\u00e9duction du surapprentissage, possiblement plus robuste. Peut r\u00e9duire l\u00e9g\u00e8rement la performance sur les donn\u00e9es d'entra\u00eenement mais am\u00e9liorer sur les donn\u00e9es de test. Changement d'optimiseur SGD plus lent \u00e0 converger qu'Adam mais parfois plus stable. Performance finale similaire mais n\u00e9cessite plus d'\u00e9poques. Augmentation du nombre d'\u00e9poques Am\u00e9lioration des performances jusqu'\u00e0 un plateau. Au-del\u00e0, risque de surapprentissage. <p>Test sur donn\u00e9es bruit\u00e9es</p> Version Performances attendues Mod\u00e8le de base ~70-80% sur donn\u00e9es bruit\u00e9es Mod\u00e8le avec plus de filtres ~75-85% sur donn\u00e9es bruit\u00e9es Mod\u00e8le avec Dropout ~80-90% sur donn\u00e9es bruit\u00e9es (g\u00e9n\u00e9ralement plus robuste) <p>\u00c9valuation des analyses</p> <p>Une bonne analyse devrait inclure: - Identification correcte des modifications ayant le plus d'impact positif - Compr\u00e9hension de l'effet du Dropout sur la robustesse - Observation pertinente des types d'erreurs (ex: confusion entre 3/8, 4/9, etc.) - R\u00e9flexion sur les compromis entre complexit\u00e9 du mod\u00e8le et performances</p>"},{"location":"module1/ressources/Module1-fiche-enseignante-avec-corrections/#evaluation-et-suivi","title":"\u00c9valuation et suivi","text":""},{"location":"module1/ressources/Module1-fiche-enseignante-avec-corrections/#livrables-a-recuperer","title":"Livrables \u00e0 r\u00e9cup\u00e9rer","text":"<ul> <li>Fiche d'observations Phase 1 : \"Hello World du Deep Learning\"</li> <li>Fiche d'observations Phase 2 : \"Concepts fondamentaux\"</li> <li>Fiche d'observations Phase 3 : \"Mini-projet d'am\u00e9lioration\"</li> </ul>"},{"location":"module1/ressources/Module1-fiche-enseignante-avec-corrections/#criteres-devaluation","title":"Crit\u00e8res d'\u00e9valuation","text":"Crit\u00e8re Indicateurs de r\u00e9ussite Manipulation technique \u2022 Notebook fonctionnel\u2022 Modifications correctement impl\u00e9ment\u00e9es Compr\u00e9hension des concepts \u2022 Explication correcte des \u00e9l\u00e9ments du r\u00e9seau\u2022 Sch\u00e9ma conceptuel bien compl\u00e9t\u00e9 Analyse critique \u2022 Observations pertinentes sur les performances\u2022 Identification correcte des forces/faiblesses Documentation \u2022 Fiches d'observations compl\u00e8tes et pr\u00e9cises\u2022 Justification des choix techniques"},{"location":"module1/ressources/Module1-fiche-enseignante-avec-corrections/#bareme-suggere-sur-20-points","title":"Bar\u00e8me sugg\u00e9r\u00e9 (sur 20 points)","text":"Livrable Points \u00c9l\u00e9ments \u00e9valu\u00e9s Fiche Phase 1 6 pts \u2022 Compl\u00e9tude (3 pts)\u2022 Pertinence des observations (3 pts) Fiche Phase 2 8 pts \u2022 Tableau comparatif ML/DL (3 pts)\u2022 Sch\u00e9ma conceptuel correct (3 pts)\u2022 Processus d'apprentissage (2 pts) Fiche Phase 3 6 pts \u2022 Modifications impl\u00e9ment\u00e9es (2 pts)\u2022 Analyse des r\u00e9sultats (2 pts)\u2022 Pertinence des conclusions (2 pts)"},{"location":"module1/ressources/Module1-fiche-enseignante-avec-corrections/#ressources-et-materiel","title":"Ressources et mat\u00e9riel","text":""},{"location":"module1/ressources/Module1-fiche-enseignante-avec-corrections/#pour-lenseignant","title":"Pour l'enseignant","text":"<ul> <li>Pr\u00e9sentations des concepts cl\u00e9s (neurones, couches, fonctions d'activation)</li> <li>Solutions compl\u00e8tes des notebooks</li> <li>Exemples d'am\u00e9liorations possibles avec impact attendu</li> <li>Glossaire des erreurs courantes et leurs solutions</li> </ul>"},{"location":"module1/ressources/Module1-fiche-enseignante-avec-corrections/#pour-les-etudiants","title":"Pour les \u00e9tudiants","text":"<ul> <li>Notebooks pr\u00e9-configur\u00e9s</li> <li>Fiches d'observations \u00e0 compl\u00e9ter</li> <li>Guide d'utilisation de Google Colab</li> <li>Glossaire des termes techniques</li> </ul>"},{"location":"module1/ressources/Module1-fiche-enseignante-avec-corrections/#adaptations-possibles","title":"Adaptations possibles","text":""},{"location":"module1/ressources/Module1-fiche-enseignante-avec-corrections/#pour-les-etudiants-avances","title":"Pour les \u00e9tudiants avanc\u00e9s","text":"<ul> <li>Proposer des architectures plus complexes \u00e0 explorer</li> <li>Sugg\u00e9rer des d\u00e9fis suppl\u00e9mentaires (ex: atteindre une pr\u00e9cision cible)</li> <li>Encourager l'exploration de datasets alternatifs</li> </ul>"},{"location":"module1/ressources/Module1-fiche-enseignante-avec-corrections/#pour-les-etudiants-en-difficulte","title":"Pour les \u00e9tudiants en difficult\u00e9","text":"<ul> <li>Fournir des mod\u00e8les pr\u00e9-configur\u00e9s avec modifications \u00e0 choisir</li> <li>Proposer un travail en bin\u00f4me</li> <li>Simplifier les fiches d'observations avec plus de guidage</li> </ul>"},{"location":"module1/ressources/Module1-fiche-enseignante-avec-corrections/#points-de-vigilance-et-conseils","title":"Points de vigilance et conseils","text":""},{"location":"module1/ressources/Module1-fiche-enseignante-avec-corrections/#difficultes-techniques-courantes","title":"Difficult\u00e9s techniques courantes","text":"<ul> <li>Probl\u00e8mes d'acc\u00e8s \u00e0 Google Colab \u2192 Pr\u00e9parer un environnement de secours</li> <li>Temps d'ex\u00e9cution trop long \u2192 R\u00e9duire taille du dataset ou nombre d'\u00e9poques</li> <li>Erreurs dans le code \u2192 Pr\u00e9voir des checkpoints de code fonctionnel</li> </ul>"},{"location":"module1/ressources/Module1-fiche-enseignante-avec-corrections/#difficultes-conceptuelles-courantes","title":"Difficult\u00e9s conceptuelles courantes","text":"<ul> <li>Confusion entre les types de couches \u2192 Utiliser des analogies visuelles</li> <li>Difficult\u00e9 \u00e0 comprendre la backpropagation \u2192 Simplifier avec des exemples concrets</li> <li>Interpr\u00e9tation des m\u00e9triques \u2192 Fournir des r\u00e9f\u00e9rences de comparaison</li> </ul>"},{"location":"module1/ressources/Module1-fiche-enseignante-avec-corrections/#gestion-du-temps","title":"Gestion du temps","text":"<ul> <li>Pr\u00e9voir une marge pour les probl\u00e8mes techniques</li> <li>Adapter le nombre de modifications \u00e0 tester selon l'avancement</li> <li>Pr\u00e9parer des activit\u00e9s \"tampons\" pour les plus rapides</li> </ul>"},{"location":"module1/ressources/Module1-fiche-enseignante-avec-corrections/#prolongements-possibles","title":"Prolongements possibles","text":"<ul> <li>QCM d'auto-\u00e9valuation pour v\u00e9rifier les acquis</li> <li>Exercices compl\u00e9mentaires pour renforcer la compr\u00e9hension</li> <li>Suggestions de projets personnels pour prolonger l'apprentissage</li> </ul>"},{"location":"module1/ressources/Module1-fiche-enseignante-avec-corrections/#annexe-concepts-cles-a-aborder","title":"Annexe : Concepts cl\u00e9s \u00e0 aborder","text":""},{"location":"module1/ressources/Module1-fiche-enseignante-avec-corrections/#vocabulaire-essentiel","title":"Vocabulaire essentiel","text":"<ul> <li>Neurone artificiel</li> <li>Poids et biais</li> <li>Couches (entr\u00e9e, cach\u00e9e, sortie)</li> <li>Fonction d'activation</li> <li>Forward propagation</li> <li>Backpropagation</li> <li>Gradient descent</li> <li>Epoch (\u00e9poque)</li> <li>Batch</li> <li>Loss function</li> </ul>"},{"location":"module1/ressources/Module1-fiche-enseignante-avec-corrections/#differences-ml-vs-dl-a-souligner","title":"Diff\u00e9rences ML vs DL \u00e0 souligner","text":"<ul> <li>Extraction manuelle vs automatique des caract\u00e9ristiques</li> <li>Processus d'entra\u00eenement</li> <li>Besoins en donn\u00e9es et en calcul</li> <li>Domaines d'application privil\u00e9gi\u00e9s</li> </ul>"},{"location":"module1/ressources/Module1-fiche-enseignante-avec-corrections/#architectures-a-presenter","title":"Architectures \u00e0 pr\u00e9senter","text":"<ul> <li>Perceptron multicouche (MLP)</li> <li>R\u00e9seau convolutif (CNN) - introduction</li> <li>R\u00e9seau r\u00e9current (RNN) - mention</li> </ul>"},{"location":"module1/ressources/Module1-fiche-enseignante-avec-corrections/#annexe-faq-anticipees","title":"Annexe : FAQ anticip\u00e9es","text":"<p>Q: Pourquoi utiliser le Deep Learning plut\u00f4t que le Machine Learning classique ? R: Le Deep Learning excelle pour les donn\u00e9es complexes (images, texte, son) o\u00f9 l'extraction manuelle de caract\u00e9ristiques est difficile. Il peut apprendre des repr\u00e9sentations hi\u00e9rarchiques des donn\u00e9es.</p> <p>Q: Comment choisir le nombre de couches et de neurones ? R: C'est souvent empirique. Plus le probl\u00e8me est complexe, plus le r\u00e9seau doit \u00eatre profond. On commence g\u00e9n\u00e9ralement avec des architectures standard puis on ajuste.</p> <p>Q: Pourquoi normaliser les donn\u00e9es d'entr\u00e9e ? R: Pour homog\u00e9n\u00e9iser les \u00e9chelles et faciliter la convergence de l'entra\u00eenement. Des valeurs trop disparates peuvent causer des instabilit\u00e9s num\u00e9riques.</p> <p>Q: Comment \u00e9viter le surapprentissage (overfitting) ? R: Techniques de r\u00e9gularisation comme le dropout, l'augmentation de donn\u00e9es, l'arr\u00eat pr\u00e9coce (early stopping).</p> <p>Q: Quel mat\u00e9riel est n\u00e9cessaire pour faire du Deep Learning ? R: Pour l'apprentissage, un GPU est souvent n\u00e9cessaire. Pour ce TP, Google Colab fournit gratuitement l'acc\u00e8s \u00e0 des GPUs.</p>"},{"location":"module1/ressources/Partie1-Phase1-fiche-observations/","title":"\ud83d\udccb Fiche d'observations - Hello World du Deep Learning","text":"<p>Cette fiche d'observations vous accompagne \u00e9tape par \u00e9tape dans l'exploration du notebook. Pour chaque section, notez les r\u00e9f\u00e9rences aux cellules correspondantes du notebook.</p>"},{"location":"module1/ressources/Partie1-Phase1-fiche-observations/#informations-generales","title":"Informations g\u00e9n\u00e9rales","text":"<p>Nom et pr\u00e9nom : ____</p> <p>Date : ____</p> <p>Groupe : ____</p>"},{"location":"module1/ressources/Partie1-Phase1-fiche-observations/#partie-1-configuration-et-verification-de-lenvironnement-cellule-2","title":"Partie 1 : Configuration et v\u00e9rification de l'environnement (Cellule 2)","text":"Question Observation Version de TensorFlow d\u00e9tect\u00e9e GPU disponible ? (Oui/Non) Quelle est l'importance d'avoir un GPU pour le Deep Learning ?"},{"location":"module1/ressources/Partie1-Phase1-fiche-observations/#partie-2-chargement-et-preparation-des-donnees-cellule-3","title":"Partie 2 : Chargement et pr\u00e9paration des donn\u00e9es (Cellule 3)","text":"Question Observation Combien d'exemples d'entra\u00eenement sont disponibles ? Combien d'exemples de test sont disponibles ? Quelle est la dimension des images ? Pourquoi normalise-t-on les valeurs des pixels entre 0 et 1 ? D'apr\u00e8s les exemples affich\u00e9s, quelles difficult\u00e9s pourrait rencontrer le mod\u00e8le ?"},{"location":"module1/ressources/Partie1-Phase1-fiche-observations/#partie-3-architecture-du-modele-cellule-4","title":"Partie 3 : Architecture du mod\u00e8le (Cellule 4)","text":"<p>Dessinez le sch\u00e9ma simplifi\u00e9 de l'architecture du r\u00e9seau de neurones utilis\u00e9 :</p> <pre><code>[Sch\u00e9ma \u00e0 compl\u00e9ter]\n</code></pre> Question Observation Combien de couches comporte le mod\u00e8le ? Combien de param\u00e8tres entra\u00eenables au total ? Quel est le r\u00f4le des couches de convolution ? Quel est le r\u00f4le des couches de pooling ? Pourquoi utilise-t-on 'softmax' comme activation de la derni\u00e8re couche ?"},{"location":"module1/ressources/Partie1-Phase1-fiche-observations/#partie-4-entrainement-du-modele-cellule-5","title":"Partie 4 : Entra\u00eenement du mod\u00e8le (Cellule 5)","text":"Question Observation Combien d'\u00e9poques ont \u00e9t\u00e9 effectu\u00e9es ? Quelle est la pr\u00e9cision finale sur les donn\u00e9es d'entra\u00eenement ? Quelle est la pr\u00e9cision finale sur les donn\u00e9es de validation ? Quelle est la pr\u00e9cision sur l'ensemble de test ? Y a-t-il un signe de surapprentissage (overfitting) ? Pourquoi ?"},{"location":"module1/ressources/Partie1-Phase1-fiche-observations/#partie-5-visualisation-des-resultats-cellule-6","title":"Partie 5 : Visualisation des r\u00e9sultats (Cellule 6)","text":"<p>Analysez les graphiques d'apprentissage :</p> Question Observation La courbe de pr\u00e9cision d'entra\u00eenement est-elle croissante ? La courbe de perte d'entra\u00eenement est-elle d\u00e9croissante ? Y a-t-il un \u00e9cart important entre les courbes d'entra\u00eenement et de validation ? D'apr\u00e8s vous, l'entra\u00eenement a-t-il \u00e9t\u00e9 suffisant (nombre d'\u00e9poques) ?"},{"location":"module1/ressources/Partie1-Phase1-fiche-observations/#partie-6-predictions-sur-des-exemples-de-test-cellule-7","title":"Partie 6 : Pr\u00e9dictions sur des exemples de test (Cellule 7)","text":"<p>Observez les 10 exemples de pr\u00e9diction :</p> Question Observation Combien de pr\u00e9dictions sont correctes sur les 10 exemples ? Pour les pr\u00e9dictions incorrectes, quelles pourraient \u00eatre les raisons d'erreur ? Certains chiffres semblent-ils plus difficiles \u00e0 reconna\u00eetre que d'autres ?"},{"location":"module1/ressources/Partie1-Phase1-fiche-observations/#partie-7-test-avec-votre-propre-dessin-cellule-8","title":"Partie 7 : Test avec votre propre dessin (Cellule 8)","text":"Question Observation Quels chiffres avez-vous dessin\u00e9s ? Combien ont \u00e9t\u00e9 correctement pr\u00e9dits ? Pour ceux mal pr\u00e9dits, quelle \u00e9tait la pr\u00e9diction et pourquoi selon vous ? Comment le pr\u00e9traitement de l'image a-t-il transform\u00e9 votre dessin ?"},{"location":"module1/ressources/Partie1-Phase1-fiche-observations/#partie-8-experimentations-cellule-9","title":"Partie 8 : Exp\u00e9rimentations (Cellule 9)","text":"<p>Documentez vos exp\u00e9rimentations en modifiant le mod\u00e8le ou les param\u00e8tres :</p>"},{"location":"module1/ressources/Partie1-Phase1-fiche-observations/#experimentation-1","title":"Exp\u00e9rimentation 1","text":"<p>Modification effectu\u00e9e : _______</p> Param\u00e8tre Valeur originale Nouvelle valeur <p>R\u00e9sultats : - Pr\u00e9cision test : _% - Observations : _____</p>"},{"location":"module1/ressources/Partie1-Phase1-fiche-observations/#experimentation-2","title":"Exp\u00e9rimentation 2","text":"<p>Modification effectu\u00e9e : _______</p> Param\u00e8tre Valeur originale Nouvelle valeur <p>R\u00e9sultats : - Pr\u00e9cision test : _% - Observations : _____</p>"},{"location":"module1/ressources/Partie1-Phase1-fiche-observations/#conclusion","title":"Conclusion","text":"Question R\u00e9ponse Quels sont les 3 principaux apprentissages de ce TP ? 1.2.3. Quelles am\u00e9liorations pourriez-vous sugg\u00e9rer pour ce mod\u00e8le ? Comment ce mod\u00e8le se compare-t-il aux capacit\u00e9s humaines de reconnaissance de chiffres ? Quelles autres applications de la vision par ordinateur vous int\u00e9ressent ?"},{"location":"module1/ressources/Partie1-Phase1-fiche-observations/#glossaire-des-termes-cles-rencontres","title":"Glossaire des termes cl\u00e9s rencontr\u00e9s","text":"Terme Votre d\u00e9finition Convolution Pooling Epoch (\u00e9poque) Batch Dropout Softmax Overfitting (surapprentissage)"},{"location":"module1/ressources/Partie1-Phase2-fiche-observations/","title":"\ud83d\udccb Fiche d'observations - Concepts fondamentaux du Deep Learning","text":""},{"location":"module1/ressources/Partie1-Phase2-fiche-observations/#partie-1-comparaison-machine-learning-vs-deep-learning","title":"Partie 1 : Comparaison Machine Learning vs Deep Learning","text":"<p>Remplissez ce tableau comparatif apr\u00e8s avoir exp\u00e9riment\u00e9 avec les deux approches.</p> Aspect observ\u00e9 Machine Learning (Random Forest) Deep Learning (CNN) Pr\u00e9paration des donn\u00e9es Extraction de caract\u00e9ristiques Temps d'entra\u00eenement Pr\u00e9cision globale Pr\u00e9cision sur donn\u00e9es bruit\u00e9es Pr\u00e9cision sur donn\u00e9es avec rotation Facilit\u00e9 d'impl\u00e9mentation Interpr\u00e9tabilit\u00e9 Capacit\u00e9 de g\u00e9n\u00e9ralisation"},{"location":"module1/ressources/Partie1-Phase2-fiche-observations/#partie-2-schema-conceptuel-du-reseau-de-neurones","title":"Partie 2 : Sch\u00e9ma conceptuel du r\u00e9seau de neurones","text":"<p>Compl\u00e9tez le sch\u00e9ma ci-dessous en identifiant les \u00e9l\u00e9ments num\u00e9rot\u00e9s.</p> <pre><code>                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502             \u2502\n                    \u2502      1      \u2502\n                    \u2502             \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502                           \u2502\n            \u2502             2             \u2502\n            \u2502                           \u2502\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n      \u2502                                   \u2502\n      \u2502               3                   \u2502\n      \u2502                                   \u2502\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502                           \u2502\n            \u2502             4             \u2502\n            \u2502                           \u2502\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502             \u2502\n                    \u2502      5      \u2502\n                    \u2502             \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502             \u2502\n                    \u2502      6      \u2502\n                    \u2502             \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u25b2\n                          \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502             \u2502\n                    \u2502      7      \u2502\n                    \u2502             \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"module1/ressources/Partie1-Phase2-fiche-observations/#identification-des-elements","title":"Identification des \u00e9l\u00e9ments","text":""},{"location":"module1/ressources/Partie1-Phase2-fiche-observations/#partie-3-structure-du-reseau-de-neurones","title":"Partie 3 : Structure du r\u00e9seau de neurones","text":"<p>Pour un r\u00e9seau de neurones d\u00e9di\u00e9 \u00e0 la reconnaissance de chiffres manuscrits (MNIST), pr\u00e9cisez :</p> Couche Nombre de neurones Couche d'entr\u00e9e _ Premi\u00e8re couche cach\u00e9e _ Deuxi\u00e8me couche cach\u00e9e _ Couche de sortie _"},{"location":"module1/ressources/Partie1-Phase2-fiche-observations/#fonctions-dactivation","title":"Fonctions d'activation","text":"<p>Indiquez quelle fonction d'activation serait la plus appropri\u00e9e pour :</p> <ul> <li>Les couches cach\u00e9es : _____</li> <li>La couche de sortie : _____</li> </ul>"},{"location":"module1/ressources/Partie1-Phase2-fiche-observations/#partie-4-processus-dapprentissage","title":"Partie 4 : Processus d'apprentissage","text":"<p>D\u00e9crivez bri\u00e8vement les \u00e9tapes du processus d'apprentissage d'un r\u00e9seau de neurones :</p>"},{"location":"module1/ressources/Partie1-Phase2-fiche-observations/#partie-5-reflexion-et-synthese-personnelle","title":"Partie 5 : R\u00e9flexion et synth\u00e8se personnelle","text":"<p>Pour v\u00e9rifier votre compr\u00e9hension, r\u00e9pondez \u00e0 ces questions :</p>"},{"location":"module1/ressources/Partie1-Phase2-fiche-observations/#comparaison-ml-classique-et-deep-learning","title":"Comparaison ML classique et Deep Learning","text":"<p>Comment expliqueriez-vous la diff\u00e9rence principale entre ML classique et Deep Learning \u00e0 un camarade ?</p>"},{"location":"module1/ressources/Partie1-Phase2-fiche-observations/#fonctionnement-dun-neurone-artificiel","title":"Fonctionnement d'un neurone artificiel","text":"<p>D\u00e9crivez le fonctionnement d'un neurone artificiel et son r\u00f4le dans un r\u00e9seau :</p>"},{"location":"module1/ressources/Partie1-Phase2-fiche-observations/#processus-dapprentissage","title":"Processus d'apprentissage","text":"<p>Comment un r\u00e9seau de neurones \"apprend\"-il \u00e0 partir de donn\u00e9es ?</p>"},{"location":"module1/ressources/Partie1-Phase2-fiche-observations/#applications-pratiques","title":"Applications pratiques","text":"<p>Dans quelles situations le Deep Learning serait-il pr\u00e9f\u00e9rable au ML classique, et vice versa ?</p>"},{"location":"module1/ressources/Partie1-Phase2-fiche-observations/#conclusion","title":"Conclusion","text":"<p>Cette fiche d'observations vous a permis de documenter votre apprentissage des concepts fondamentaux du Deep Learning \u00e0 travers une approche exp\u00e9rimentale et comparative. Les observations que vous avez not\u00e9es serviront de base solide pour la suite de votre parcours et pour votre projet final de chatbot p\u00e9dagogique.</p>"},{"location":"module1/ressources/Partie1-Phase3-fiche-observations/","title":"\ud83d\udccb Fiche d'observations - Mini-projet d'am\u00e9lioration d'un mod\u00e8le","text":""},{"location":"module1/ressources/Partie1-Phase3-fiche-observations/#informations-generales","title":"Informations g\u00e9n\u00e9rales","text":"<p>Nom et pr\u00e9nom : ____</p> <p>Date : ____</p> <p>Groupe : ____</p>"},{"location":"module1/ressources/Partie1-Phase3-fiche-observations/#partie-1-modele-de-base","title":"Partie 1 : Mod\u00e8le de base","text":""},{"location":"module1/ressources/Partie1-Phase3-fiche-observations/#caracteristiques-du-modele-initial","title":"Caract\u00e9ristiques du mod\u00e8le initial","text":"\u00c9l\u00e9ment Description Architecture (nombre de couches) Nombre total de param\u00e8tres Fonction d'activation Optimiseur"},{"location":"module1/ressources/Partie1-Phase3-fiche-observations/#performance-du-modele-de-base","title":"Performance du mod\u00e8le de base","text":"M\u00e9trique Valeur Pr\u00e9cision sur l'ensemble de test Temps d'entra\u00eenement"},{"location":"module1/ressources/Partie1-Phase3-fiche-observations/#analyse-des-courbes-dapprentissage","title":"Analyse des courbes d'apprentissage","text":"<p>Observations sur les courbes de pr\u00e9cision (accuracy) : <pre><code>_________________________________________________________________\n_________________________________________________________________\n</code></pre></p> <p>Observations sur les courbes de perte (loss) : <pre><code>_________________________________________________________________\n_________________________________________________________________\n</code></pre></p>"},{"location":"module1/ressources/Partie1-Phase3-fiche-observations/#partie-2-modifications-effectuees","title":"Partie 2 : Modifications effectu\u00e9es","text":""},{"location":"module1/ressources/Partie1-Phase3-fiche-observations/#modification-1","title":"Modification 1","text":"<p>Description de la modification : <pre><code>_________________________________________________________________\n_________________________________________________________________\n</code></pre></p> <p>Raison du choix : <pre><code>_________________________________________________________________\n_________________________________________________________________\n</code></pre></p> <p>Impact sur l'architecture (nombre de couches, param\u00e8tres) : <pre><code>_________________________________________________________________\n</code></pre></p>"},{"location":"module1/ressources/Partie1-Phase3-fiche-observations/#modification-2","title":"Modification 2","text":"<p>Description de la modification : <pre><code>_________________________________________________________________\n_________________________________________________________________\n</code></pre></p> <p>Raison du choix : <pre><code>_________________________________________________________________\n_________________________________________________________________\n</code></pre></p> <p>Impact sur l'architecture (nombre de couches, param\u00e8tres) : <pre><code>_________________________________________________________________\n</code></pre></p>"},{"location":"module1/ressources/Partie1-Phase3-fiche-observations/#autre-modification-si-applicable","title":"Autre modification (si applicable)","text":"<p>Description de la modification : <pre><code>_________________________________________________________________\n_________________________________________________________________\n</code></pre></p> <p>Raison du choix : <pre><code>_________________________________________________________________\n_________________________________________________________________\n</code></pre></p>"},{"location":"module1/ressources/Partie1-Phase3-fiche-observations/#partie-3-resultats-des-modifications","title":"Partie 3 : R\u00e9sultats des modifications","text":""},{"location":"module1/ressources/Partie1-Phase3-fiche-observations/#tableau-comparatif","title":"Tableau comparatif","text":"Version du mod\u00e8le Pr\u00e9cision (test) Temps d'entra\u00eenement Commentaires Mod\u00e8le de base Mod\u00e8le apr\u00e8s modification 1 Mod\u00e8le apr\u00e8s modification 2 Autre modification"},{"location":"module1/ressources/Partie1-Phase3-fiche-observations/#test-sur-donnees-bruitees","title":"Test sur donn\u00e9es bruit\u00e9es","text":"Version du mod\u00e8le Pr\u00e9cision sur donn\u00e9es normales Pr\u00e9cision sur donn\u00e9es bruit\u00e9es Diff\u00e9rence Mod\u00e8le de base Meilleur mod\u00e8le modifi\u00e9"},{"location":"module1/ressources/Partie1-Phase3-fiche-observations/#visualisation-des-predictions","title":"Visualisation des pr\u00e9dictions","text":"<p>Types d'erreurs les plus fr\u00e9quentes : <pre><code>_________________________________________________________________\n_________________________________________________________________\n</code></pre></p> <p>Diff\u00e9rence entre les donn\u00e9es normales et bruit\u00e9es : <pre><code>_________________________________________________________________\n_________________________________________________________________\n</code></pre></p>"},{"location":"module1/ressources/Partie1-Phase3-fiche-observations/#partie-4-analyse-et-interpretation","title":"Partie 4 : Analyse et interpr\u00e9tation","text":""},{"location":"module1/ressources/Partie1-Phase3-fiche-observations/#impact-des-modifications","title":"Impact des modifications","text":"<p>Quelle modification a eu le plus grand impact positif ? Pourquoi ? <pre><code>_________________________________________________________________\n_________________________________________________________________\n</code></pre></p> <p>Y a-t-il eu des modifications qui ont d\u00e9grad\u00e9 les performances ? Pourquoi ? <pre><code>_________________________________________________________________\n_________________________________________________________________\n</code></pre></p>"},{"location":"module1/ressources/Partie1-Phase3-fiche-observations/#reflexion-globale","title":"R\u00e9flexion globale","text":"<p>Votre compr\u00e9hension des facteurs qui influencent les performances d'un r\u00e9seau de neurones : <pre><code>_________________________________________________________________\n_________________________________________________________________\n_________________________________________________________________\n</code></pre></p> <p>Autres modifications que vous auriez aim\u00e9 essayer : <pre><code>_________________________________________________________________\n_________________________________________________________________\n</code></pre></p>"},{"location":"module1/ressources/Partie1-Phase3-fiche-observations/#conclusion","title":"Conclusion","text":"<p>R\u00e9sum\u00e9 des principales observations : <pre><code>_________________________________________________________________\n_________________________________________________________________\n_________________________________________________________________\n</code></pre></p> <p>Applications potentielles de ce que vous avez appris : <pre><code>_________________________________________________________________\n_________________________________________________________________\n</code></pre></p>"},{"location":"module1/ressources/anatomie-reseau/","title":"Anatomie d'un r\u00e9seau de neurones","text":"<p>Ce document contient le code et les explications pour le notebook d'exploration interactive d'un r\u00e9seau de neurones. Vous pouvez copier-coller chaque section dans une cellule Google Colab.</p>"},{"location":"module1/ressources/anatomie-reseau/#cellule-1-markdown-introduction","title":"Cellule 1 (Markdown) - Introduction","text":"<pre><code># Anatomie d'un r\u00e9seau de neurones\n\n## Exploration interactive du fonctionnement interne d'un r\u00e9seau de neurones\n\nDans ce notebook, nous allons explorer de mani\u00e8re interactive le fonctionnement interne d'un r\u00e9seau de neurones. Vous pourrez manipuler directement les composants fondamentaux (neurones, poids, biais) et observer leur impact sur les pr\u00e9dictions.\n\n### Objectifs :\n- Comprendre le fonctionnement d'un neurone artificiel\n- Visualiser l'effet des poids et du biais sur les d\u00e9cisions\n- Explorer le flux d'information dans un r\u00e9seau multicouche\n- Observer l'\u00e9volution des poids pendant l'entra\u00eenement\n</code></pre>"},{"location":"module1/ressources/anatomie-reseau/#cellule-2-code-configuration-initiale","title":"Cellule 2 (Code) - Configuration initiale","text":"<pre><code># Partie 1: Configuration initiale\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom google.colab import output\noutput.enable_custom_widget_manager()\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output\nfrom matplotlib.colors import LinearSegmentedColormap\n\nprint(\"Configuration termin\u00e9e!\")\n</code></pre>"},{"location":"module1/ressources/anatomie-reseau/#cellule-3-markdown-exploration-dun-neurone-unique","title":"Cellule 3 (Markdown) - Exploration d'un neurone unique","text":"<pre><code>## Exploration d'un neurone unique\n\nDans cette partie, nous allons observer le fonctionnement d'un neurone artificiel, l'unit\u00e9 fondamentale des r\u00e9seaux de neurones.\n\nUn neurone artificiel effectue deux op\u00e9rations principales :\n1. Une **somme pond\u00e9r\u00e9e** des entr\u00e9es (z = w\u2081x\u2081 + w\u2082x\u2082 + ... + b)\n2. L'application d'une **fonction d'activation** qui introduit la non-lin\u00e9arit\u00e9 (a = f(z))\n\nUtilisez les contr\u00f4les interactifs ci-dessous pour observer comment un neurone traite l'information.\n</code></pre>"},{"location":"module1/ressources/anatomie-reseau/#cellule-4-code-fonctions-du-neurone","title":"Cellule 4 (Code) - Fonctions du neurone","text":"<pre><code># Fonction pour calculer la sortie d'un neurone\ndef neuron_output(x1, x2, w1, w2, b, activation=\"relu\"):\n    # Calcul de la somme pond\u00e9r\u00e9e\n    z = x1 * w1 + x2 * w2 + b\n\n    # Application de la fonction d'activation\n    if activation == \"relu\":\n        a = max(0, z)\n    elif activation == \"sigmoid\":\n        a = 1 / (1 + np.exp(-z))\n    elif activation == \"tanh\":\n        a = np.tanh(z)\n    else:\n        a = z  # Lin\u00e9aire\n\n    return z, a\n\n# Fonction pour visualiser un neurone\ndef visualize_neuron(x1, x2, w1, w2, b, activation=\"relu\"):\n    # Calculer la sortie\n    z, a = neuron_output(x1, x2, w1, w2, b, activation)\n\n    # Cr\u00e9er la figure\n    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n    # 1. Repr\u00e9sentation du neurone\n    ax = axes[0]\n    ax.set_xlim(-0.5, 2.5)\n    ax.set_ylim(-0.5, 2.5)\n\n    # Dessiner le neurone\n    circle = plt.Circle((1, 1), 0.4, fill=True, color='lightblue', alpha=0.7)\n    ax.add_artist(circle)\n\n    # Dessiner les entr\u00e9es\n    ax.plot(0, 0.7, 'ro', markersize=10)\n    ax.plot(0, 1.3, 'ro', markersize=10)\n\n    # Dessiner la sortie\n    ax.plot(2, 1, 'go', markersize=10)\n\n    # Ajouter les connexions\n    ax.arrow(0, 0.7, 0.6, 0.1, head_width=0.1, head_length=0.1, fc='black', ec='black', linewidth=2)\n    ax.arrow(0, 1.3, 0.6, -0.1, head_width=0.1, head_length=0.1, fc='black', ec='black', linewidth=2)\n    ax.arrow(1.4, 1, 0.6, 0, head_width=0.1, head_length=0.1, fc='black', ec='black', linewidth=2)\n\n    # Ajouter les textes\n    ax.text(-0.1, 0.7, f\"x\u2081 = {x1:.2f}\", fontsize=12, ha='right')\n    ax.text(-0.1, 1.3, f\"x\u2082 = {x2:.2f}\", fontsize=12, ha='right')\n    ax.text(1, 1, f\"z = {z:.2f}\\na = {a:.2f}\", fontsize=12, ha='center')\n    ax.text(0.5, 0.95, f\"w\u2081 = {w1:.2f}\", fontsize=10, rotation=15)\n    ax.text(0.5, 1.15, f\"w\u2082 = {w2:.2f}\", fontsize=10, rotation=-15)\n    ax.text(2.1, 1, f\"Sortie = {a:.2f}\", fontsize=12, ha='left')\n    ax.text(1, 0.5, f\"Biais = {b:.2f}\", fontsize=10)\n\n    ax.set_title(\"Neurone artificiel\", fontsize=14)\n    ax.set_axis_off()\n\n    # 2. Repr\u00e9sentation de la fonction d'activation\n    ax = axes[1]\n    x = np.linspace(-5, 5, 100)\n\n    if activation == \"relu\":\n        y = np.maximum(0, x)\n        title = \"Fonction d'activation: ReLU\"\n    elif activation == \"sigmoid\":\n        y = 1 / (1 + np.exp(-x))\n        title = \"Fonction d'activation: Sigmoid\"\n    elif activation == \"tanh\":\n        y = np.tanh(x)\n        title = \"Fonction d'activation: Tanh\"\n    else:\n        y = x\n        title = \"Fonction d'activation: Lin\u00e9aire\"\n\n    ax.plot(x, y, 'b-', linewidth=2)\n    ax.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n    ax.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n\n    # Marquer le point correspondant \u00e0 z\n    ax.plot(z, a, 'ro', markersize=8)\n    ax.plot([z, z], [0, a], 'r--', alpha=0.5)\n    ax.plot([0, z], [a, a], 'r--', alpha=0.5)\n\n    ax.set_xlim(-5, 5)\n    ax.set_ylim(-1.5, 1.5)\n    ax.set_xlabel(\"z (somme pond\u00e9r\u00e9e)\")\n    ax.set_ylabel(\"a (activation)\")\n    ax.set_title(title, fontsize=14)\n    ax.grid(True, alpha=0.3)\n\n    # 3. Visualisation de la fronti\u00e8re de d\u00e9cision\n    ax = axes[2]\n\n    # Cr\u00e9er des points pour former une grille\n    grid_size = 20\n    x1_values = np.linspace(0, 1, grid_size)\n    x2_values = np.linspace(0, 1, grid_size)\n    x1_grid, x2_grid = np.meshgrid(x1_values, x2_values)\n\n    # Calculer la sortie pour chaque point de la grille\n    z_grid = x1_grid * w1 + x2_grid * w2 + b\n\n    if activation == \"relu\":\n        a_grid = np.maximum(0, z_grid)\n    elif activation == \"sigmoid\":\n        a_grid = 1 / (1 + np.exp(-z_grid))\n    elif activation == \"tanh\":\n        a_grid = np.tanh(z_grid)\n    else:\n        a_grid = z_grid\n\n    # Cr\u00e9er une carte de couleur\n    cmap = plt.get_cmap('coolwarm')\n\n    # Tracer la heatmap\n    im = ax.imshow(a_grid, origin='lower', extent=[0, 1, 0, 1], \n                   cmap=cmap, vmin=0, vmax=1)\n    plt.colorbar(im, ax=ax, label=\"Activation\")\n\n    # Ajouter le point actuel\n    ax.plot(x1, x2, 'ko', markersize=8)\n\n    # Tracer la fronti\u00e8re de d\u00e9cision (a = 0.5)\n    if activation in [\"sigmoid\", \"tanh\"]:\n        threshold = 0.5\n        CS = ax.contour(x1_grid, x2_grid, a_grid, levels=[threshold], \n                         colors='k', linestyles='--')\n        ax.clabel(CS, inline=True, fontsize=10, fmt={threshold: \"a = 0.5\"})\n\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_xlabel(\"x\u2081\")\n    ax.set_ylabel(\"x\u2082\")\n    ax.set_title(\"Carte d'activation\", fontsize=14)\n\n    plt.tight_layout()\n    plt.show()\n\n    return a\n</code></pre>"},{"location":"module1/ressources/anatomie-reseau/#cellule-5-code-interface-interactive-pour-un-neurone","title":"Cellule 5 (Code) - Interface interactive pour un neurone","text":"<pre><code># Cr\u00e9er des widgets interactifs pour le neurone\nw1_slider = widgets.FloatSlider(value=1.0, min=-3.0, max=3.0, step=0.1, description='Poids w\u2081:')\nw2_slider = widgets.FloatSlider(value=1.0, min=-3.0, max=3.0, step=0.1, description='Poids w\u2082:')\nb_slider = widgets.FloatSlider(value=0.0, min=-3.0, max=3.0, step=0.1, description='Biais:')\nx1_slider = widgets.FloatSlider(value=0.5, min=0.0, max=1.0, step=0.05, description='Entr\u00e9e x\u2081:')\nx2_slider = widgets.FloatSlider(value=0.5, min=0.0, max=1.0, step=0.05, description='Entr\u00e9e x\u2082:')\nactivation_dropdown = widgets.Dropdown(\n    options=['relu', 'sigmoid', 'tanh', 'linear'],\n    value='relu',\n    description='Activation:'\n)\n\n# Fonction pour mettre \u00e0 jour la visualisation\ndef update_neuron_visualization(w1, w2, b, x1, x2, activation):\n    clear_output(wait=True)\n    output = visualize_neuron(x1, x2, w1, w2, b, activation)\n    print(f\"Sortie du neurone: {output:.4f}\")\n\n    # Expliquer le calcul\n    z = x1 * w1 + x2 * w2 + b\n    print(f\"\\nCalcul d\u00e9taill\u00e9:\")\n    print(f\"z = (x\u2081 \u00d7 w\u2081) + (x\u2082 \u00d7 w\u2082) + b\")\n    print(f\"z = ({x1:.2f} \u00d7 {w1:.2f}) + ({x2:.2f} \u00d7 {w2:.2f}) + {b:.2f}\")\n    print(f\"z = {x1*w1:.2f} + {x2*w2:.2f} + {b:.2f}\")\n    print(f\"z = {z:.2f}\")\n\n    if activation == \"relu\":\n        print(f\"a = ReLU(z) = max(0, z) = max(0, {z:.2f}) = {max(0, z):.2f}\")\n    elif activation == \"sigmoid\":\n        sig_z = 1 / (1 + np.exp(-z))\n        print(f\"a = Sigmoid(z) = 1 / (1 + e^(-z)) = 1 / (1 + e^(-{z:.2f})) = {sig_z:.2f}\")\n    elif activation == \"tanh\":\n        tanh_z = np.tanh(z)\n        print(f\"a = tanh(z) = tanh({z:.2f}) = {tanh_z:.2f}\")\n    else:\n        print(f\"a = z = {z:.2f}\")  # Lin\u00e9aire\n\n# Interface interactive pour le neurone\nneuron_output = widgets.interactive_output(\n    update_neuron_visualization,\n    {'w1': w1_slider, 'w2': w2_slider, 'b': b_slider, \n     'x1': x1_slider, 'x2': x2_slider, 'activation': activation_dropdown}\n)\n\n# Afficher les widgets\nprint(\"Utilisez les contr\u00f4les ci-dessous pour modifier les propri\u00e9t\u00e9s du neurone:\")\ndisplay(widgets.VBox([\n    widgets.HBox([x1_slider, x2_slider]),\n    widgets.HBox([w1_slider, w2_slider]),\n    widgets.HBox([b_slider, activation_dropdown])\n]))\ndisplay(neuron_output)\n</code></pre>"},{"location":"module1/ressources/anatomie-reseau/#cellule-6-markdown-de-lunique-au-reseau","title":"Cellule 6 (Markdown) - De l'unique au r\u00e9seau","text":"<pre><code>## De l'unique au r\u00e9seau\n\nMaintenant que nous avons explor\u00e9 un neurone unique, passons \u00e0 un r\u00e9seau simple. Un r\u00e9seau de neurones est compos\u00e9 de plusieurs neurones organis\u00e9s en couches, o\u00f9 l'information se propage de l'entr\u00e9e vers la sortie.\n\nLe r\u00e9seau ci-dessous contient :\n- Une couche d'entr\u00e9e (2 neurones)\n- Une couche cach\u00e9e (nombre ajustable de neurones)\n- Une couche de sortie (1 neurone)\n\nObservez comment l'information circule \u00e0 travers le r\u00e9seau et comment les diff\u00e9rents poids affectent les activations.\n</code></pre>"},{"location":"module1/ressources/anatomie-reseau/#cellule-7-code-fonctions-du-reseau","title":"Cellule 7 (Code) - Fonctions du r\u00e9seau","text":"<pre><code># Fonction pour cr\u00e9er et visualiser un r\u00e9seau simple\ndef create_simple_network(hidden_units=3, activation='relu'):\n    # Cr\u00e9er un mod\u00e8le s\u00e9quentiel\n    model = Sequential([\n        Dense(hidden_units, activation=activation, input_shape=(2,)),\n        Dense(1, activation='sigmoid')\n    ])\n\n    # Compiler le mod\u00e8le (bien que nous ne l'entra\u00eenerons pas)\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n    return model\n\n# Fonction pour visualiser un r\u00e9seau simple\ndef visualize_network(inputs, weights1=None, biases1=None, weights2=None, biases2=None, hidden_units=3, activation='relu'):\n    # Cr\u00e9er le mod\u00e8le si non fourni\n    model = create_simple_network(hidden_units, activation)\n\n    # Si des poids sont fournis, les appliquer\n    if weights1 is not None and biases1 is not None and weights2 is not None and biases2 is not None:\n        model.layers[0].set_weights([weights1, biases1])\n        model.layers[1].set_weights([weights2, biases2])\n\n    # Convertir les entr\u00e9es pour pr\u00e9diction\n    x = np.array([inputs])\n\n    # Obtenir les activations interm\u00e9diaires\n    intermediate_layer_model = tf.keras.Model(inputs=model.input,\n                                             outputs=model.layers[0].output)\n    intermediate_activations = intermediate_layer_model.predict(x)[0]\n\n    # Obtenir les activations de sortie\n    output_activation = model.predict(x)[0][0]\n\n    # Extraire les poids et biais\n    weights1, biases1 = model.layers[0].get_weights()\n    weights2, biases2 = model.layers[1].get_weights()\n\n    # Cr\u00e9er la figure pour visualiser le r\u00e9seau\n    plt.figure(figsize=(12, 8))\n\n    # D\u00e9finir les positions des neurones\n    input_layer_y = np.array([0.2, 0.8])\n    hidden_layer_y = np.linspace(0.1, 0.9, hidden_units)\n    output_layer_y = np.array([0.5])\n\n    input_layer_x = 0.1\n    hidden_layer_x = 0.5\n    output_layer_x = 0.9\n\n    # Dessiner les neurones d'entr\u00e9e\n    for i, y in enumerate(input_layer_y):\n        plt.scatter(input_layer_x, y, s=200, c='blue', alpha=0.7)\n        plt.text(input_layer_x, y, f\"x{i+1}={inputs[i]:.2f}\", fontsize=12, ha='center', va='center', color='white')\n\n    # Dessiner les neurones cach\u00e9s\n    for i, y in enumerate(hidden_layer_y):\n        # Calculer la somme pond\u00e9r\u00e9e\n        z = np.dot(inputs, weights1[:, i]) + biases1[i]\n\n        # Appliquer l'activation\n        if activation == 'relu':\n            a = max(0, z)\n        elif activation == 'sigmoid':\n            a = 1 / (1 + np.exp(-z))\n        elif activation == 'tanh':\n            a = np.tanh(z)\n        else:\n            a = z\n\n        # Couleur bas\u00e9e sur l'activation\n        color = plt.cm.viridis(a)\n\n        plt.scatter(hidden_layer_x, y, s=200, c=[color], alpha=0.7)\n        plt.text(hidden_layer_x, y, f\"{a:.2f}\", fontsize=12, ha='center', va='center', color='white')\n\n    # Dessiner le neurone de sortie\n    plt.scatter(output_layer_x, output_layer_y, s=200, c='red', alpha=0.7)\n    plt.text(output_layer_x, output_layer_y, f\"{output_activation:.2f}\", fontsize=12, ha='center', va='center', color='white')\n\n    # Dessiner les connexions entre couches d'entr\u00e9e et cach\u00e9e\n    for i, y_in in enumerate(input_layer_y):\n        for j, y_hid in enumerate(hidden_layer_y):\n            # Couleur et \u00e9paisseur bas\u00e9es sur le poids\n            weight = weights1[i, j]\n            width = abs(weight) * 3\n            color = 'red' if weight &lt; 0 else 'green'\n            alpha = min(abs(weight), 1.0)\n\n            plt.plot([input_layer_x, hidden_layer_x], [y_in, y_hid], \n                    c=color, linewidth=width, alpha=alpha)\n\n    # Dessiner les connexions entre couche cach\u00e9e et sortie\n    for i, y_hid in enumerate(hidden_layer_y):\n        # Couleur et \u00e9paisseur bas\u00e9es sur le poids\n        weight = weights2[i, 0]\n        width = abs(weight) * 3\n        color = 'red' if weight &lt; 0 else 'green'\n        alpha = min(abs(weight), 1.0)\n\n        plt.plot([hidden_layer_x, output_layer_x], [y_hid, output_layer_y], \n                c=color, linewidth=width, alpha=alpha)\n\n    # \u00c9tiquettes\n    plt.text(input_layer_x, 0.03, \"Couche d'entr\u00e9e\", fontsize=14, ha='center')\n    plt.text(hidden_layer_x, 0.03, \"Couche cach\u00e9e\", fontsize=14, ha='center')\n    plt.text(output_layer_x, 0.03, \"Couche de sortie\", fontsize=14, ha='center')\n\n    # Enlever les axes\n    plt.axis('off')\n    plt.title(f\"R\u00e9seau de neurones - Activation cach\u00e9e: {activation}\", fontsize=16)\n    plt.tight_layout()\n    plt.show()\n\n    # Afficher les calculs d\u00e9taill\u00e9s\n    print(\"\\nCalculs d\u00e9taill\u00e9s pour chaque neurone de la couche cach\u00e9e:\")\n    for i in range(hidden_units):\n        z = np.dot(inputs, weights1[:, i]) + biases1[i]\n        print(f\"\\nNeurone cach\u00e9 {i+1}:\")\n        print(f\"z = (x\u2081 \u00d7 w\u2081,{i+1}) + (x\u2082 \u00d7 w\u2082,{i+1}) + b{i+1}\")\n        print(f\"z = ({inputs[0]:.2f} \u00d7 {weights1[0, i]:.2f}) + ({inputs[1]:.2f} \u00d7 {weights1[1, i]:.2f}) + {biases1[i]:.2f}\")\n        print(f\"z = {inputs[0] * weights1[0, i]:.2f} + {inputs[1] * weights1[1, i]:.2f} + {biases1[i]:.2f} = {z:.2f}\")\n\n        if activation == 'relu':\n            a = max(0, z)\n            print(f\"a = ReLU(z) = max(0, {z:.2f}) = {a:.2f}\")\n        elif activation == 'sigmoid':\n            a = 1 / (1 + np.exp(-z))\n            print(f\"a = Sigmoid(z) = 1 / (1 + e^(-{z:.2f})) = {a:.2f}\")\n        elif activation == 'tanh':\n            a = np.tanh(z)\n            print(f\"a = tanh(z) = tanh({z:.2f}) = {a:.2f}\")\n        else:\n            a = z\n            print(f\"a = z = {z:.2f}\")\n\n    print(\"\\nCalcul pour le neurone de sortie:\")\n    z_out = np.dot(intermediate_activations, weights2[:, 0]) + biases2[0]\n    print(f\"z = \u03a3(a_cach\u00e9 \u00d7 w_sortie) + b_sortie = {z_out:.2f}\")\n    print(f\"sortie = Sigmoid(z) = 1 / (1 + e^(-{z_out:.2f})) = {output_activation:.2f}\")\n\n    return model, weights1, biases1, weights2, biases2\n\n# Fonction pour g\u00e9n\u00e9rer des poids al\u00e9atoires\ndef generate_random_weights(hidden_units=3):\n    # G\u00e9n\u00e9rer des poids al\u00e9atoires pour la premi\u00e8re couche\n    weights1 = np.random.normal(0, 1, (2, hidden_units))\n    biases1 = np.random.normal(0, 1, hidden_units)\n\n    # G\u00e9n\u00e9rer des poids al\u00e9atoires pour la couche de sortie\n    weights2 = np.random.normal(0, 1, (hidden_units, 1))\n    biases2 = np.random.normal(0, 1, 1)\n\n    return weights1, biases1, weights2, biases2\n</code></pre>"},{"location":"module1/ressources/anatomie-reseau/#cellule-8-code-interface-interactive-pour-le-reseau","title":"Cellule 8 (Code) - Interface interactive pour le r\u00e9seau","text":"<pre><code># Cr\u00e9er des widgets interactifs pour le r\u00e9seau\nx1_net_slider = widgets.FloatSlider(value=0.5, min=0.0, max=1.0, step=0.05, description='Entr\u00e9e x\u2081:')\nx2_net_slider = widgets.FloatSlider(value=0.5, min=0.0, max=1.0, step=0.05, description='Entr\u00e9e x\u2082:')\nhidden_units_slider = widgets.IntSlider(value=3, min=1, max=5, description='Neurones cach\u00e9s:')\nactivation_net_dropdown = widgets.Dropdown(\n    options=['relu', 'sigmoid', 'tanh', 'linear'],\n    value='relu',\n    description='Activation:'\n)\nrandom_button = widgets.Button(description=\"Poids al\u00e9atoires\")\n\n# Variables pour stocker les poids courants\ncurrent_weights1, current_biases1, current_weights2, current_biases2 = generate_random_weights()\n\n# Fonction pour visualiser le r\u00e9seau\ndef update_network_visualization(x1, x2, hidden_units, activation):\n    global current_weights1, current_biases1, current_weights2, current_biases2\n\n    # Ajuster les dimensions des poids si n\u00e9cessaire\n    if current_weights1.shape[1] != hidden_units:\n        current_weights1, current_biases1, current_weights2, current_biases2 = generate_random_weights(hidden_units)\n\n    # Visualiser le r\u00e9seau\n    inputs = np.array([x1, x2])\n    _, w1, b1, w2, b2 = visualize_network(\n        inputs, current_weights1, current_biases1, current_weights2, current_biases2, \n        hidden_units, activation\n    )\n\n    # Mettre \u00e0 jour les poids courants\n    current_weights1, current_biases1 = w1, b1\n    current_weights2, current_biases2 = w2, b2\n\n# Fonction pour g\u00e9n\u00e9rer de nouveaux poids al\u00e9atoires\ndef regenerate_weights(b):\n    global current_weights1, current_biases1, current_weights2, current_biases2\n    current_weights1, current_biases1, current_weights2, current_biases2 = generate_random_weights(\n        hidden_units_slider.value\n    )\n    # Mettre \u00e0 jour la visualisation\n    update_network_visualization(\n        x1_net_slider.value, x2_net_slider.value,\n        hidden_units_slider.value, activation_net_dropdown.value\n    )\n\n# Associer la fonction au bouton\nrandom_button.on_click(regenerate_weights)\n\n# Interface interactive pour le r\u00e9seau\nnetwork_output = widgets.interactive_output(\n    update_network_visualization,\n    {'x1': x1_net_slider, 'x2': x2_net_slider, \n     'hidden_units': hidden_units_slider, 'activation': activation_net_dropdown}\n)\n\n# Afficher les widgets pour le r\u00e9seau\nprint(\"\\nExplorez le comportement d'un r\u00e9seau simple:\")\ndisplay(widgets.VBox([\n    widgets.HBox([x1_net_slider, x2_net_slider]),\n    widgets.HBox([hidden_units_slider, activation_net_dropdown]),\n    random_button\n]))\ndisplay(network_output)\n</code></pre>"},{"location":"module1/ressources/anatomie-reseau/#cellule-9-markdown-visualisation-de-lentrainement","title":"Cellule 9 (Markdown) - Visualisation de l'entra\u00eenement","text":"<pre><code>## Visualisation de l'entra\u00eenement\n\nDans cette derni\u00e8re partie, nous allons observer l'\u00e9volution des poids pendant l'entra\u00eenement d'un r\u00e9seau de neurones sur un probl\u00e8me classique : le probl\u00e8me XOR.\n\nLe probl\u00e8me XOR (OU exclusif) consiste \u00e0 pr\u00e9dire la sortie de la fonction logique XOR :\n- (0,0) \u2192 0\n- (0,1) \u2192 1\n- (1,0) \u2192 1\n- (1,1) \u2192 0\n\nCe probl\u00e8me n'est pas lin\u00e9airement s\u00e9parable, ce qui signifie qu'il ne peut pas \u00eatre r\u00e9solu par un seul neurone.\n</code></pre>"},{"location":"module1/ressources/anatomie-reseau/#cellule-10-code-generation-de-donnees-xor","title":"Cellule 10 (Code) - G\u00e9n\u00e9ration de donn\u00e9es XOR","text":"<pre><code># G\u00e9n\u00e9rer des donn\u00e9es XOR\ndef generate_xor_data(n_samples=100):\n    X = np.random.rand(n_samples, 2)\n    y = np.logical_xor(X[:, 0] &gt; 0.5, X[:, 1] &gt; 0.5).astype(np.float32)\n    return X, y\n\n# Afficher quelques exemples de donn\u00e9es XOR\nX_sample, y_sample = generate_xor_data(20)\nplt.figure(figsize=(6, 6))\nplt.scatter(X_sample[:, 0], X_sample[:, 1], c=y_sample, cmap='coolwarm', s=100)\nplt.xlabel('x\u2081')\nplt.ylabel('x\u2082')\nplt.title('Probl\u00e8me XOR')\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(\"Exemples de donn\u00e9es XOR:\")\nfor i in range(5):\n    x1, x2 = X_sample[i]\n    y = y_sample[i]\n    print(f\"x1={x1:.2f}, x2={x2:.2f} \u2192 y={y:.0f}\")\n</code></pre>"},{"location":"module1/ressources/anatomie-reseau/#cellule-11-code-creation-et-entrainement-du-modele-xor","title":"Cellule 11 (Code) - Cr\u00e9ation et entra\u00eenement du mod\u00e8le XOR","text":"<pre><code># Cr\u00e9er un mod\u00e8le pour r\u00e9soudre XOR\nlearning_rate = 0.1\nhidden_units = 4\nepochs = 20\n\n# G\u00e9n\u00e9rer des donn\u00e9es\nX_train, y_train = generate_xor_data(200)\n\n# Cr\u00e9er un mod\u00e8le\nmodel = Sequential([\n    Dense(hidden_units, activation='relu', input_shape=(2,)),\n    Dense(1, activation='sigmoid')\n])\n\n# Compiler avec un optimiseur personnalis\u00e9\noptimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n\n# Entra\u00eener le mod\u00e8le\nhistory = model.fit(\n    X_train, y_train,\n    epochs=epochs,\n    batch_size=32,\n    verbose=1\n)\n\n# Afficher les r\u00e9sultats d'entra\u00eenement\nplt.figure(figsize=(12, 5))\n\n# Graphique de pr\u00e9cision\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], '-o')\nplt.title('Pr\u00e9cision pendant l\\'entra\u00eenement')\nplt.xlabel('\u00c9poque')\nplt.ylabel('Pr\u00e9cision')\nplt.grid(True, alpha=0.3)\n\n# Graphique de perte\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], '-o')\nplt.title('Perte pendant l\\'entra\u00eenement')\nplt.xlabel('\u00c9poque')\nplt.ylabel('Perte')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Je vais compl\u00e9ter le fichier anatomie-reseau.md \u00e0 partir de la cellule 12, en continuant le document l\u00e0 o\u00f9 il a \u00e9t\u00e9 interrompu.</p>"},{"location":"module1/ressources/anatomie-reseau/#cellule-12-code-visualisation-de-la-frontiere-de-decision","title":"Cellule 12 (Code) - Visualisation de la fronti\u00e8re de d\u00e9cision","text":"<pre><code># Visualiser la fronti\u00e8re de d\u00e9cision finale\nh = 0.01\nx_min, x_max = 0, 1\ny_min, y_max = 0, 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\ngrid_points = np.c_[xx.ravel(), yy.ravel()]\n\n# Convertir les points en format appropri\u00e9 pour le mod\u00e8le\ngrid_pred = model.predict(grid_points)\ngrid_pred = grid_pred.reshape(xx.shape)\n\n# Tracer la fronti\u00e8re de d\u00e9cision\nplt.figure(figsize=(8, 6))\nplt.contourf(xx, yy, grid_pred, alpha=0.8, cmap=plt.cm.RdBu)\n\n# Tracer les donn\u00e9es d'entra\u00eenement\nplt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.RdBu, edgecolors='k')\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.title('Fronti\u00e8re de d\u00e9cision pour le probl\u00e8me XOR')\nplt.colorbar()\nplt.show()\n\n# \u00c9valuer les performances finales\ntrain_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\nprint(f\"Pr\u00e9cision finale sur l'ensemble d'entra\u00eenement: {train_acc*100:.2f}%\")\n</code></pre>"},{"location":"module1/ressources/anatomie-reseau/#cellule-13-markdown-exploration-interactive-avancee","title":"Cellule 13 (Markdown) - Exploration interactive avanc\u00e9e","text":"<pre><code>## Exploration interactive avanc\u00e9e\n\nMaintenant que nous avons explor\u00e9 les bases des r\u00e9seaux de neurones, exploitons davantage l'interactivit\u00e9 pour comprendre comment ils apprennent et se comportent.\n\nUtilisez les widgets interactifs ci-dessous pour explorer diff\u00e9rentes architectures et configurations du r\u00e9seau sur le probl\u00e8me XOR. Observez comment les changements affectent la fronti\u00e8re de d\u00e9cision et les performances.\n</code></pre>"},{"location":"module1/ressources/anatomie-reseau/#cellule-14-code-interface-interactive-avancee","title":"Cellule 14 (Code) - Interface interactive avanc\u00e9e","text":"<pre><code># Cr\u00e9er des widgets interactifs pour l'exploration avanc\u00e9e\nnum_hidden_slider = widgets.IntSlider(value=4, min=2, max=10, step=1, description='Neurones cach\u00e9s:')\nlearning_rate_slider = widgets.FloatLogSlider(value=0.1, base=10, min=-3, max=0, step=0.1, description='Learning rate:')\nepochs_slider = widgets.IntSlider(value=100, min=10, max=500, step=10, description='\u00c9poques:')\nactivation_dropdown = widgets.Dropdown(\n    options=['relu', 'tanh', 'sigmoid'],\n    value='relu',\n    description='Activation:'\n)\n\n# Fonction pour cr\u00e9er et entra\u00eener le mod\u00e8le avec les param\u00e8tres sp\u00e9cifi\u00e9s\ndef create_and_train_model(hidden_units, learning_rate, epochs, activation):\n    # Cr\u00e9er un mod\u00e8le\n    model = Sequential([\n        Dense(hidden_units, activation=activation, input_shape=(2,)),\n        Dense(1, activation='sigmoid')\n    ])\n\n    # Compiler le mod\u00e8le\n    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n\n    # Cr\u00e9er des donn\u00e9es\n    X, y = generate_xor_data(200)\n\n    # Afficher les donn\u00e9es\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm')\n    plt.title('Donn\u00e9es XOR')\n    plt.xlabel('x1')\n    plt.ylabel('x2')\n\n    # Entra\u00eener le mod\u00e8le\n    history = model.fit(\n        X, y,\n        epochs=epochs,\n        batch_size=32,\n        verbose=0\n    )\n\n    # Afficher l'historique d'entra\u00eenement\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'])\n    plt.title('Perte pendant l\\'entra\u00eenement')\n    plt.xlabel('\u00c9poque')\n    plt.ylabel('Perte')\n    plt.tight_layout()\n    plt.show()\n\n    # Visualiser la fronti\u00e8re de d\u00e9cision\n    h = 0.01\n    x_min, x_max = 0, 1\n    y_min, y_max = 0, 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    grid_points = np.c_[xx.ravel(), yy.ravel()]\n\n    # Obtenir les pr\u00e9dictions\n    grid_pred = model.predict(grid_points, verbose=0)\n    grid_pred = grid_pred.reshape(xx.shape)\n\n    # Tracer la fronti\u00e8re de d\u00e9cision\n    plt.figure(figsize=(8, 6))\n    plt.contourf(xx, yy, grid_pred, alpha=0.8, cmap=plt.cm.RdBu)\n\n    # Tracer les donn\u00e9es\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdBu, edgecolors='k')\n    plt.xlabel('x1')\n    plt.ylabel('x2')\n    plt.title(f'Fronti\u00e8re de d\u00e9cision (Neurones: {hidden_units}, LR: {learning_rate:.4f}, Activation: {activation})')\n    plt.colorbar()\n    plt.show()\n\n    # \u00c9valuer le mod\u00e8le\n    loss, acc = model.evaluate(X, y, verbose=0)\n    print(f\"Architecture: {hidden_units} neurones cach\u00e9s, learning rate: {learning_rate:.4f}, activation: {activation}\")\n    print(f\"Pr\u00e9cision: {acc*100:.2f}%\")\n    print(f\"Perte: {loss:.4f}\")\n\n    # Afficher les poids du r\u00e9seau pour comprendre ce qu'il a appris\n    weights1, biases1 = model.layers[0].get_weights()\n    weights2, biases2 = model.layers[1].get_weights()\n\n    print(\"\\nPoids de la couche cach\u00e9e:\")\n    for i in range(hidden_units):\n        print(f\"Neurone {i+1}: {weights1[:, i]} (biais: {biases1[i]:.4f})\")\n\n    print(\"\\nPoids de la couche de sortie:\")\n    print(f\"{weights2.flatten()} (biais: {biases2[0]:.4f})\")\n\n# Interface interactive\ninteractive_output = widgets.interactive_output(\n    create_and_train_model,\n    {'hidden_units': num_hidden_slider, \n     'learning_rate': learning_rate_slider, \n     'epochs': epochs_slider, \n     'activation': activation_dropdown}\n)\n\n# Afficher les widgets\nprint(\"Explorez diff\u00e9rentes architectures et configurations:\")\ndisplay(widgets.VBox([\n    widgets.HBox([num_hidden_slider, activation_dropdown]),\n    widgets.HBox([learning_rate_slider, epochs_slider])\n]))\ndisplay(interactive_output)\n</code></pre>"},{"location":"module1/ressources/anatomie-reseau/#cellule-15-markdown-interpreter-les-resultats","title":"Cellule 15 (Markdown) - Interpr\u00e9ter les r\u00e9sultats","text":"<pre><code>## Interpr\u00e9ter les r\u00e9sultats\n\nMaintenant que vous avez explor\u00e9 diff\u00e9rentes configurations de r\u00e9seaux de neurones, prenons un moment pour analyser et comprendre les r\u00e9sultats :\n\n### Observations cl\u00e9s\n\n1. **Nombre de neurones cach\u00e9s** :\n   - Trop peu de neurones (2-3) limitent la capacit\u00e9 du r\u00e9seau \u00e0 apprendre la fonction XOR\n   - Un nombre appropri\u00e9 (4-6) permet g\u00e9n\u00e9ralement une bonne s\u00e9paration\n   - Trop de neurones peuvent parfois mener \u00e0 du surapprentissage (la fronti\u00e8re devient trop complexe)\n\n2. **Taux d'apprentissage (Learning Rate)** :\n   - Trop faible (&lt; 0.01) : apprentissage tr\u00e8s lent, peut ne pas converger dans le nombre d'\u00e9poques donn\u00e9\n   - Appropri\u00e9 (0.01 - 0.1) : bonne convergence avec une fronti\u00e8re stable\n   - Trop \u00e9lev\u00e9 (&gt; 0.5) : instabilit\u00e9, oscillations, voire divergence\n\n3. **Fonction d'activation** :\n   - ReLU : rapide, peut parfois cr\u00e9er des fronti\u00e8res plus angulaires\n   - Tanh : fronti\u00e8res plus lisses, parfois meilleure pour ce probl\u00e8me sp\u00e9cifique\n   - Sigmoid : peut \u00eatre plus lente \u00e0 converger pour des probl\u00e8mes comme XOR\n\n4. **Nombre d'\u00e9poques** :\n   - Insuffisant : mod\u00e8le sous-entra\u00een\u00e9, fronti\u00e8re impr\u00e9cise\n   - Suffisant : bonne fronti\u00e8re de d\u00e9cision\n   - Excessif : risque de surapprentissage, mais moins probl\u00e9matique pour ce cas simple\n\n### Comment le r\u00e9seau apprend-il le XOR ?\n\nLe probl\u00e8me XOR est int\u00e9ressant car il n'est pas lin\u00e9airement s\u00e9parable. En d'autres termes, on ne peut pas tracer une seule ligne droite pour s\u00e9parer les classes.\n\nUn r\u00e9seau avec une couche cach\u00e9e r\u00e9sout ce probl\u00e8me en :\n1. Cr\u00e9ant des \"lignes de s\u00e9paration\" avec chaque neurone de la couche cach\u00e9e\n2. Combinant ces lignes pour former des r\u00e9gions complexes\n3. Ajustant les poids pour positionner ces lignes de mani\u00e8re optimale\n\nC'est une parfaite illustration de pourquoi nous avons besoin de r\u00e9seaux multicouches pour r\u00e9soudre des probl\u00e8mes non lin\u00e9aires.\n</code></pre>"},{"location":"module1/ressources/anatomie-reseau/#cellule-16-code-visualisation-des-neurones-caches","title":"Cellule 16 (Code) - Visualisation des neurones cach\u00e9s","text":"<pre><code># Fonction pour visualiser la contribution de chaque neurone cach\u00e9\ndef visualize_hidden_neurons(hidden_units=4, activation='relu'):\n    # Cr\u00e9er un mod\u00e8le\n    model = Sequential([\n        Dense(hidden_units, activation=activation, input_shape=(2,)),\n        Dense(1, activation='sigmoid')\n    ])\n\n    # Compiler le mod\u00e8le\n    optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n\n    # Cr\u00e9er des donn\u00e9es\n    X, y = generate_xor_data(200)\n\n    # Entra\u00eener le mod\u00e8le\n    model.fit(X, y, epochs=100, batch_size=32, verbose=0)\n\n    # Obtenir les poids\n    weights1, biases1 = model.layers[0].get_weights()\n    weights2, biases2 = model.layers[1].get_weights()\n\n    # Cr\u00e9er une grille de points pour visualisation\n    h = 0.01\n    x_min, x_max = 0, 1\n    y_min, y_max = 0, 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    grid_points = np.c_[xx.ravel(), yy.ravel()]\n\n    # Cr\u00e9er un mod\u00e8le interm\u00e9diaire pour obtenir les activations de la couche cach\u00e9e\n    intermediate_model = tf.keras.Model(inputs=model.input, outputs=model.layers[0].output)\n    hidden_activations = intermediate_model.predict(grid_points, verbose=0)\n\n    # Visualiser la contribution de chaque neurone cach\u00e9\n    fig, axes = plt.subplots(2, hidden_units, figsize=(4*hidden_units, 8))\n\n    # Pour chaque neurone cach\u00e9\n    for i in range(hidden_units):\n        # Activation du neurone\n        neuron_activation = hidden_activations[:, i].reshape(xx.shape)\n\n        # La ligne de d\u00e9cision du neurone (o\u00f9 l'activation est proche de 0)\n        if activation == 'tanh':\n            decision_boundary = np.zeros_like(neuron_activation)\n        elif activation == 'relu':\n            decision_boundary = np.zeros_like(neuron_activation)\n        else:  # sigmoid\n            decision_boundary = np.ones_like(neuron_activation) * 0.5\n\n        # Visualiser l'activation du neurone\n        im = axes[0, i].contourf(xx, yy, neuron_activation, cmap='viridis')\n        axes[0, i].set_title(f'Neurone {i+1}\\nw=[{weights1[0, i]:.2f}, {weights1[1, i]:.2f}]\\nb={biases1[i]:.2f}')\n        axes[0, i].set_xlabel('x1')\n        axes[0, i].set_ylabel('x2')\n        plt.colorbar(im, ax=axes[0, i])\n\n        # Visualiser la ligne de d\u00e9cision\n        axes[1, i].contour(xx, yy, neuron_activation, levels=[0] if activation in ['tanh', 'relu'] else [0.5], \n                           colors='r', linewidths=2)\n        axes[1, i].scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')\n        axes[1, i].set_title(f'Ligne de d\u00e9cision\\nContribution finale: {\"+\" if weights2[i, 0] &gt; 0 else \"-\"}{abs(weights2[i, 0]):.2f}')\n        axes[1, i].set_xlabel('x1')\n        axes[1, i].set_ylabel('x2')\n\n    plt.tight_layout()\n    plt.show()\n\n    # Afficher la fronti\u00e8re de d\u00e9cision finale\n    hidden_output = np.dot(hidden_activations, weights2) + biases2\n    final_pred = 1 / (1 + np.exp(-hidden_output))  # sigmoid\n    final_pred = final_pred.reshape(xx.shape)\n\n    plt.figure(figsize=(8, 6))\n    plt.contourf(xx, yy, final_pred, alpha=0.8, cmap=plt.cm.RdBu)\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdBu, edgecolors='k')\n    plt.xlabel('x1')\n    plt.ylabel('x2')\n    plt.title('Fronti\u00e8re de d\u00e9cision finale (combinaison des neurones cach\u00e9s)')\n    plt.colorbar()\n    plt.show()\n\n    # Expliquer comment les neurones se combinent\n    print(\"Comment les neurones cach\u00e9s se combinent pour r\u00e9soudre le probl\u00e8me XOR:\")\n    print(\"-\" * 80)\n    print(\"1. Chaque neurone cach\u00e9 cr\u00e9e une 'ligne de d\u00e9cision' dans l'espace d'entr\u00e9e\")\n    print(\"2. Le signe du poids de sortie d\u00e9termine si le neurone contribue positivement ou n\u00e9gativement\")\n    print(\"3. La combinaison de ces lignes forme la fronti\u00e8re de d\u00e9cision complexe finale\")\n    print(\"-\" * 80)\n    print(\"\\nPoids de la couche de sortie:\")\n    for i in range(hidden_units):\n        print(f\"Neurone {i+1} \u2192 Sortie: {'positif' if weights2[i, 0] &gt; 0 else 'n\u00e9gatif'} ({weights2[i, 0]:.4f})\")\n    print(f\"Biais de sortie: {biases2[0]:.4f}\")\n\n# Cr\u00e9er des widgets pour l'exploration\nhidden_units_viz = widgets.IntSlider(value=4, min=2, max=8, step=1, description='Neurones:')\nactivation_viz = widgets.Dropdown(\n    options=['relu', 'tanh', 'sigmoid'],\n    value='relu',\n    description='Activation:'\n)\n\n# Interface interactive\nviz_output = widgets.interactive_output(\n    visualize_hidden_neurons,\n    {'hidden_units': hidden_units_viz, 'activation': activation_viz}\n)\n\n# Afficher les widgets\nprint(\"Explorez comment chaque neurone cach\u00e9 contribue \u00e0 la solution:\")\ndisplay(widgets.HBox([hidden_units_viz, activation_viz]))\ndisplay(viz_output)\n</code></pre>"},{"location":"module1/ressources/anatomie-reseau/#cellule-17-markdown-conclusion","title":"Cellule 17 (Markdown) - Conclusion","text":"<pre><code>## Conclusion : L'anatomie d'un r\u00e9seau de neurones\n\n### Ce que nous avons explor\u00e9\n\nDans ce notebook, nous avons diss\u00e9qu\u00e9 le fonctionnement interne d'un r\u00e9seau de neurones en explorant :\n\n1. **Le neurone individuel**\n   - Comment les entr\u00e9es sont pond\u00e9r\u00e9es et combin\u00e9es\n   - L'effet du biais sur le seuil d'activation\n   - L'impact des diff\u00e9rentes fonctions d'activation\n\n2. **La structure d'un r\u00e9seau**\n   - Comment les neurones s'organisent en couches\n   - Comment l'information se propage \u00e0 travers le r\u00e9seau\n   - Comment les couches interagissent pour cr\u00e9er des repr\u00e9sentations complexes\n\n3. **Le processus d'apprentissage**\n   - Comment un r\u00e9seau s'entra\u00eene par descente de gradient\n   - Comment les poids s'ajustent pour minimiser l'erreur\n   - Comment le r\u00e9seau apprend progressivement \u00e0 r\u00e9soudre des probl\u00e8mes complexes\n\n4. **La r\u00e9solution de probl\u00e8mes non lin\u00e9aires**\n   - Comment un probl\u00e8me comme XOR n\u00e9cessite plusieurs neurones\n   - Comment chaque neurone cach\u00e9 contribue \u00e0 la solution finale\n   - Comment les fronti\u00e8res de d\u00e9cision complexes \u00e9mergent de la combinaison de neurones simples\n\n### Applications pratiques\n\nCes connaissances fondamentales vous permettront de :\n\n- **Concevoir** des architectures appropri\u00e9es pour diff\u00e9rents probl\u00e8mes\n- **Diagnostiquer** les probl\u00e8mes dans vos mod\u00e8les (sous-apprentissage, sur-apprentissage)\n- **Optimiser** les performances de vos r\u00e9seaux\n- **Expliquer** le fonctionnement interne des mod\u00e8les de Deep Learning\n\n### Prochaines \u00e9tapes\n\nDans les modules suivants, nous approfondirons ces concepts en explorant :\n\n- Les r\u00e9seaux de neurones convolutifs (CNN) pour la vision par ordinateur\n- Les r\u00e9seaux r\u00e9currents (RNN) pour le traitement de s\u00e9quences\n- Les techniques avanc\u00e9es d'entra\u00eenement et d'optimisation\n- L'application pratique de ces connaissances dans des projets r\u00e9els\n\nMaintenant que vous avez une compr\u00e9hension solide de l'anatomie d'un r\u00e9seau de neurones, vous \u00eates pr\u00eat \u00e0 aborder des architectures plus complexes et sp\u00e9cialis\u00e9es !\n</code></pre>"},{"location":"module1/ressources/anatomie-reseau/#cellule-18-code-schema-conceptuel-a-completer","title":"Cellule 18 (Code) - Sch\u00e9ma conceptuel \u00e0 compl\u00e9ter","text":"<pre><code># Fonction pour g\u00e9n\u00e9rer un sch\u00e9ma conceptuel \u00e0 compl\u00e9ter\ndef create_conceptual_diagram():\n    # Cr\u00e9er la figure\n    plt.figure(figsize=(12, 10))\n\n    # D\u00e9finir les positions des composants\n    input_layer_x = 0.1\n    hidden_layer1_x = 0.3\n    hidden_layer2_x = 0.5\n    output_layer_x = 0.7\n    prediction_x = 0.9\n\n    input_layer_y = [0.2, 0.5, 0.8]\n    hidden_layer1_y = [0.2, 0.5, 0.8]\n    hidden_layer2_y = [0.3, 0.7]\n    output_layer_y = [0.5]\n\n    # Dessiner les couches\n    plt.text(0.02, 0.5, \"1. Couche d'entr\u00e9e\", fontsize=12, ha='left', va='center')\n    for y in input_layer_y:\n        circle = plt.Circle((input_layer_x, y), 0.05, fill=True, color='lightblue', alpha=0.7)\n        plt.gca().add_patch(circle)\n\n    plt.text(hidden_layer1_x-0.08, 0.08, \"2. Premi\u00e8re couche cach\u00e9e\", fontsize=12, ha='center', va='center')\n    for y in hidden_layer1_y:\n        circle = plt.Circle((hidden_layer1_x, y), 0.05, fill=True, color='lightgreen', alpha=0.7)\n        plt.gca().add_patch(circle)\n\n    plt.text(hidden_layer2_x-0.08, 0.08, \"3. Deuxi\u00e8me couche cach\u00e9e\", fontsize=12, ha='center', va='center')\n    for y in hidden_layer2_y:\n        circle = plt.Circle((hidden_layer2_x, y), 0.05, fill=True, color='lightsalmon', alpha=0.7)\n        plt.gca().add_patch(circle)\n\n    plt.text(output_layer_x-0.02, 0.08, \"4. Couche de sortie\", fontsize=12, ha='center', va='center')\n    for y in output_layer_y:\n        circle = plt.Circle((output_layer_x, y), 0.05, fill=True, color='plum', alpha=0.7)\n        plt.gca().add_patch(circle)\n\n    # Dessiner le processus d'apprentissage\n    plt.text(prediction_x-0.02, 0.08, \"5. Pr\u00e9diction\", fontsize=12, ha='center', va='center')\n    rect = plt.Rectangle((prediction_x-0.06, 0.45), 0.12, 0.1, fill=True, color='lightgrey', alpha=0.7)\n    plt.gca().add_patch(rect)\n    plt.text(prediction_x, 0.5, \"\u0177\", fontsize=14, ha='center', va='center')\n\n    # Erreur et donn\u00e9es r\u00e9elles\n    plt.text(prediction_x-0.02, 0.35, \"6. Calcul de l'erreur\", fontsize=12, ha='center', va='center')\n    rect = plt.Rectangle((prediction_x-0.06, 0.25), 0.12, 0.1, fill=True, color='lightcoral', alpha=0.7)\n    plt.gca().add_patch(rect)\n    plt.text(prediction_x, 0.3, \"Loss\", fontsize=14, ha='center', va='center')\n\n    plt.text(prediction_x-0.02, 0.15, \"7. Donn\u00e9es r\u00e9elles\", fontsize=12, ha='center', va='center')\n    rect = plt.Rectangle((prediction_x-0.06, 0.15), 0.12, 0.1, fill=True, color='lightblue', alpha=0.7)\n    plt.gca().add_patch(rect)\n    plt.text(prediction_x, 0.2, \"y\", fontsize=14, ha='center', va='center')\n\n    # Connexions entre les couches\n    for y1 in input_layer_y:\n        for y2 in hidden_layer1_y:\n            plt.plot([input_layer_x, hidden_layer1_x], [y1, y2], 'k-', alpha=0.3)\n\n    for y1 in hidden_layer1_y:\n        for y2 in hidden_layer2_y:\n            plt.plot([hidden_layer1_x, hidden_layer2_x], [y1, y2], 'k-', alpha=0.3)\n\n    for y1 in hidden_layer2_y:\n        for y2 in output_layer_y:\n            plt.plot([hidden_layer2_x, output_layer_x], [y1, y2], 'k-', alpha=0.3)\n\n    # Connexion sortie -&gt; pr\u00e9diction\n    plt.plot([output_layer_x, prediction_x], [output_layer_y[0], 0.5], 'k-', alpha=0.3)\n\n    # Flux d'erreur\n    plt.plot([prediction_x, prediction_x], [0.45, 0.35], 'r--', alpha=0.7)\n    plt.arrow(prediction_x, 0.2, 0, 0.05, head_width=0.01, head_length=0.01, fc='blue', ec='blue')\n\n    # R\u00e9tropropagation\n    plt.arrow(prediction_x-0.1, 0.3, -0.1, 0, head_width=0.01, head_length=0.01, fc='red', ec='red', linestyle='dashed')\n    plt.text(prediction_x-0.15, 0.33, \"R\u00e9tropropagation\", fontsize=10, ha='center', va='center', color='red')\n\n    # Propagation avant\n    plt.arrow(hidden_layer2_x+0.1, 0.5, 0.1, 0, head_width=0.01, head_length=0.01, fc='green', ec='green')\n    plt.text(hidden_layer2_x+0.15, 0.53, \"Propagation avant\", fontsize=10, ha='center', va='center', color='green')\n\n    # Finalisation du sch\u00e9ma\n    plt.xlim(0, 1)\n    plt.ylim(0, 1)\n    plt.title(\"Sch\u00e9ma conceptuel d'un r\u00e9seau de neurones\", fontsize=16)\n    plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n\n    print(\"Compl\u00e9tez le sch\u00e9ma conceptuel en identifiant les \u00e9l\u00e9ments num\u00e9rot\u00e9s:\")\n    print(\"1. ________________________________\")\n    print(\"2. ________________________________\")\n    print(\"3. ________________________________\")\n    print(\"4. ________________________________\")\n    print(\"5. ________________________________\")\n    print(\"6. ________________________________\")\n    print(\"7. ________________________________\")\n\n# Afficher le sch\u00e9ma conceptuel\ncreate_conceptual_diagram()\n</code></pre>"},{"location":"module1/ressources/anatomie-reseau/#cellule-19-markdown-exercice-final","title":"Cellule 19 (Markdown) - Exercice final","text":"<pre><code>## Exercice final : Synth\u00e8se des connaissances\n\nPour consolider votre compr\u00e9hension des r\u00e9seaux de neurones, compl\u00e9tez les informations suivantes :\n\n### Structure d'un r\u00e9seau de neurones pour la reconnaissance de chiffres manuscrits (MNIST)\n\n| Couche | Nombre de neurones | Fonction d'activation recommand\u00e9e |\n|--------|-------------------|----------------------------------|\n| Couche d'entr\u00e9e | _______ | _______ |\n| Premi\u00e8re couche cach\u00e9e | _______ | _______ |\n| Deuxi\u00e8me couche cach\u00e9e (facultative) | _______ | _______ |\n| Couche de sortie | _______ | _______ |\n\n### Processus d'apprentissage\n\nD\u00e9crivez bri\u00e8vement les \u00e9tapes du processus d'apprentissage d'un r\u00e9seau de neurones :\n\n1. _________________________________________________________________\n\n2. _________________________________________________________________\n\n3. _________________________________________________________________\n\n4. _________________________________________________________________\n\n### R\u00e9flexion personnelle\n\nComment expliqueriez-vous maintenant le fonctionnement d'un r\u00e9seau de neurones \u00e0 un camarade qui n'a jamais \u00e9tudi\u00e9 ce sujet ?\n\n_________________________________________________________________\n\n_________________________________________________________________\n\n_________________________________________________________________\n\n### Auto-\u00e9valuation\n\nSur une \u00e9chelle de 1 \u00e0 5, \u00e9valuez votre niveau de compr\u00e9hension actuel des \u00e9l\u00e9ments suivants :\n\n- Structure d'un neurone : ___/5\n- Fonctions d'activation : ___/5\n- Architecture d'un r\u00e9seau : ___/5\n- Processus d'apprentissage : ___/5\n</code></pre> <p>Ce contenu compl\u00e8te le document sur l'anatomie d'un r\u00e9seau de neurones, en ajoutant les cellules 12 \u00e0 19 qui manquaient dans la version originale.</p>"},{"location":"module1/ressources/ddf/","title":"Ddf","text":"<p>Je comprends maintenant que vous pr\u00e9f\u00e9rez utiliser une extension Markdown pour repr\u00e9senter la carte mentale plut\u00f4t qu'une image SVG. Mermaid est l'extension id\u00e9ale pour ce cas, car elle est bien support\u00e9e par MkDocs Material et permet de cr\u00e9er des diagrammes directement en Markdown.</p> <p>Voici la section d'auto-\u00e9valuation avec une carte mentale au format Mermaid:</p>"},{"location":"module1/ressources/ddf/#auto-evaluation-et-synthese-20-min","title":"\ud83d\udcdd Auto-\u00e9valuation et synth\u00e8se (20 min)","text":"<p>Cette \u00e9tape finale du module vous permettra de consolider vos connaissances et d'\u00e9valuer votre compr\u00e9hension.</p>"},{"location":"module1/ressources/ddf/#carte-heuristique-des-fondamentaux-du-deep-learning","title":"\ud83e\udde0 Carte heuristique des fondamentaux du Deep Learning","text":"<pre><code>mindmap\n  root((Deep Learning))\n    ::icon(fa fa-brain)\n\n    (C'est quoi?)\n      ::icon(fa fa-question)\n      [R\u00e9seaux de neurones multicouches]\n      ::icon(fa fa-network-wired)\n      [Apprend automatiquement les caract\u00e9ristiques]\n      ::icon(fa fa-cogs)\n      [Id\u00e9al pour images, texte, son]\n      ::icon(fa fa-images)\n      [Plus puissant que ML classique]\n      ::icon(fa fa-rocket)\n\n    (Architecture)\n      ::icon(fa fa-layer-group)\n      [Neurones artificiels interconnect\u00e9s]\n      [Couche d'entr\u00e9e (donn\u00e9es brutes)]\n      [Couches cach\u00e9es (traitement)]\n      [Couche de sortie (pr\u00e9diction)]\n\n    (Types de r\u00e9seaux)\n      ::icon(fa fa-sitemap)\n      [CNN: pour les images]\n      ::icon(fa fa-eye)\n      [RNN/LSTM: pour les textes/s\u00e9quences]\n      ::icon(fa fa-file-alt)\n      [Transformers: pour le langage avanc\u00e9]\n      ::icon(fa fa-language)\n      [GAN: pour g\u00e9n\u00e9rer du contenu]\n      ::icon(fa fa-paint-brush)\n\n    (Apprentissage)\n      ::icon(fa fa-graduation-cap)\n      [Forward propagation: pr\u00e9diction]\n      [Calcul d'erreur: \u00e9cart avec r\u00e9alit\u00e9]\n      [Backpropagation: ajustement des poids]\n      [\u00c9poque: passage complet des donn\u00e9es]\n\n    (Vs ML classique)\n      ::icon(fa fa-exchange-alt)\n      [ML: extraction manuelle de caract\u00e9ristiques]\n      [DL: extraction automatique de caract\u00e9ristiques]\n      [ML: plus simple mais moins puissant]\n      [DL: plus complexe mais plus performant]\n\n    (Applications)\n      ::icon(fa fa-laptop-code)\n      [Reconnaissance d'images et objets]\n      [Traduction et g\u00e9n\u00e9ration de texte]\n      [Recommandation de contenu]\n      [Voitures autonomes]\n      [Applications m\u00e9dicales]\n\n    (D\u00e9fis actuels)\n      ::icon(fa fa-exclamation-triangle)\n      [Besoin de grandes quantit\u00e9s de donn\u00e9es]\n      [Consommation \u00e9lev\u00e9e d'\u00e9nergie]\n      [Difficile d'expliquer les d\u00e9cisions]\n      [Risques de biais dans les mod\u00e8les]\n      [Co\u00fbts importants pour l'entra\u00eenement]\n\n    (Conseils pratiques)\n      ::icon(fa fa-lightbulb)\n      [Commencer simple et it\u00e9rer]\n      [Bien pr\u00e9parer ses donn\u00e9es]\n      [Surveiller l'entra\u00eenement]\n      [Tester sur donn\u00e9es vari\u00e9es]</code></pre> <p>\ud83d\udca1 Astuce</p> <p>Pour explorer plus en d\u00e9tail chaque concept, cliquez sur les diff\u00e9rentes branches de la carte mentale interactive ci-dessus.</p>"},{"location":"module1/ressources/ddf/#qcm-dauto-evaluation","title":"\u2705 QCM d'auto-\u00e9valuation","text":"<p>Testez vos connaissances en Machine Learning</p> <p>Ce QCM couvre l'ensemble des concepts fondamentaux abord\u00e9s dans ce module:</p> <ul> <li>15 questions sur les fondamentaux du Deep Learning</li> <li>\u00c9valuation de votre compr\u00e9hension des diff\u00e9rentes architectures</li> <li>Explication d\u00e9taill\u00e9e des r\u00e9ponses pour renforcer votre apprentissage</li> </ul> <p>Commencer le QCM</p>"},{"location":"module1/ressources/ddf/#synthese-personnelle","title":"\ud83d\udcdd Synth\u00e8se personnelle","text":"<p>Intelligence Artificielle - R\u00e9flexion globale</p> <p>Avant de conclure ce module, prenez quelques minutes pour r\u00e9fl\u00e9chir \u00e0 votre apprentissage:</p> <ol> <li>Identifiez les 3 concepts qui vous ont sembl\u00e9 les plus importants</li> <li>Comparez les approches de Machine Learning classique et de Deep Learning</li> <li>R\u00e9fl\u00e9chissez aux applications potentielles dans votre domaine professionnel</li> </ol> <p>Cette r\u00e9flexion personnelle contribuera significativement \u00e0 ancrer vos apprentissages.</p> <p>Cette version utilise:</p> <ol> <li>Mermaid Mindmap: Une syntaxe Markdown qui sera rendue comme une carte mentale interactive directement dans la page</li> <li>Ic\u00f4nes Font Awesome: Pour enrichir visuellement le mindmap (si votre configuration MkDocs les supporte)</li> <li>Admonitions natives: De MkDocs Material pour les sections d'information, qui adopteront automatiquement les couleurs du th\u00e8me</li> <li>Boutons Markdown: Pour les appels \u00e0 l'action</li> </ol> <p>Avantages de cette approche: - Vraiment natif Markdown: Tout est \u00e9crit en texte, pas d'images externes - Interactivit\u00e9: La carte mentale Mermaid peut \u00eatre interactive (expansion/r\u00e9duction des n\u0153uds) - Accessibilit\u00e9: Le contenu reste accessible m\u00eame si le rendu graphique \u00e9choue - Maintenance simplifi\u00e9e: Facile \u00e0 modifier directement dans le fichier Markdown - SEO am\u00e9lior\u00e9: Le contenu est indexable par les moteurs de recherche</p> <p>Note: Si votre installation de MkDocs Material n'a pas l'extension Mermaid activ\u00e9e, vous devrez l'ajouter dans votre fichier <code>mkdocs.yml</code>:</p> <pre><code>markdown_extensions:\n  - pymdownx.superfences:\n      custom_fences:\n        - name: mermaid\n          class: mermaid\n          format: !!python/name:pymdownx.superfences.fence_code_format\n</code></pre>"},{"location":"module1/ressources/deep-learning/","title":"Machine Learning Classique - Classification MNIST avec Random Forest","text":"<p>Ce document contient le code et les explications pour le notebook de classification d'images MNIST avec Random Forest (approche Machine Learning classique). Vous pouvez copier-coller chaque section dans une cellule Google Colab.</p>"},{"location":"module1/ressources/deep-learning/#cellule-1-markdown-introduction","title":"Cellule 1 (Markdown) - Introduction","text":"<pre><code># Classification avec Machine Learning classique\n\n## Reconnaissance de chiffres manuscrits avec Random Forest\n\nDans ce notebook, nous allons impl\u00e9menter une approche de Machine Learning classique pour la classification des chiffres manuscrits en utilisant le dataset MNIST. Nous utiliserons l'algorithme Random Forest, qui est bas\u00e9 sur un ensemble d'arbres de d\u00e9cision.\n\n### Objectifs :\n- Comprendre comment pr\u00e9parer des donn\u00e9es d'images pour le ML classique\n- Impl\u00e9menter un classificateur Random Forest\n- \u00c9valuer ses performances et ses limites\n- Comparer cette approche avec le Deep Learning\n</code></pre>"},{"location":"module1/ressources/deep-learning/#cellule-2-code-importation-des-bibliotheques","title":"Cellule 2 (Code) - Importation des biblioth\u00e8ques","text":"<pre><code># Importation des biblioth\u00e8ques n\u00e9cessaires\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport time\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\nfrom sklearn.pipeline import Pipeline\nfrom google.colab import output\noutput.enable_custom_widget_manager()\nimport ipywidgets as widgets\n</code></pre>"},{"location":"module1/ressources/deep-learning/#cellule-3-markdown-chargement-des-donnees","title":"Cellule 3 (Markdown) - Chargement des donn\u00e9es","text":"<pre><code>## Chargement et exploration des donn\u00e9es\n\nLe dataset MNIST contient 70 000 images de chiffres manuscrits (0-9) en niveaux de gris. Chaque image est de taille 28x28 pixels, ce qui donne 784 pixels par image.\n</code></pre>"},{"location":"module1/ressources/deep-learning/#cellule-4-code-chargement-des-donnees-mnist","title":"Cellule 4 (Code) - Chargement des donn\u00e9es MNIST","text":"<pre><code>print(\"Chargement du jeu de donn\u00e9es MNIST...\")\n# Utilisation du jeu de donn\u00e9es MNIST int\u00e9gr\u00e9 \u00e0 sklearn\nfrom sklearn.datasets import fetch_openml\nmnist = fetch_openml('mnist_784', version=1, as_frame=False)\nX, y = mnist[\"data\"], mnist[\"target\"]\nX = X / 255.0  # Normalisation des valeurs de pixels entre 0 et 1\ny = y.astype(np.uint8)  # Conversion des labels en entiers\n\n# Exploration des donn\u00e9es\nprint(f\"Dimensions du jeu de donn\u00e9es: {X.shape}\")\nprint(f\"Nombre de classes: {len(np.unique(y))}\")\n</code></pre>"},{"location":"module1/ressources/deep-learning/#cellule-5-code-visualisation-des-exemples","title":"Cellule 5 (Code) - Visualisation des exemples","text":"<pre><code># Affichage de quelques exemples\nplt.figure(figsize=(10, 5))\nfor i in range(10):\n    plt.subplot(2, 5, i + 1)\n    plt.imshow(X[i].reshape(28, 28), cmap='gray')\n    plt.title(f\"Label: {y[i]}\")\n    plt.axis('off')\nplt.tight_layout()\nplt.suptitle(\"Exemples de chiffres manuscrits\", y=1.05)\nplt.show()\n</code></pre>"},{"location":"module1/ressources/deep-learning/#cellule-6-markdown-preparation-des-donnees","title":"Cellule 6 (Markdown) - Pr\u00e9paration des donn\u00e9es","text":"<pre><code>## Pr\u00e9paration des donn\u00e9es pour Machine Learning classique\n\nContrairement aux r\u00e9seaux de neurones convolutifs (CNN), les algorithmes de ML classiques comme Random Forest ne sont pas con\u00e7us pour traiter directement des images. Nous devons donc :\n\n1. R\u00e9duire la dimensionnalit\u00e9 des donn\u00e9es (784 caract\u00e9ristiques est trop \u00e9lev\u00e9)\n2. Extraire des caract\u00e9ristiques pertinentes\n\nNous utiliserons l'Analyse en Composantes Principales (PCA) pour r\u00e9duire la dimensionnalit\u00e9 tout en conservant l'essentiel de l'information.\n</code></pre>"},{"location":"module1/ressources/deep-learning/#cellule-7-code-preparation-des-donnees","title":"Cellule 7 (Code) - Pr\u00e9paration des donn\u00e9es","text":"<pre><code>print(\"\\n--- Pr\u00e9paration des donn\u00e9es pour Random Forest ---\")\nprint(\"Pour le Machine Learning classique, nous devons souvent extraire des caract\u00e9ristiques manuellement.\")\n\n# R\u00e9duction de dimension avec PCA pour acc\u00e9l\u00e9rer l'entra\u00eenement\nprint(\"Application d'une r\u00e9duction de dimension (PCA)...\")\nn_components = 50  # R\u00e9duire de 784 \u00e0 50 caract\u00e9ristiques\n\n# S\u00e9paration en ensembles d'entra\u00eenement et de test\n# Utilisation d'un \u00e9chantillon r\u00e9duit pour acc\u00e9l\u00e9rer la d\u00e9monstration\nX_sample = X[:10000]\ny_sample = y[:10000]\n\nX_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.2, random_state=42)\n\nprint(f\"Taille de l'ensemble d'entra\u00eenement: {X_train.shape}\")\nprint(f\"Taille de l'ensemble de test: {X_test.shape}\")\n\n# Cr\u00e9ation d'un pipeline d'extraction de caract\u00e9ristiques\nfeature_pipeline = Pipeline([\n    ('scaler', StandardScaler()),  # Normalisation des donn\u00e9es\n    ('pca', PCA(n_components=n_components))  # R\u00e9duction de dimension par PCA\n])\n\n# Application aux donn\u00e9es\nprint(\"Extraction de caract\u00e9ristiques...\")\nX_train_features = feature_pipeline.fit_transform(X_train)\nX_test_features = feature_pipeline.transform(X_test)\n\nprint(f\"Dimensions apr\u00e8s extraction de caract\u00e9ristiques: {X_train_features.shape}\")\n</code></pre>"},{"location":"module1/ressources/deep-learning/#cellule-8-markdown-entrainement-du-modele","title":"Cellule 8 (Markdown) - Entra\u00eenement du mod\u00e8le","text":"<pre><code>## Entra\u00eenement du mod\u00e8le Random Forest\n\nNous allons maintenant entra\u00eener un classificateur Random Forest sur nos donn\u00e9es pr\u00e9trait\u00e9es. Random Forest est un algorithme d'ensemble qui combine les pr\u00e9dictions de plusieurs arbres de d\u00e9cision pour am\u00e9liorer la pr\u00e9cision et contr\u00f4ler le sur-apprentissage.\n\nPrincipaux hyperparam\u00e8tres :\n- **n_estimators** : Nombre d'arbres dans la for\u00eat\n- **max_depth** : Profondeur maximale de chaque arbre\n- **min_samples_split** : Nombre minimum d'\u00e9chantillons requis pour diviser un n\u0153ud\n</code></pre>"},{"location":"module1/ressources/deep-learning/#cellule-9-code-entrainement-du-modele","title":"Cellule 9 (Code) - Entra\u00eenement du mod\u00e8le","text":"<pre><code>print(\"\\n--- Entra\u00eenement du mod\u00e8le Random Forest ---\")\n\n# Param\u00e8tres du mod\u00e8le - vous pouvez les modifier\nn_estimators = 100  # Nombre d'arbres\nmax_depth = 10      # Profondeur maximale des arbres\nmin_samples_split = 2  # Nombre minimum d'\u00e9chantillons requis pour diviser un n\u0153ud\n\n# Cr\u00e9ation du mod\u00e8le\nrf_model = RandomForestClassifier(\n    n_estimators=n_estimators,\n    max_depth=max_depth,\n    min_samples_split=min_samples_split,\n    random_state=42,\n    n_jobs=-1  # Utiliser tous les c\u0153urs disponibles\n)\n\n# Mesure du temps d'entra\u00eenement\nstart_time = time.time()\nprint(\"Entra\u00eenement du mod\u00e8le en cours...\")\nrf_model.fit(X_train_features, y_train)\nend_time = time.time()\ntraining_time = end_time - start_time\n\nprint(f\"Temps d'entra\u00eenement: {training_time:.2f} secondes\")\n</code></pre>"},{"location":"module1/ressources/deep-learning/#cellule-10-markdown-evaluation-du-modele","title":"Cellule 10 (Markdown) - \u00c9valuation du mod\u00e8le","text":"<pre><code>## \u00c9valuation du mod\u00e8le\n\n\u00c9valuons maintenant les performances de notre mod\u00e8le Random Forest sur l'ensemble de test. Nous utiliserons plusieurs m\u00e9triques :\n- Pr\u00e9cision globale (accuracy)\n- Matrice de confusion\n- Rapport de classification d\u00e9taill\u00e9 (pr\u00e9cision, rappel, F1-score pour chaque classe)\n</code></pre>"},{"location":"module1/ressources/deep-learning/#cellule-11-code-evaluation-et-metriques","title":"Cellule 11 (Code) - \u00c9valuation et m\u00e9triques","text":"<pre><code>print(\"\\n--- \u00c9valuation du mod\u00e8le Random Forest ---\")\n\n# Pr\u00e9dictions sur l'ensemble de test\ny_pred = rf_model.predict(X_test_features)\n\n# Calcul des m\u00e9triques\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nclass_report = classification_report(y_test, y_pred)\n\nprint(f\"Pr\u00e9cision globale: {accuracy*100:.2f}%\")\nprint(\"\\nMatrice de confusion:\")\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Pr\u00e9dictions')\nplt.ylabel('Valeurs r\u00e9elles')\nplt.title('Matrice de confusion')\nplt.show()\n\nprint(\"\\nRapport de classification:\")\nprint(class_report)\n</code></pre>"},{"location":"module1/ressources/deep-learning/#cellule-12-markdown-analyse-des-erreurs","title":"Cellule 12 (Markdown) - Analyse des erreurs","text":"<pre><code>## Analyse des erreurs\n\nExaminons quelques exemples que notre mod\u00e8le a mal classifi\u00e9s pour mieux comprendre ses limites.\n</code></pre>"},{"location":"module1/ressources/deep-learning/#cellule-13-code-visualisation-des-erreurs","title":"Cellule 13 (Code) - Visualisation des erreurs","text":"<pre><code>print(\"\\n--- Analyse des erreurs ---\")\n\n# Identifier les erreurs\nerror_indices = np.where(y_pred != y_test)[0]\nn_errors = min(10, len(error_indices))  # Afficher max 10 erreurs\n\nif n_errors &gt; 0:\n    plt.figure(figsize=(12, 4))\n    for i, idx in enumerate(error_indices[:n_errors]):\n        plt.subplot(2, 5, i + 1)\n        # R\u00e9cup\u00e9rer l'image originale\n        img = X_test[idx].reshape(28, 28)\n        plt.imshow(img, cmap='gray')\n        plt.title(f\"R\u00e9el: {y_test[idx]}\\nPr\u00e9dit: {y_pred[idx]}\")\n        plt.axis('off')\n    plt.tight_layout()\n    plt.suptitle(\"Exemples d'erreurs de classification\", y=1.05)\n    plt.show()\nelse:\n    print(\"Aucune erreur trouv\u00e9e dans l'\u00e9chantillon de test!\")\n</code></pre>"},{"location":"module1/ressources/deep-learning/#cellule-14-markdown-importance-des-caracteristiques","title":"Cellule 14 (Markdown) - Importance des caract\u00e9ristiques","text":"<pre><code>## Importance des caract\u00e9ristiques\n\nUn avantage des mod\u00e8les comme Random Forest est leur interpr\u00e9tabilit\u00e9. Nous pouvons examiner quelles caract\u00e9ristiques (ici, quelles composantes principales) le mod\u00e8le consid\u00e8re comme les plus importantes pour faire ses pr\u00e9dictions.\n</code></pre>"},{"location":"module1/ressources/deep-learning/#cellule-15-code-visualisation-de-limportance-des-caracteristiques","title":"Cellule 15 (Code) - Visualisation de l'importance des caract\u00e9ristiques","text":"<pre><code>print(\"\\n--- Importance des caract\u00e9ristiques ---\")\n# Visualiser l'importance des composantes principales\nfeature_importance = rf_model.feature_importances_\nsorted_idx = np.argsort(feature_importance)[::-1]\n\nplt.figure(figsize=(10, 5))\nplt.bar(range(20), feature_importance[sorted_idx[:20]])\nplt.xticks(range(20), [f\"Feat {i}\" for i in sorted_idx[:20]], rotation=90)\nplt.xlabel('Composantes principales')\nplt.ylabel('Importance')\nplt.title('Top 20 des composantes principales les plus importantes')\nplt.tight_layout()\nplt.show()\n\nprint(\"Les caract\u00e9ristiques les plus importantes sont les composantes principales qui capturent le plus de variance dans les donn\u00e9es.\")\n</code></pre>"},{"location":"module1/ressources/glossaire-dl/","title":"Glossaire","text":"<p>Voici le glossaire du Deep Learning avec des liens vers les d\u00e9finitions des termes techniques mentionn\u00e9s :</p>"},{"location":"module1/ressources/glossaire-dl/#glossaire-du-deep-learning","title":"\ud83d\udcd5 Glossaire du Deep Learning","text":""},{"location":"module1/ressources/glossaire-dl/#termes-fondamentaux","title":"Termes fondamentaux","text":"Terme D\u00e9finition Exemple concret Deep Learning Sous-domaine du Machine Learning utilisant des r\u00e9seaux de neurones \u00e0 plusieurs couches pour mod\u00e9liser des abstractions de haut niveau dans les donn\u00e9es. Reconnaissance d'objets dans des photos. R\u00e9seau de neurones Syst\u00e8me inspir\u00e9 du cerveau humain compos\u00e9 de n\u0153uds (neurones) interconnect\u00e9s qui traitent les informations. R\u00e9seau capable de reconna\u00eetre des chiffres manuscrits. Neurone artificiel Unit\u00e9 de calcul de base dans un r\u00e9seau de neurones qui re\u00e7oit des entr\u00e9es, applique une transformation et produit une sortie. Un neurone qui s'active quand il d\u00e9tecte un contour vertical. Couche Ensemble de neurones situ\u00e9s au m\u00eame niveau dans le r\u00e9seau. Couche d'entr\u00e9e, couche cach\u00e9e, couche de sortie. Poids Valeurs num\u00e9riques qui d\u00e9finissent l'importance relative de chaque connexion entre les neurones. Un poids \u00e9lev\u00e9 (ex: 0.8) indique une forte influence. Biais Valeur ajout\u00e9e \u00e0 la somme pond\u00e9r\u00e9e des entr\u00e9es d'un neurone pour ajuster le seuil d'activation. Permet \u00e0 un neurone de s'activer m\u00eame si toutes les entr\u00e9es sont nulles. Fonction d'activation Fonction math\u00e9matique qui d\u00e9termine la sortie d'un neurone en fonction de ses entr\u00e9es. ReLU, Sigmoid, Tanh."},{"location":"module1/ressources/glossaire-dl/#architectures-de-reseaux","title":"Architectures de r\u00e9seaux","text":"Terme D\u00e9finition Cas d'utilisation R\u00e9seau dense R\u00e9seau o\u00f9 chaque neurone est connect\u00e9 \u00e0 tous les neurones de la couche pr\u00e9c\u00e9dente. Classification d'images simples, pr\u00e9diction de valeurs. R\u00e9seau convolutif (CNN) R\u00e9seau sp\u00e9cialis\u00e9 dans le traitement des donn\u00e9es en grille comme les images, utilisant des filtres pour d\u00e9tecter des caract\u00e9ristiques. Reconnaissance d'objets, classification d'images. R\u00e9seau r\u00e9current (RNN) R\u00e9seau avec des connexions formant des cycles, adapt\u00e9 aux donn\u00e9es s\u00e9quentielles. Traduction automatique, g\u00e9n\u00e9ration de texte. LSTM/GRU Types de RNN capables de m\u00e9moriser l'information sur de longues s\u00e9quences gr\u00e2ce \u00e0 des m\u00e9canismes de m\u00e9moire. Analyse de texte long, pr\u00e9diction de s\u00e9ries temporelles. Transformer Architecture bas\u00e9e sur des m\u00e9canismes d'attention, sans r\u00e9currence, permettant de traiter les donn\u00e9es en parall\u00e8le. Mod\u00e8les de langage avanc\u00e9s comme GPT, BERT, Mistral. Autoencoder R\u00e9seau qui apprend \u00e0 encoder puis d\u00e9coder les donn\u00e9es pour r\u00e9duire la dimensionnalit\u00e9 ou d\u00e9tecter des anomalies. R\u00e9duction de dimensionnalit\u00e9, d\u00e9tection d'anomalies. GAN (Generative Adversarial Network) Deux r\u00e9seaux en comp\u00e9tition : un g\u00e9n\u00e9rateur cr\u00e9e des donn\u00e9es et un discriminateur essaie de les distinguer des donn\u00e9es r\u00e9elles. Cr\u00e9ation d'images r\u00e9alistes, deepfakes."},{"location":"module1/ressources/glossaire-dl/#apprentissage","title":"Apprentissage","text":"Terme D\u00e9finition Exemple Forward propagation Passage des donn\u00e9es d'entr\u00e9e \u00e0 travers le r\u00e9seau pour produire une pr\u00e9diction. Calcul de la sortie d'un mod\u00e8le pour une image d'entr\u00e9e. Loss (perte) Mesure de l'\u00e9cart entre les pr\u00e9dictions du mod\u00e8le et les valeurs r\u00e9elles. Erreur quadratique moyenne, entropie crois\u00e9e. Backpropagation Algorithme qui calcule le gradient de l'erreur par rapport aux poids du r\u00e9seau pour les ajuster. Calcul de la contribution de chaque poids \u00e0 l'erreur totale. Descente de gradient Algorithme d'optimisation qui ajuste les poids du r\u00e9seau pour minimiser l'erreur. Modification it\u00e9rative des poids dans la direction du gradient n\u00e9gatif. \u00c9poque Un passage complet \u00e0 travers l'ensemble des donn\u00e9es d'entra\u00eenement. Entra\u00eener un mod\u00e8le pendant 10 \u00e9poques. Batch Sous-ensemble des donn\u00e9es trait\u00e9 avant une mise \u00e0 jour des poids. Traiter les donn\u00e9es par lots de 32 exemples. Optimiseur Algorithme qui impl\u00e9mente la descente de gradient pour ajuster les poids du r\u00e9seau. Adam, SGD, RMSprop. Learning rate Taux qui contr\u00f4le l'ampleur des ajustements des poids lors de l'entra\u00eenement. Trop \u00e9lev\u00e9 : divergence, trop faible : apprentissage lent."},{"location":"module1/ressources/glossaire-dl/#techniques-specifiques","title":"Techniques sp\u00e9cifiques","text":"Terme D\u00e9finition Utilisation Transfer learning R\u00e9utilisation d'un mod\u00e8le pr\u00e9-entra\u00een\u00e9 sur une nouvelle t\u00e2che pour b\u00e9n\u00e9ficier de ses connaissances. Adapter un mod\u00e8le ImageNet pour reconna\u00eetre des maladies de plantes. Fine-tuning Ajustement d'un mod\u00e8le pr\u00e9-entra\u00een\u00e9 sur des donn\u00e9es sp\u00e9cifiques pour am\u00e9liorer ses performances sur une t\u00e2che particuli\u00e8re. R\u00e9entra\u00eener les derni\u00e8res couches d'un mod\u00e8le BERT pour la classification de texte. Data augmentation G\u00e9n\u00e9ration de nouvelles donn\u00e9es d'entra\u00eenement par transformation des donn\u00e9es existantes pour augmenter la diversit\u00e9. Rotation, mise \u00e0 l'\u00e9chelle, distorsion d'images. Dropout Technique o\u00f9 des neurones sont al\u00e9atoirement d\u00e9sactiv\u00e9s pendant l'entra\u00eenement pour r\u00e9duire l'overfitting. Force le r\u00e9seau \u00e0 \u00eatre redondant et robuste. Batch normalization Normalisation des activations d'une couche pour stabiliser et acc\u00e9l\u00e9rer l'apprentissage. Am\u00e9liore la convergence et permet d'utiliser des taux d'apprentissage plus \u00e9lev\u00e9s. Early stopping Arr\u00eat de l'entra\u00eenement quand les performances sur la validation cessent de s'am\u00e9liorer pour \u00e9viter l'overfitting. Emp\u00eache le surajustement aux donn\u00e9es d'entra\u00eenement. Embedding Conversion de donn\u00e9es cat\u00e9gorielles en vecteurs denses pour les repr\u00e9senter dans un espace continu. Word embeddings dans le NLP (Word2Vec, GloVe)."},{"location":"module1/ressources/glossaire-dl/#convolutions-et-cnn","title":"Convolutions et CNN","text":"Terme D\u00e9finition R\u00f4le Filtre (kernel) Matrice de poids appliqu\u00e9e \u00e0 une r\u00e9gion de l'image pour d\u00e9tecter des caract\u00e9ristiques sp\u00e9cifiques. D\u00e9tecte des caract\u00e9ristiques sp\u00e9cifiques (bords, textures). Feature map Sortie d'un filtre de convolution appliqu\u00e9 \u00e0 une image, repr\u00e9sentant les caract\u00e9ristiques d\u00e9tect\u00e9es. Carte d'activation des caract\u00e9ristiques d\u00e9tect\u00e9es. Pooling Op\u00e9ration de sous-\u00e9chantillonnage r\u00e9duisant les dimensions de la feature map pour g\u00e9n\u00e9raliser les caract\u00e9ristiques. R\u00e9duit la complexit\u00e9 computationnelle et contr\u00f4le l'overfitting. Padding Ajout de pixels (g\u00e9n\u00e9ralement z\u00e9ros) aux bords d'une image pour conserver les dimensions apr\u00e8s convolution. Permet de conserver les dimensions de l'image apr\u00e8s l'application des filtres. Stride Pas de d\u00e9placement du filtre sur l'image, contr\u00f4lant le chevauchement des champs r\u00e9ceptifs. Contr\u00f4le la taille de la feature map et la quantit\u00e9 de chevauchement."},{"location":"module1/ressources/glossaire-dl/#metriques-devaluation","title":"M\u00e9triques d'\u00e9valuation","text":"M\u00e9trique D\u00e9finition Cas d'usage Accuracy Proportion de pr\u00e9dictions correctes parmi toutes les pr\u00e9dictions. Classification \u00e9quilibr\u00e9e. Precision Proportion des pr\u00e9dictions positives qui sont correctes. Quand les faux positifs sont co\u00fbteux. Recall Proportion des cas positifs r\u00e9els correctement identifi\u00e9s. Quand les faux n\u00e9gatifs sont co\u00fbteux. F1-Score Moyenne harmonique de la pr\u00e9cision et du rappel, \u00e9quilibrant les deux m\u00e9triques. Classification avec classes d\u00e9s\u00e9quilibr\u00e9es. ROC-AUC Aire sous la courbe ROC, mesurant la qualit\u00e9 de la discrimination entre les classes. \u00c9valuation des mod\u00e8les de classification. MAE (Mean Absolute Error) Moyenne des valeurs absolues des erreurs entre les pr\u00e9dictions et les valeurs r\u00e9elles. R\u00e9gression, quand les \u00e9carts importants ne sont pas surpond\u00e9r\u00e9s. RMSE (Root Mean Squared Error) Racine carr\u00e9e de la moyenne des carr\u00e9s des erreurs entre les pr\u00e9dictions et les valeurs r\u00e9elles. R\u00e9gression, p\u00e9nalise davantage les grands \u00e9carts."},{"location":"module1/ressources/glossaire-dl/#problemes-courants","title":"Probl\u00e8mes courants","text":"Terme D\u00e9finition Solution possible Overfitting Le mod\u00e8le apprend trop bien les donn\u00e9es d'entra\u00eenement au d\u00e9triment de la g\u00e9n\u00e9ralisation sur de nouvelles donn\u00e9es. R\u00e9gularisation, dropout, plus de donn\u00e9es. Underfitting Le mod\u00e8le est trop simple pour capturer la complexit\u00e9 des donn\u00e9es, r\u00e9sultant en de mauvaises performances. Augmenter la complexit\u00e9 du mod\u00e8le, entra\u00eener plus longtemps. Vanishing gradient Probl\u00e8me o\u00f9 le gradient devient tr\u00e8s petit, ralentissant l'apprentissage dans les couches profondes. Utiliser ReLU, LSTM, initialisation des poids adapt\u00e9e. Exploding gradient Probl\u00e8me o\u00f9 le gradient devient tr\u00e8s grand, d\u00e9stabilisant l'apprentissage. Gradient clipping, normalisation des poids. Imbalanced data Jeu de donn\u00e9es o\u00f9 certaines classes sont beaucoup plus fr\u00e9quentes que d'autres, biaisant le mod\u00e8le. R\u00e9\u00e9chantillonnage, pond\u00e9ration des classes, techniques d'augmentation."},{"location":"module1/ressources/glossaire-dl/#termes-relatifs-aux-modeles-de-langage","title":"Termes relatifs aux mod\u00e8les de langage","text":"Terme D\u00e9finition Exemple Token Unit\u00e9 de base du texte pour les mod\u00e8les de langage, comme un mot, sous-mot ou caract\u00e8re. \"Je suis pr\u00eat\" \u2192 [\"Je\", \"suis\", \"pr\u00eat\"]. Tokenization Processus de d\u00e9coupage du texte en tokens pour les traiter dans un mod\u00e8le de langage. \"Je suis pr\u00eat\" \u2192 [\"Je\", \"suis\", \"pr\u00eat\"]. Prompt Texte initial fourni \u00e0 un mod\u00e8le de langage pour guider sa g\u00e9n\u00e9ration de texte. \"R\u00e9dige un po\u00e8me sur le printemps:\". Context window Nombre maximum de tokens qu'un mod\u00e8le peut traiter en une fois, d\u00e9terminant la quantit\u00e9 d'information contextuelle. GPT-4 a une fen\u00eatre contextuelle de 8k-32k tokens. Attention M\u00e9canisme permettant au mod\u00e8le de se concentrer sur diff\u00e9rentes parties de l'entr\u00e9e pour g\u00e9n\u00e9rer une sortie pertinente. Self-attention dans les Transformers. Fine-tuning Adaptation d'un mod\u00e8le pr\u00e9-entra\u00een\u00e9 \u00e0 une t\u00e2che sp\u00e9cifique en ajustant ses poids sur des donn\u00e9es sp\u00e9cifiques. Ajuster GPT pour une t\u00e2che de customer support. Few-shot learning Capacit\u00e9 d'un mod\u00e8le \u00e0 apprendre \u00e0 partir de tr\u00e8s peu d'exemples, souvent en fournissant quelques exemples dans le prompt. Donner 2-3 exemples dans le prompt pour guider le mod\u00e8le."},{"location":"module1/ressources/glossaire-dl/#frameworks-et-outils","title":"Frameworks et outils","text":"Terme D\u00e9finition Cas d'utilisation TensorFlow Framework de Machine Learning d\u00e9velopp\u00e9 par Google, utilis\u00e9 pour cr\u00e9er et entra\u00eener des mod\u00e8les de Deep Learning. D\u00e9ploiement en production, applications mobiles. PyTorch Framework de Machine Learning d\u00e9velopp\u00e9 par Facebook, connu pour sa flexibilit\u00e9 et sa facilit\u00e9 d'utilisation. Recherche, prototypage rapide. Keras API de haut niveau s'ex\u00e9cutant sur TensorFlow, facilitant le d\u00e9veloppement rapide de mod\u00e8les de Deep Learning. D\u00e9veloppement rapide de prototypes. Hugging Face Biblioth\u00e8que pour les mod\u00e8les de NLP pr\u00e9-entra\u00een\u00e9s, facilitant leur utilisation et leur fine-tuning. Utilisation de BERT, GPT et autres mod\u00e8les de langage. ONNX Format d'\u00e9change pour mod\u00e8les de Machine Learning, permettant l'interop\u00e9rabilit\u00e9 entre diff\u00e9rents frameworks. Transfert de mod\u00e8les entre TensorFlow, PyTorch, etc. TensorBoard Outil de visualisation pour TensorFlow, permettant de suivre les m\u00e9triques d'entra\u00eenement et de visualiser les graphes de mod\u00e8les. Suivi des m\u00e9triques d'entra\u00eenement. MLflow Plateforme pour g\u00e9rer le cycle de vie des mod\u00e8les de Machine Learning, incluant le suivi des exp\u00e9riences et la gestion des mod\u00e8les. Suivi des exp\u00e9riences, gestion des mod\u00e8les."},{"location":"module1/ressources/glossaire-dl/#applications-du-deep-learning","title":"Applications du Deep Learning","text":"Application Description Architecture typique Computer Vision Domaine du Deep Learning d\u00e9di\u00e9 \u00e0 l'analyse et la compr\u00e9hension d'images et de vid\u00e9os. CNN (ResNet, YOLO, EfficientNet). Natural Language Processing (NLP) Domaine du Deep Learning d\u00e9di\u00e9 au traitement et \u00e0 la g\u00e9n\u00e9ration de texte. Transformers (BERT, GPT, T5). Speech Recognition Conversion de la parole en texte \u00e0 l'aide de mod\u00e8les de Deep Learning. RNN, Transformers (Wav2Vec). Recommendation Systems Syst\u00e8mes qui sugg\u00e8rent du contenu personnalis\u00e9 en fonction des pr\u00e9f\u00e9rences de l'utilisateur. R\u00e9seaux de neurones profonds, embeddings. Generative AI Cr\u00e9ation de contenu nouveau (images, texte, audio) \u00e0 l'aide de mod\u00e8les de Deep Learning. GANs, Diffusion Models, Transformers. Reinforcement Learning Apprentissage par essai-erreur et r\u00e9compense, o\u00f9 un agent apprend \u00e0 prendre des d\u00e9cisions pour maximiser une r\u00e9compense. Deep Q-Networks, Policy Gradients. Time Series Analysis Pr\u00e9diction de valeurs futures dans des s\u00e9quences temporelles \u00e0 l'aide de mod\u00e8les de Deep Learning. LSTM, Transformers temporels."},{"location":"module1/ressources/glossaire-dl/#explications-des-termes-techniques","title":"Explications des termes techniques","text":""},{"location":"module1/ressources/glossaire-dl/#fonctions-dactivation","title":"Fonctions d'activation","text":"<ul> <li>ReLU (Rectified Linear Unit) : Fonction d'activation qui retourne 0 si l'entr\u00e9e est n\u00e9gative et l'entr\u00e9e elle-m\u00eame si elle est positive. Elle est couramment utilis\u00e9e dans les r\u00e9seaux de neurones pour introduire de la non-lin\u00e9arit\u00e9.</li> <li>Sigmoid : Fonction d'activation qui mappe les valeurs d'entr\u00e9e \u00e0 une plage de 0 \u00e0 1, souvent utilis\u00e9e pour les probl\u00e8mes de classification binaire.</li> <li>Tanh (Hyperbolic Tangent) : Fonction d'activation qui mappe les valeurs d'entr\u00e9e \u00e0 une plage de -1 \u00e0 1, souvent utilis\u00e9e dans les r\u00e9seaux r\u00e9currents.</li> </ul>"},{"location":"module1/ressources/glossaire-dl/#optimiseurs","title":"Optimiseurs","text":"<ul> <li>Adam (Adaptive Moment Estimation) : Algorithme d'optimisation qui combine les avantages de deux autres extensions de la descente de gradient stochastique, \u00e0 savoir AdaGrad et RMSProp. Il est largement utilis\u00e9 pour entra\u00eener des r\u00e9seaux de neurones.</li> <li>SGD (Stochastic Gradient Descent) : Algorithme d'optimisation qui met \u00e0 jour les poids du r\u00e9seau en utilisant une estimation stochastique du gradient de la fonction de perte.</li> <li>RMSprop : Algorithme d'optimisation qui adapte le taux d'apprentissage pour chaque param\u00e8tre, ce qui permet de stabiliser et d'acc\u00e9l\u00e9rer l'entra\u00eenement.</li> </ul>"},{"location":"module1/ressources/glossaire-dl/#modeles-de-langage","title":"Mod\u00e8les de langage","text":"<ul> <li>Word2Vec : Mod\u00e8le de langage qui apprend des repr\u00e9sentations vectorielles des mots (embeddings) en utilisant des r\u00e9seaux de neurones. Il est utilis\u00e9 pour capturer les relations s\u00e9mantiques entre les mots.</li> <li>GloVe (Global Vectors for Word Representation) : Mod\u00e8le de langage qui apprend des embeddings de mots en utilisant une matrice de co-occurrence des mots dans un corpus.</li> </ul>"},{"location":"module1/ressources/glossaire-dl/#modeles-de-reconnaissance-vocale","title":"Mod\u00e8les de reconnaissance vocale","text":"<ul> <li>Wav2Vec : Mod\u00e8le de reconnaissance vocale qui apprend des repr\u00e9sentations vectorielles des segments audio en utilisant des r\u00e9seaux de neurones. Il est utilis\u00e9 pour convertir la parole en texte.</li> </ul>"},{"location":"module1/ressources/glossaire-dl/#architectures-de-reseaux_1","title":"Architectures de r\u00e9seaux","text":"<ul> <li>ResNet (Residual Networks) : Architecture de r\u00e9seau de neurones convolutifs qui utilise des connexions r\u00e9siduelles pour permettre l'entra\u00eenement de r\u00e9seaux tr\u00e8s profonds sans d\u00e9gradation des performances.</li> <li>YOLO (You Only Look Once) : Architecture de r\u00e9seau de neurones convolutifs utilis\u00e9e pour la d\u00e9tection d'objets en temps r\u00e9el. Elle divise l'image en une grille et pr\u00e9dit des bo\u00eetes englobantes et des classes pour chaque cellule de la grille.</li> <li>EfficientNet : Architecture de r\u00e9seau de neurones convolutifs qui utilise une approche de mise \u00e0 l'\u00e9chelle compos\u00e9e pour optimiser la pr\u00e9cision et l'efficacit\u00e9 du mod\u00e8le.</li> </ul>"},{"location":"module1/ressources/glossaire-dl/#modeles-de-langage-avances","title":"Mod\u00e8les de langage avanc\u00e9s","text":"<ul> <li>BERT (Bidirectional Encoder Representations from Transformers) : Mod\u00e8le de langage bas\u00e9 sur les Transformers qui utilise des m\u00e9canismes d'attention bidirectionnelle pour capturer le contexte des mots dans une phrase. Il est largement utilis\u00e9 pour des t\u00e2ches de traitement du langage naturel.</li> <li>GPT (Generative Pre-trained Transformer) : Mod\u00e8le de langage bas\u00e9 sur les Transformers qui est pr\u00e9-entra\u00een\u00e9 sur un grand corpus de texte et peut \u00eatre fine-tun\u00e9 pour des t\u00e2ches sp\u00e9cifiques. Il est utilis\u00e9 pour la g\u00e9n\u00e9ration de texte et d'autres t\u00e2ches de traitement du langage naturel.</li> </ul>"},{"location":"module1/ressources/guide-colab/","title":"\ud83d\udcda Guide d'utilisation de Google Colab","text":""},{"location":"module1/ressources/guide-colab/#introduction-a-google-colab","title":"Introduction \u00e0 Google Colab","text":"<p>Google Colab (ou Colaboratory) est un environnement de notebook Jupyter h\u00e9berg\u00e9 par Google. Il permet d'ex\u00e9cuter du code Python dans votre navigateur et est particuli\u00e8rement adapt\u00e9 au machine learning, \u00e0 l'analyse de donn\u00e9es et \u00e0 l'\u00e9ducation.</p>"},{"location":"module1/ressources/guide-colab/#avantages-de-google-colab","title":"Avantages de Google Colab","text":"<ul> <li>Gratuit : pas besoin d'installer Python ou des biblioth\u00e8ques sur votre ordinateur</li> <li>Puissant : acc\u00e8s \u00e0 des GPU et TPU gratuits</li> <li>Collaboratif : facilit\u00e9 de partage et de travail en \u00e9quipe</li> <li>Pr\u00eat \u00e0 l'emploi : biblioth\u00e8ques populaires d\u00e9j\u00e0 install\u00e9es (TensorFlow, PyTorch, etc.)</li> </ul>"},{"location":"module1/ressources/guide-colab/#acceder-a-google-colab","title":"Acc\u00e9der \u00e0 Google Colab","text":"<ol> <li>Allez sur colab.research.google.com</li> <li>Connectez-vous avec votre compte Google</li> <li>Sur la page d'accueil, vous pouvez:</li> <li>Cr\u00e9er un nouveau notebook</li> <li>Ouvrir un notebook existant</li> <li>Acc\u00e9der \u00e0 des tutoriels</li> </ol>"},{"location":"module1/ressources/guide-colab/#interface-de-colab","title":"Interface de Colab","text":"<p>L'interface de Colab est compos\u00e9e de:</p> <ol> <li>Barre de menu : Fichier, \u00c9dition, Affichage, etc.</li> <li>Barre d'outils : actions rapides</li> <li>Panneau de cellules : o\u00f9 vous \u00e9crivez et ex\u00e9cutez votre code</li> <li>Panneau lat\u00e9ral : pour acc\u00e9der aux fichiers, tableaux, etc.</li> </ol>"},{"location":"module1/ressources/guide-colab/#types-de-cellules","title":"Types de cellules","text":"<p>Dans Colab, il existe deux types principaux de cellules:</p> <ul> <li>Cellules de code : pour ex\u00e9cuter du code Python</li> <li>Cellules de texte : pour \u00e9crire des commentaires en Markdown</li> </ul>"},{"location":"module1/ressources/guide-colab/#cellules-de-code","title":"Cellules de code","text":"<pre><code># Exemple de cellule de code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\nplt.plot(x, y)\nplt.title(\"Fonction sinus\")\nplt.show()\n</code></pre>"},{"location":"module1/ressources/guide-colab/#cellules-de-texte-markdown","title":"Cellules de texte (Markdown)","text":"<p>Les cellules de texte utilisent la syntaxe Markdown:</p> <pre><code># Titre principal\n## Sous-titre\n\nTexte normal avec **texte en gras** et *texte en italique*.\n\nListe \u00e0 puces:\n- Item 1\n- Item 2\n\n\u00c9quation math\u00e9matique: $y = mx + b$\n</code></pre>"},{"location":"module1/ressources/guide-colab/#executer-du-code","title":"Ex\u00e9cuter du code","text":"<p>Pour ex\u00e9cuter une cellule: - Cliquez sur le bouton \u25b6\ufe0f \u00e0 gauche de la cellule - Ou utilisez le raccourci clavier <code>Shift+Enter</code></p> <p>Le r\u00e9sultat s'affiche directement sous la cellule.</p>"},{"location":"module1/ressources/guide-colab/#raccourcis-clavier-utiles","title":"Raccourcis clavier utiles","text":"<ul> <li><code>Ctrl+Enter</code> : Ex\u00e9cuter la cellule</li> <li><code>Shift+Enter</code> : Ex\u00e9cuter la cellule et passer \u00e0 la suivante</li> <li><code>Alt+Enter</code> : Ex\u00e9cuter la cellule et ins\u00e9rer une nouvelle cellule en dessous</li> <li><code>Ctrl+M D</code> : Supprimer la cellule</li> <li><code>Ctrl+M A</code> : Ins\u00e9rer une cellule au-dessus</li> <li><code>Ctrl+M B</code> : Ins\u00e9rer une cellule en-dessous</li> <li><code>Ctrl+M M</code> : Transformer en cellule Markdown</li> <li><code>Ctrl+M Y</code> : Transformer en cellule de code</li> </ul>"},{"location":"module1/ressources/guide-colab/#utiliser-le-gputpu","title":"Utiliser le GPU/TPU","text":"<p>Pour acc\u00e9l\u00e9rer l'ex\u00e9cution de votre code:</p> <ol> <li>Cliquez sur <code>Modifier</code> &gt; <code>Param\u00e8tres du notebook</code></li> <li>Sous <code>Acc\u00e9l\u00e9rateur mat\u00e9riel</code>, s\u00e9lectionnez <code>GPU</code> ou <code>TPU</code></li> <li>Cliquez sur <code>Enregistrer</code></li> </ol>"},{"location":"module1/ressources/guide-colab/#installer-des-bibliotheques","title":"Installer des biblioth\u00e8ques","text":"<p>Colab poss\u00e8de d\u00e9j\u00e0 de nombreuses biblioth\u00e8ques install\u00e9es, mais vous pouvez en ajouter d'autres:</p> <pre><code>!pip install nom_de_la_biblioth\u00e8que\n</code></pre> <p>Exemple: <pre><code>!pip install transformers\n</code></pre></p> <p>Apr\u00e8s l'installation, red\u00e9marrez l'environnement d'ex\u00e9cution: 1. <code>Ex\u00e9cution</code> &gt; <code>Red\u00e9marrer l'environnement d'ex\u00e9cution...</code></p>"},{"location":"module1/ressources/guide-colab/#gerer-les-fichiers","title":"G\u00e9rer les fichiers","text":""},{"location":"module1/ressources/guide-colab/#importer-des-fichiers","title":"Importer des fichiers","text":"<ol> <li>Cliquez sur l'ic\u00f4ne \ud83d\udcc2 dans le panneau lat\u00e9ral gauche</li> <li>Cliquez sur <code>Importer</code> pour t\u00e9l\u00e9charger un fichier</li> </ol> <p>Ou via le code: <pre><code>from google.colab import files\nuploaded = files.upload()\n</code></pre></p>"},{"location":"module1/ressources/guide-colab/#acceder-aux-fichiers-de-google-drive","title":"Acc\u00e9der aux fichiers de Google Drive","text":"<pre><code>from google.colab import drive\ndrive.mount('/content/drive')\n\n# Acc\u00e9der aux fichiers dans Drive\n!ls \"/content/drive/My Drive\"\n</code></pre>"},{"location":"module1/ressources/guide-colab/#telecharger-des-fichiers","title":"T\u00e9l\u00e9charger des fichiers","text":"<pre><code>from google.colab import files\nfiles.download('nom_du_fichier.ext')\n</code></pre>"},{"location":"module1/ressources/guide-colab/#enregistrer-votre-travail","title":"Enregistrer votre travail","text":"<p>Colab enregistre automatiquement votre travail dans Google Drive, mais vous pouvez aussi:</p> <ol> <li><code>Fichier</code> &gt; <code>Enregistrer une copie dans Drive</code></li> <li><code>Fichier</code> &gt; <code>T\u00e9l\u00e9charger</code> &gt; <code>T\u00e9l\u00e9charger .ipynb</code></li> </ol>"},{"location":"module1/ressources/guide-colab/#partager-un-notebook","title":"Partager un notebook","text":"<ol> <li>Cliquez sur <code>Partager</code> en haut \u00e0 droite</li> <li>Entrez les adresses e-mail ou obtenez un lien de partage</li> <li>D\u00e9finissez les autorisations d'acc\u00e8s (Lecteur ou \u00c9diteur)</li> </ol>"},{"location":"module1/ressources/guide-colab/#depannage-courant","title":"D\u00e9pannage courant","text":""},{"location":"module1/ressources/guide-colab/#erreur-cuda-out-of-memory","title":"Erreur \"CUDA out of memory\"","text":"<ul> <li>Red\u00e9marrez l'environnement d'ex\u00e9cution (Ex\u00e9cution &gt; Red\u00e9marrer...)</li> <li>R\u00e9duisez la taille de votre mod\u00e8le ou de vos donn\u00e9es</li> <li>Utilisez un lot (batch) plus petit</li> </ul>"},{"location":"module1/ressources/guide-colab/#deconnexion-apres-inactivite","title":"D\u00e9connexion apr\u00e8s inactivit\u00e9","text":"<ul> <li>Colab se d\u00e9connecte apr\u00e8s environ 90 minutes d'inactivit\u00e9</li> <li>Utilisez <code>Outils</code> &gt; <code>Param\u00e8tres</code> &gt; <code>Param\u00e8tres avanc\u00e9s</code> &gt; <code>D\u00e9sactiver l'interruption apr\u00e8s inactivit\u00e9</code></li> </ul>"},{"location":"module1/ressources/guide-colab/#limites-de-temps-dexecution","title":"Limites de temps d'ex\u00e9cution","text":"<ul> <li>Les sessions sont limit\u00e9es \u00e0 environ 12 heures</li> <li>Pour des calculs plus longs, enregistrez p\u00e9riodiquement votre travail</li> </ul>"},{"location":"module1/ressources/guide-colab/#perte-de-variables","title":"Perte de variables","text":"<ul> <li>Si vous ex\u00e9cutez les cellules dans un ordre diff\u00e9rent, certaines variables peuvent \u00eatre perdues</li> <li>Mieux vaut ex\u00e9cuter les cellules dans l'ordre s\u00e9quentiel</li> </ul>"},{"location":"module1/ressources/guide-colab/#astuces-pour-les-tps-de-deep-learning","title":"Astuces pour les TPs de Deep Learning","text":"<ol> <li> <p>V\u00e9rifiez l'acc\u00e9l\u00e9rateur mat\u00e9riel avant de commencer un entra\u00eenement lourd    <pre><code>import tensorflow as tf\nprint(\"GPU disponible:\", tf.config.list_physical_devices('GPU'))\n</code></pre></p> </li> <li> <p>Sauvegardez vos mod\u00e8les r\u00e9guli\u00e8rement    <pre><code>model.save('mon_modele.h5')\n</code></pre></p> </li> <li> <p>Visualisez vos donn\u00e9es avant l'entra\u00eenement    <pre><code>import matplotlib.pyplot as plt\nplt.imshow(X_train[0])\nplt.show()\n</code></pre></p> </li> <li> <p>Utilisez tqdm pour les barres de progression    <pre><code>!pip install tqdm\nfrom tqdm.notebook import tqdm\n\nfor epoch in tqdm(range(100)):\n    # votre boucle d'entra\u00eenement\n</code></pre></p> </li> <li> <p>Profitez de TensorBoard <pre><code>%load_ext tensorboard\n%tensorboard --logdir logs\n</code></pre></p> </li> </ol>"},{"location":"module1/ressources/guide-colab/#ressources-supplementaires","title":"Ressources suppl\u00e9mentaires","text":"<ul> <li>Documentation officielle de Google Colab</li> <li>Tutoriels TensorFlow dans Colab</li> <li>Tutoriels PyTorch dans Colab</li> </ul> <p>Bonne exploration et bon apprentissage du Deep Learning avec Google Colab!</p>"},{"location":"module1/ressources/hello-world-dl/","title":"Hello World du Deep Learning - Reconnaissance de chiffres manuscrits","text":"<p>Ce notebook vous guidera \u00e0 travers la cr\u00e9ation, l'entra\u00eenement et l'utilisation d'un r\u00e9seau de neurones pour reconna\u00eetre des chiffres manuscrits. Copiez chaque cellule dans votre notebook Google Colab et ex\u00e9cutez-les dans l'ordre.</p>"},{"location":"module1/ressources/hello-world-dl/#cellule-1-introduction-cellule-markdown","title":"Cellule 1 : Introduction (Cellule Markdown)","text":"<pre><code># \ud83d\ude80 Hello World du Deep Learning\n\n## Reconnaissance de chiffres manuscrits avec TensorFlow et Keras\n\n### Objectifs de ce notebook\n\n- Charger et pr\u00e9parer un jeu de donn\u00e9es de chiffres manuscrits\n- Cr\u00e9er un r\u00e9seau de neurones simple\n- Entra\u00eener le mod\u00e8le\n- Visualiser les r\u00e9sultats\n- Tester le mod\u00e8le avec vos propres dessins\n\n### BTS SIO - D\u00e9couverte du Deep Learning\n</code></pre>"},{"location":"module1/ressources/hello-world-dl/#cellule-2-configuration-cellule-code","title":"Cellule 2 : Configuration (Cellule Code)","text":"<pre><code># Importation des biblioth\u00e8ques n\u00e9cessaires\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# V\u00e9rification de la version de TensorFlow\nprint(f\"TensorFlow version: {tf.__version__}\")\nprint(f\"Keras version: {keras.__version__}\")\n\n# V\u00e9rification du GPU (m\u00e9thode recommand\u00e9e)\nprint(\"GPU disponible :\", len(tf.config.list_physical_devices('GPU')) &gt; 0)\n</code></pre>"},{"location":"module1/ressources/hello-world-dl/#cellule-3-chargement-des-donnees-cellule-code","title":"Cellule 3 : Chargement des donn\u00e9es (Cellule Code)","text":"<pre><code># Chargement du dataset MNIST\n(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n\n# Pr\u00e9traitement des donn\u00e9es\nX_train = X_train.reshape((60000, 28, 28, 1)) / 255.0\nX_test = X_test.reshape((10000, 28, 28, 1)) / 255.0\n\n# Conversion des labels en cat\u00e9gories\ny_train = keras.utils.to_categorical(y_train)\ny_test = keras.utils.to_categorical(y_test)\n\n# Affichage de quelques exemples\nplt.figure(figsize=(10, 2))\nfor i in range(10):\n    plt.subplot(1, 10, i+1)\n    plt.imshow(X_train[i].reshape(28, 28), cmap='gray')\n    plt.axis('off')\nplt.suptitle(\"Exemples de chiffres manuscrits\")\nplt.show()\n\nprint(f\"Nombre d'exemples d'entra\u00eenement : {X_train.shape[0]}\")\nprint(f\"Nombre d'exemples de test : {X_test.shape[0]}\")\nprint(f\"Dimensions d'une image : {X_train.shape[1:3]}\")\nprint(f\"Valeurs des pixels apr\u00e8s normalisation : de 0 \u00e0 1\")\n</code></pre>"},{"location":"module1/ressources/hello-world-dl/#cellule-4-creation-du-modele-cellule-code","title":"Cellule 4 : Cr\u00e9ation du mod\u00e8le (Cellule Code)","text":"<pre><code># Cr\u00e9ation du mod\u00e8le de r\u00e9seau de neurones\n# On utilise Input comme premi\u00e8re couche (recommand\u00e9)\ninputs = keras.Input(shape=(28, 28, 1))\n\n# Couche de convolution\nx = layers.Conv2D(32, (3, 3), activation='relu')(inputs)\nx = layers.MaxPooling2D((2, 2))(x)\n\n# Couche de convolution suppl\u00e9mentaire\nx = layers.Conv2D(64, (3, 3), activation='relu')(x)\nx = layers.MaxPooling2D((2, 2))(x)\n\n# Aplatissement\nx = layers.Flatten()(x)\n\n# Couche dense\nx = layers.Dense(64, activation='relu')(x)\n\n# Couche de sortie\noutputs = layers.Dense(10, activation='softmax')(x)\n\n# Cr\u00e9ation du mod\u00e8le\nmodel = keras.Model(inputs, outputs, name=\"mnist_model\")\n\n# Compilation du mod\u00e8le\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# Affichage du r\u00e9sum\u00e9 du mod\u00e8le\nmodel.summary()\n</code></pre>"},{"location":"module1/ressources/hello-world-dl/#cellule-5-entrainement-cellule-code","title":"Cellule 5 : Entra\u00eenement (Cellule Code)","text":"<pre><code># Entra\u00eenement du mod\u00e8le\n# Note : Nombre d'\u00e9poques r\u00e9duit pour la d\u00e9monstration\nhistory = model.fit(\n    X_train, y_train,\n    epochs=5,\n    batch_size=64,\n    validation_split=0.2,\n    verbose=1\n)\n\n# \u00c9valuation du mod\u00e8le\ntest_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\nprint(f\"\\nPr\u00e9cision sur l'ensemble de test : {test_accuracy*100:.2f}%\")\n</code></pre>"},{"location":"module1/ressources/hello-world-dl/#cellule-6-visualisation-cellule-code","title":"Cellule 6 : Visualisation (Cellule Code)","text":"<pre><code># Visualisation de la pr\u00e9cision et de la perte\nplt.figure(figsize=(12, 4))\n\n# Pr\u00e9cision\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Pr\u00e9cision entra\u00eenement')\nplt.plot(history.history['val_accuracy'], label='Pr\u00e9cision validation')\nplt.title('Pr\u00e9cision du mod\u00e8le')\nplt.xlabel('\u00c9poque')\nplt.ylabel('Pr\u00e9cision')\nplt.legend()\n\n# Perte\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Perte entra\u00eenement')\nplt.plot(history.history['val_loss'], label='Perte validation')\nplt.title('Perte du mod\u00e8le')\nplt.xlabel('\u00c9poque')\nplt.ylabel('Perte')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"module1/ressources/hello-world-dl/#cellule-7-predictions-cellule-code","title":"Cellule 7 : Pr\u00e9dictions (Cellule Code)","text":"<pre><code># Pr\u00e9dictions et visualisation\n# Pr\u00e9dire sur quelques images de test\npredictions = model.predict(X_test[:10])\n\nplt.figure(figsize=(15, 6))\nfor i in range(10):\n    plt.subplot(2, 10, i+1)\n    plt.imshow(X_test[i].reshape(28, 28), cmap='gray')\n    plt.title(f\"R\u00e9el: {np.argmax(y_test[i])}\")\n    plt.axis('off')\n\n    plt.subplot(2, 10, i+11)\n    plt.bar(range(10), predictions[i])\n    plt.title(f\"Pr\u00e9dit: {np.argmax(predictions[i])}\")\n    plt.xticks(range(10))\n    plt.ylim(0, 1)\n\nplt.suptitle(\"Pr\u00e9dictions du mod\u00e8le\")\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"module1/ressources/hello-world-dl/#cellule-8-dessin-interactif-cellule-code","title":"Cellule 8 : Dessin interactif (Cellule Code)","text":"<pre><code># Interface interactive pour dessiner et pr\u00e9dire\n# Cette cellule permet de dessiner un chiffre directement dans Colab\n\nfrom google.colab import output\nfrom IPython.display import display, HTML\nimport io\nimport base64\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Fonction pour cr\u00e9er un canvas HTML\ndef create_canvas():\n    canvas_html = \"\"\"\n    &lt;canvas id=\"canvas\" width=\"280\" height=\"280\" style=\"border: 2px solid black; background-color: white;\"&gt;&lt;/canvas&gt;\n    &lt;div style=\"margin-top: 10px;\"&gt;\n      &lt;button id=\"predict_button\" style=\"padding: 5px 10px; background-color: #4CAF50; color: white; border: none; border-radius: 4px; cursor: pointer;\"&gt;Pr\u00e9dire&lt;/button&gt;\n      &lt;button id=\"clear_button\" style=\"margin-left: 10px; padding: 5px 10px; background-color: #f44336; color: white; border: none; border-radius: 4px; cursor: pointer;\"&gt;Effacer&lt;/button&gt;\n    &lt;/div&gt;\n    &lt;div id=\"result\" style=\"margin-top: 10px; font-weight: bold;\"&gt;&lt;/div&gt;\n\n    &lt;script&gt;\n      var canvas = document.getElementById('canvas');\n      var ctx = canvas.getContext('2d');\n      var isDrawing = false;\n\n      // Remplir le fond en blanc d\u00e8s le d\u00e9part\n      ctx.fillStyle = \"white\";\n      ctx.fillRect(0, 0, canvas.width, canvas.height);\n\n      ctx.lineWidth = 15;\n      ctx.lineCap = 'round';\n      ctx.lineJoin = 'round';\n      ctx.strokeStyle = 'black';\n\n      canvas.addEventListener('mousedown', function(e) {\n        isDrawing = true;\n        ctx.beginPath();\n        ctx.moveTo(e.clientX - canvas.getBoundingClientRect().left, e.clientY - canvas.getBoundingClientRect().top);\n      });\n\n      canvas.addEventListener('mousemove', function(e) {\n        if (isDrawing) {\n          ctx.lineTo(e.clientX - canvas.getBoundingClientRect().left, e.clientY - canvas.getBoundingClientRect().top);\n          ctx.stroke();\n        }\n      });\n\n      canvas.addEventListener('mouseup', function() {\n        isDrawing = false;\n      });\n\n      canvas.addEventListener('mouseleave', function() {\n        isDrawing = false;\n      });\n\n      document.getElementById('clear_button').addEventListener('click', function() {\n        ctx.fillStyle = \"white\";\n        ctx.fillRect(0, 0, canvas.width, canvas.height);\n        document.getElementById('result').innerHTML = '';\n      });\n\n      document.getElementById('predict_button').addEventListener('click', function() {\n        var imageData = canvas.toDataURL('image/png');\n        document.getElementById('result').innerHTML = 'Analyse en cours...';\n        google.colab.kernel.invokeFunction('notebook.predict', [imageData], {});\n      });\n    &lt;/script&gt;\n    \"\"\"\n    return canvas_html\n\n# Fonction pour pr\u00e9traiter l'image dessin\u00e9e\ndef preprocess_image(image_data):\n    image_data = image_data.split(',')[1]\n    image = Image.open(io.BytesIO(base64.b64decode(image_data)))\n\n    # Convertir en niveaux de gris et redimensionner\n    image = image.convert('L').resize((28, 28))\n\n    # Convertir en tableau numpy\n    image_array = np.array(image)\n\n    # V\u00e9rification si inversion des couleurs est n\u00e9cessaire\n    if np.mean(image_array) &gt; 127:  # Fond clair, chiffre sombre\n        image_array = 255 - image_array\n\n    # Normaliser les pixels\n    image_array = image_array / 255.0\n\n    return image_array\n\n# Fonction de pr\u00e9diction\ndef predict_digit(image_data):\n    image_array = preprocess_image(image_data)\n    image_array = image_array.reshape(1, 28, 28, 1)\n\n    # Affichage de l'image pr\u00e9trait\u00e9e pour v\u00e9rifier\n    plt.figure(figsize=(6, 3))\n    plt.subplot(1, 2, 1)\n    plt.imshow(image_array.reshape(28, 28), cmap='gray')\n    plt.title(\"Image pr\u00e9trait\u00e9e\")\n    plt.axis('off')\n\n    # Pr\u00e9diction\n    prediction = model.predict(image_array)[0]\n    digit = np.argmax(prediction)\n    confidence = prediction[digit] * 100\n\n    # Affichage du graphique des probabilit\u00e9s\n    plt.subplot(1, 2, 2)\n    plt.bar(range(10), prediction)\n    plt.title(f\"Pr\u00e9diction: {digit}\")\n    plt.xlabel(\"Chiffre\")\n    plt.ylabel(\"Confiance\")\n    plt.xticks(range(10))\n    plt.show()\n\n    # Afficher le r\u00e9sultat dans le notebook\n    output.eval_js(f\"\"\"\n    document.getElementById('result').innerHTML = 'Pr\u00e9diction: {digit} (Confiance: {confidence:.2f}%)';\n    \"\"\")\n\n# Enregistrer la fonction pour \u00eatre appel\u00e9e depuis JavaScript\noutput.register_callback('notebook.predict', predict_digit)\n\n# Afficher le canvas\ndisplay(HTML(create_canvas()))\nprint(\"Dessinez un chiffre dans le canvas ci-dessus et cliquez sur 'Pr\u00e9dire'\")\n</code></pre>"},{"location":"module1/ressources/hello-world-dl/#cellule-9-experimentation-cellule-markdown","title":"Cellule 9 : Exp\u00e9rimentation (Cellule Markdown)","text":"<pre><code>## \ud83e\uddea Exp\u00e9rimentations\n\nVoici quelques modifications que vous pouvez essayer pour am\u00e9liorer ou observer les effets sur le mod\u00e8le :\n\n1. **Modifier l'architecture du r\u00e9seau :**\n   - Augmenter/diminuer le nombre de neurones dans chaque couche\n   - Ajouter ou supprimer des couches dans le r\u00e9seau\n   - Essayer d'ajouter une couche Dropout (qui d\u00e9sactive al\u00e9atoirement certains neurones pendant l'entra\u00eenement)\n\n2. **Ajuster les param\u00e8tres d'entra\u00eenement :**\n   - Changer le nombre de cycles d'entra\u00eenement (\u00e9poques)\n   - Modifier le nombre d'exemples trait\u00e9s \u00e0 la fois (taille du batch)\n   - Tester diff\u00e9rentes m\u00e9thodes d'apprentissage (optimiseurs)\n\nPour chaque modification, observez l'impact sur :\n- La pr\u00e9cision finale\n- La vitesse d'entra\u00eenement\n- Les courbes d'apprentissage\n- Le comportement face \u00e0 vos propres dessins\n\nN'h\u00e9sitez pas \u00e0 documenter vos observations dans la fiche fournie.\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/","title":"Machine Learning Classique - Classification MNIST avec Random Forest","text":"<p>Ce document contient le code et les explications pour le notebook de classification d'images MNIST avec Random Forest (approche Machine Learning classique). Vous pouvez copier-coller chaque section dans une cellule Google Colab.</p>"},{"location":"module1/ressources/machine-learning-classique/#cellule-1-markdown-introduction","title":"Cellule 1 (Markdown) - Introduction","text":"<pre><code># Classification avec Machine Learning classique\n\n## Reconnaissance de chiffres manuscrits avec Random Forest\n\nDans ce notebook, nous allons impl\u00e9menter une approche de Machine Learning classique pour la classification des chiffres manuscrits en utilisant le dataset MNIST. Nous utiliserons l'algorithme Random Forest, qui est bas\u00e9 sur un ensemble d'arbres de d\u00e9cision.\n\n### Objectifs :\n- Comprendre comment pr\u00e9parer des donn\u00e9es d'images pour le ML classique\n- Impl\u00e9menter un classificateur Random Forest\n- \u00c9valuer ses performances et ses limites\n- Comparer cette approche avec le Deep Learning\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-2-code-importation-des-bibliotheques","title":"Cellule 2 (Code) - Importation des biblioth\u00e8ques","text":"<pre><code># Importation des biblioth\u00e8ques n\u00e9cessaires\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport time\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\nfrom sklearn.pipeline import Pipeline\nfrom google.colab import output\noutput.enable_custom_widget_manager()\nimport ipywidgets as widgets\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-3-markdown-chargement-des-donnees","title":"Cellule 3 (Markdown) - Chargement des donn\u00e9es","text":"<pre><code>## Chargement et exploration des donn\u00e9es\n\nLe dataset MNIST contient 70 000 images de chiffres manuscrits (0-9) en niveaux de gris. Chaque image est de taille 28x28 pixels, ce qui donne 784 pixels par image.\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-4-code-chargement-des-donnees-mnist","title":"Cellule 4 (Code) - Chargement des donn\u00e9es MNIST","text":"<pre><code>print(\"Chargement du jeu de donn\u00e9es MNIST...\")\n# Utilisation du jeu de donn\u00e9es MNIST int\u00e9gr\u00e9 \u00e0 sklearn\nfrom sklearn.datasets import fetch_openml\nmnist = fetch_openml('mnist_784', version=1, as_frame=False)\nX, y = mnist[\"data\"], mnist[\"target\"]\nX = X / 255.0  # Normalisation des valeurs de pixels entre 0 et 1\ny = y.astype(np.uint8)  # Conversion des labels en entiers\n\n# Exploration des donn\u00e9es\nprint(f\"Dimensions du jeu de donn\u00e9es: {X.shape}\")\nprint(f\"Nombre de classes: {len(np.unique(y))}\")\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-5-code-visualisation-des-exemples","title":"Cellule 5 (Code) - Visualisation des exemples","text":"<pre><code># Affichage de quelques exemples\nplt.figure(figsize=(10, 5))\nfor i in range(10):\n    plt.subplot(2, 5, i + 1)\n    plt.imshow(X[i].reshape(28, 28), cmap='gray')\n    plt.title(f\"Label: {y[i]}\")\n    plt.axis('off')\nplt.tight_layout()\nplt.suptitle(\"Exemples de chiffres manuscrits\", y=1.05)\nplt.show()\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-6-markdown-preparation-des-donnees","title":"Cellule 6 (Markdown) - Pr\u00e9paration des donn\u00e9es","text":"<pre><code>## Pr\u00e9paration des donn\u00e9es pour Machine Learning classique\n\nContrairement aux r\u00e9seaux de neurones convolutifs (CNN), les algorithmes de ML classiques comme Random Forest ne sont pas con\u00e7us pour traiter directement des images. Nous devons donc :\n\n1. R\u00e9duire la dimensionnalit\u00e9 des donn\u00e9es (784 caract\u00e9ristiques est trop \u00e9lev\u00e9)\n2. Extraire des caract\u00e9ristiques pertinentes\n\nNous utiliserons l'Analyse en Composantes Principales (PCA) pour r\u00e9duire la dimensionnalit\u00e9 tout en conservant l'essentiel de l'information.\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-7-code-preparation-des-donnees","title":"Cellule 7 (Code) - Pr\u00e9paration des donn\u00e9es","text":"<pre><code>print(\"\\n--- Pr\u00e9paration des donn\u00e9es pour Random Forest ---\")\nprint(\"Pour le Machine Learning classique, nous devons souvent extraire des caract\u00e9ristiques manuellement.\")\n\n# R\u00e9duction de dimension avec PCA pour acc\u00e9l\u00e9rer l'entra\u00eenement\nprint(\"Application d'une r\u00e9duction de dimension (PCA)...\")\nn_components = 50  # R\u00e9duire de 784 \u00e0 50 caract\u00e9ristiques\n\n# S\u00e9paration en ensembles d'entra\u00eenement et de test\n# Utilisation d'un \u00e9chantillon r\u00e9duit pour acc\u00e9l\u00e9rer la d\u00e9monstration\nX_sample = X[:10000]\ny_sample = y[:10000]\n\nX_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.2, random_state=42)\n\nprint(f\"Taille de l'ensemble d'entra\u00eenement: {X_train.shape}\")\nprint(f\"Taille de l'ensemble de test: {X_test.shape}\")\n\n# Cr\u00e9ation d'un pipeline d'extraction de caract\u00e9ristiques\nfeature_pipeline = Pipeline([\n    ('scaler', StandardScaler()),  # Normalisation des donn\u00e9es\n    ('pca', PCA(n_components=n_components))  # R\u00e9duction de dimension par PCA\n])\n\n# Application aux donn\u00e9es\nprint(\"Extraction de caract\u00e9ristiques...\")\nX_train_features = feature_pipeline.fit_transform(X_train)\nX_test_features = feature_pipeline.transform(X_test)\n\nprint(f\"Dimensions apr\u00e8s extraction de caract\u00e9ristiques: {X_train_features.shape}\")\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-8-markdown-entrainement-du-modele","title":"Cellule 8 (Markdown) - Entra\u00eenement du mod\u00e8le","text":"<pre><code>## Entra\u00eenement du mod\u00e8le Random Forest\n\nNous allons maintenant entra\u00eener un classificateur Random Forest sur nos donn\u00e9es pr\u00e9trait\u00e9es. Random Forest est un algorithme d'ensemble qui combine les pr\u00e9dictions de plusieurs arbres de d\u00e9cision pour am\u00e9liorer la pr\u00e9cision et contr\u00f4ler le sur-apprentissage.\n\nPrincipaux hyperparam\u00e8tres :\n- **n_estimators** : Nombre d'arbres dans la for\u00eat\n- **max_depth** : Profondeur maximale de chaque arbre\n- **min_samples_split** : Nombre minimum d'\u00e9chantillons requis pour diviser un n\u0153ud\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-9-code-entrainement-du-modele","title":"Cellule 9 (Code) - Entra\u00eenement du mod\u00e8le","text":"<pre><code>print(\"\\n--- Entra\u00eenement du mod\u00e8le Random Forest ---\")\n\n# Param\u00e8tres du mod\u00e8le - vous pouvez les modifier\nn_estimators = 100  # Nombre d'arbres\nmax_depth = 10      # Profondeur maximale des arbres\nmin_samples_split = 2  # Nombre minimum d'\u00e9chantillons requis pour diviser un n\u0153ud\n\n# Cr\u00e9ation du mod\u00e8le\nrf_model = RandomForestClassifier(\n    n_estimators=n_estimators,\n    max_depth=max_depth,\n    min_samples_split=min_samples_split,\n    random_state=42,\n    n_jobs=-1  # Utiliser tous les c\u0153urs disponibles\n)\n\n# Mesure du temps d'entra\u00eenement\nstart_time = time.time()\nprint(\"Entra\u00eenement du mod\u00e8le en cours...\")\nrf_model.fit(X_train_features, y_train)\nend_time = time.time()\ntraining_time = end_time - start_time\n\nprint(f\"Temps d'entra\u00eenement: {training_time:.2f} secondes\")\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-10-markdown-evaluation-du-modele","title":"Cellule 10 (Markdown) - \u00c9valuation du mod\u00e8le","text":"<pre><code>## \u00c9valuation du mod\u00e8le\n\n\u00c9valuons maintenant les performances de notre mod\u00e8le Random Forest sur l'ensemble de test. Nous utiliserons plusieurs m\u00e9triques :\n- Pr\u00e9cision globale (accuracy)\n- Matrice de confusion\n- Rapport de classification d\u00e9taill\u00e9 (pr\u00e9cision, rappel, F1-score pour chaque classe)\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-11-code-evaluation-et-metriques","title":"Cellule 11 (Code) - \u00c9valuation et m\u00e9triques","text":"<pre><code>print(\"\\n--- \u00c9valuation du mod\u00e8le Random Forest ---\")\n\n# Pr\u00e9dictions sur l'ensemble de test\ny_pred = rf_model.predict(X_test_features)\n\n# Calcul des m\u00e9triques\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nclass_report = classification_report(y_test, y_pred)\n\nprint(f\"Pr\u00e9cision globale: {accuracy*100:.2f}%\")\nprint(\"\\nMatrice de confusion:\")\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Pr\u00e9dictions')\nplt.ylabel('Valeurs r\u00e9elles')\nplt.title('Matrice de confusion')\nplt.show()\n\nprint(\"\\nRapport de classification:\")\nprint(class_report)\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-12-markdown-analyse-des-erreurs","title":"Cellule 12 (Markdown) - Analyse des erreurs","text":"<pre><code>## Analyse des erreurs\n\nExaminons quelques exemples que notre mod\u00e8le a mal classifi\u00e9s pour mieux comprendre ses limites.\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-13-code-visualisation-des-erreurs","title":"Cellule 13 (Code) - Visualisation des erreurs","text":"<pre><code>print(\"\\n--- Analyse des erreurs ---\")\n\n# Identifier les erreurs\nerror_indices = np.where(y_pred != y_test)[0]\nn_errors = min(10, len(error_indices))  # Afficher max 10 erreurs\n\nif n_errors &gt; 0:\n    plt.figure(figsize=(12, 4))\n    for i, idx in enumerate(error_indices[:n_errors]):\n        plt.subplot(2, 5, i + 1)\n        # R\u00e9cup\u00e9rer l'image originale\n        img = X_test[idx].reshape(28, 28)\n        plt.imshow(img, cmap='gray')\n        plt.title(f\"R\u00e9el: {y_test[idx]}\\nPr\u00e9dit: {y_pred[idx]}\")\n        plt.axis('off')\n    plt.tight_layout()\n    plt.suptitle(\"Exemples d'erreurs de classification\", y=1.05)\n    plt.show()\nelse:\n    print(\"Aucune erreur trouv\u00e9e dans l'\u00e9chantillon de test!\")\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-14-markdown-importance-des-caracteristiques","title":"Cellule 14 (Markdown) - Importance des caract\u00e9ristiques","text":"<pre><code>## Importance des caract\u00e9ristiques\n\nUn avantage des mod\u00e8les comme Random Forest est leur interpr\u00e9tabilit\u00e9. Nous pouvons examiner quelles caract\u00e9ristiques (ici, quelles composantes principales) le mod\u00e8le consid\u00e8re comme les plus importantes pour faire ses pr\u00e9dictions.\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-15-code-visualisation-de-limportance-des-caracteristiques","title":"Cellule 15 (Code) - Visualisation de l'importance des caract\u00e9ristiques","text":"<pre><code>print(\"\\n--- Importance des caract\u00e9ristiques ---\")\n# Visualiser l'importance des composantes principales\nfeature_importance = rf_model.feature_importances_\nsorted_idx = np.argsort(feature_importance)[::-1]\n\nplt.figure(figsize=(10, 5))\nplt.bar(range(20), feature_importance[sorted_idx[:20]])\nplt.xticks(range(20), [f\"Feat {i}\" for i in sorted_idx[:20]], rotation=90)\nplt.xlabel('Composantes principales')\nplt.ylabel('Importance')\nplt.title('Top 20 des composantes principales les plus importantes')\nplt.tight_layout()\nplt.show()\n\nprint(\"Les caract\u00e9ristiques les plus importantes sont les composantes principales qui capturent le plus de variance dans les donn\u00e9es.\")\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-16-markdown-test-de-generalisation","title":"Cellule 16 (Markdown) - Test de g\u00e9n\u00e9ralisation","text":"<pre><code>## Test de g\u00e9n\u00e9ralisation\n\nUne question fondamentale en Machine Learning est : \"Comment le mod\u00e8le se comporte-t-il face \u00e0 des donn\u00e9es l\u00e9g\u00e8rement diff\u00e9rentes de celles sur lesquelles il a \u00e9t\u00e9 entra\u00een\u00e9 ?\"\n\nTestons la robustesse de notre mod\u00e8le face \u00e0 deux types de perturbations :\n1. Ajout de bruit al\u00e9atoire\n2. Rotation des images\n\nCes tests simulent des conditions plus r\u00e9alistes o\u00f9 les donn\u00e9es peuvent varier l\u00e9g\u00e8rement.\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-17-code-creation-de-donnees-modifiees","title":"Cellule 17 (Code) - Cr\u00e9ation de donn\u00e9es modifi\u00e9es","text":"<pre><code>print(\"\\n--- D\u00e9fi de g\u00e9n\u00e9ralisation ---\")\nprint(\"Nous allons maintenant tester le mod\u00e8le sur des chiffres l\u00e9g\u00e8rement modifi\u00e9s pour \u00e9valuer sa capacit\u00e9 de g\u00e9n\u00e9ralisation.\")\n\n# Fonction pour ajouter du bruit aux images\ndef add_noise(images, noise_level=0.2):\n    noisy_images = images.copy()\n    noise = np.random.normal(0, noise_level, images.shape)\n    noisy_images = noisy_images + noise\n    # Assurer que les valeurs restent entre 0 et 1\n    noisy_images = np.clip(noisy_images, 0, 1)\n    return noisy_images\n\n# Fonction pour appliquer une rotation aux images\ndef rotate_images(images, max_angle=15):\n    from scipy.ndimage import rotate\n    rotated_images = np.zeros_like(images)\n    for i, img in enumerate(images):\n        angle = np.random.uniform(-max_angle, max_angle)\n        img_2d = img.reshape(28, 28)\n        rotated = rotate(img_2d, angle, reshape=False)\n        rotated_images[i] = rotated.flatten()\n    return rotated_images\n\n# Cr\u00e9er un jeu de donn\u00e9es modifi\u00e9\nprint(\"Cr\u00e9ation d'un jeu de donn\u00e9es avec des chiffres modifi\u00e9s...\")\n\n# Utiliser la partie restante des donn\u00e9es pour ce test\nX_new = X[10000:12000]\ny_new = y[10000:12000]\n\n# Appliquer des transformations\nX_new_noisy = add_noise(X_new, noise_level=0.2)\nX_new_rotated = rotate_images(X_new, max_angle=15)\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-18-code-visualisation-des-donnees-modifiees","title":"Cellule 18 (Code) - Visualisation des donn\u00e9es modifi\u00e9es","text":"<pre><code># Visualiser quelques exemples\nplt.figure(figsize=(12, 8))\nfor i in range(5):\n    # Original\n    plt.subplot(3, 5, i + 1)\n    plt.imshow(X_new[i].reshape(28, 28), cmap='gray')\n    plt.title(f\"Original: {y_new[i]}\")\n    plt.axis('off')\n\n    # Avec bruit\n    plt.subplot(3, 5, i + 6)\n    plt.imshow(X_new_noisy[i].reshape(28, 28), cmap='gray')\n    plt.title(\"Avec bruit\")\n    plt.axis('off')\n\n    # Avec rotation\n    plt.subplot(3, 5, i + 11)\n    plt.imshow(X_new_rotated[i].reshape(28, 28), cmap='gray')\n    plt.title(\"Avec rotation\")\n    plt.axis('off')\n\nplt.tight_layout()\nplt.suptitle(\"Exemples de chiffres modifi\u00e9s\", y=1.02)\nplt.show()\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-19-code-evaluation-sur-donnees-modifiees","title":"Cellule 19 (Code) - \u00c9valuation sur donn\u00e9es modifi\u00e9es","text":"<pre><code># \u00c9valuer le mod\u00e8le sur les donn\u00e9es modifi\u00e9es\nprint(\"\\n\u00c9valuation sur les donn\u00e9es avec bruit:\")\nX_new_noisy_features = feature_pipeline.transform(X_new_noisy)\ny_new_noisy_pred = rf_model.predict(X_new_noisy_features)\naccuracy_noisy = accuracy_score(y_new, y_new_noisy_pred)\nprint(f\"Pr\u00e9cision sur donn\u00e9es bruit\u00e9es: {accuracy_noisy*100:.2f}%\")\n\nprint(\"\\n\u00c9valuation sur les donn\u00e9es avec rotation:\")\nX_new_rotated_features = feature_pipeline.transform(X_new_rotated)\ny_new_rotated_pred = rf_model.predict(X_new_rotated_features)\naccuracy_rotated = accuracy_score(y_new, y_new_rotated_pred)\nprint(f\"Pr\u00e9cision sur donn\u00e9es pivot\u00e9es: {accuracy_rotated*100:.2f}%\")\n\nprint(\"\\nComparaison avec la pr\u00e9cision originale:\")\nprint(f\"Pr\u00e9cision sur donn\u00e9es originales: {accuracy*100:.2f}%\")\nprint(f\"Pr\u00e9cision sur donn\u00e9es bruit\u00e9es: {accuracy_noisy*100:.2f}%\")\nprint(f\"Pr\u00e9cision sur donn\u00e9es pivot\u00e9es: {accuracy_rotated*100:.2f}%\")\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-20-markdown-conclusions","title":"Cellule 20 (Markdown) - Conclusions","text":"<pre><code>## Conclusions sur le Machine Learning classique\n\nApr\u00e8s avoir impl\u00e9ment\u00e9 et test\u00e9 notre mod\u00e8le Random Forest pour la classification des chiffres manuscrits, nous pouvons tirer plusieurs conclusions :\n\n### Points forts du Random Forest:\n- Entra\u00eenement relativement rapide\n- Bonnes performances sur les donn\u00e9es originales\n- Interpr\u00e9tabilit\u00e9 (importance des caract\u00e9ristiques)\n\n### Limites:\n- N\u00e9cessite une extraction manuelle de caract\u00e9ristiques (PCA dans notre cas)\n- Sensibilit\u00e9 aux transformations des donn\u00e9es (bruit, rotation)\n- Difficult\u00e9 \u00e0 capturer des motifs complexes sans feature engineering appropri\u00e9\n\n### Questions pour la r\u00e9flexion:\n1. Pourquoi avons-nous besoin de r\u00e9duire la dimensionnalit\u00e9 pour le Random Forest?\n2. Comment pourrait-on am\u00e9liorer la robustesse aux transformations?\n3. Quelles autres caract\u00e9ristiques pourraient \u00eatre extraites manuellement pour am\u00e9liorer les performances?\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-21-markdown-widget-interactif","title":"Cellule 21 (Markdown) - Widget interactif","text":"<pre><code>## Tester le mod\u00e8le vous-m\u00eame\n\nUtilisez le widget ci-dessous pour tester le mod\u00e8le sur diff\u00e9rents exemples. Vous pourrez voir l'image et la pr\u00e9diction correspondante.\n</code></pre>"},{"location":"module1/ressources/machine-learning-classique/#cellule-22-code-widget-interactif","title":"Cellule 22 (Code) - Widget interactif","text":"<pre><code>print(\"\\n--- Testez le mod\u00e8le vous-m\u00eame ---\")\n\ndef test_model(digit_idx):\n    if digit_idx &lt; len(X_test):\n        # Afficher l'image\n        img = X_test[digit_idx].reshape(28, 28)\n        plt.figure(figsize=(6, 6))\n        plt.imshow(img, cmap='gray')\n        plt.title(f\"Chiffre \u00e0 classifier\")\n        plt.axis('off')\n        plt.show()\n\n        # Faire la pr\u00e9diction\n        features = feature_pipeline.transform([X_test[digit_idx]])\n        prediction = rf_model.predict(features)[0]\n        real_label = y_test[digit_idx]\n\n        print(f\"Pr\u00e9diction du mod\u00e8le Random Forest: {prediction}\")\n        print(f\"\u00c9tiquette r\u00e9elle: {real_label}\")\n        print(f\"Pr\u00e9diction {'correcte' if prediction == real_label else 'incorrecte'}\")\n    else:\n        print(\"Index hors limites!\")\n\n# Cr\u00e9er un slider pour s\u00e9lectionner un chiffre \u00e0 tester\ndigit_selector = widgets.IntSlider(\n    value=0,\n    min=0,\n    max=len(X_test)-1,\n    step=1,\n    description='Index:',\n    continuous_update=False\n)\n\n# Bouton pour ex\u00e9cuter le test\ntest_button = widgets.Button(description=\"Tester\")\noutput = widgets.Output()\n\ndef on_button_clicked(b):\n    with output:\n        output.clear_output()\n        test_model(digit_selector.value)\n\ntest_button.on_click(on_button_clicked)\n\n# Afficher les widgets\ndisplay(widgets.HBox([digit_selector, test_button]))\ndisplay(output)\n\nprint(\"Utilisez le slider pour s\u00e9lectionner un index et cliquez sur 'Tester' pour classifier le chiffre correspondant.\")\n</code></pre>"},{"location":"module1/ressources/synthese-visuelle/","title":"Synth\u00e8se visuelle - Module 1","text":""},{"location":"module1/ressources/synthese-visuelle/#carte-heuristique-des-fondamentaux-du-deep-learning","title":"Carte heuristique des fondamentaux du Deep Learning","text":"<p>Cette carte r\u00e9sume les concepts essentiels abord\u00e9s dans le Module 1. Utilisez-la comme r\u00e9f\u00e9rence rapide pour r\u00e9viser ou comprendre les relations entre les diff\u00e9rents concepts.</p> <pre><code>mindmap\n  root((Deep Learning))\n    C'est quoi?\n      R\u00e9seaux de neurones multicouches\n      Apprend automatiquement les caract\u00e9ristiques\n      Id\u00e9al pour images, texte, son\n      Plus puissant que ML classique\n\n    Architecture\n      Neurones artificiels interconnect\u00e9s\n      Couche d'entr\u00e9e\n      Couches cach\u00e9es\n      Couche de sortie\n\n    Types de r\u00e9seaux\n      CNN pour les images\n      RNN/LSTM pour les textes\n      Transformers pour le langage avanc\u00e9\n      GAN pour g\u00e9n\u00e9rer du contenu\n\n    Apprentissage\n      Forward propagation\n      Calcul d'erreur\n      Backpropagation\n      \u00c9poques d'entra\u00eenement\n\n    Vs ML classique\n      ML extraction manuelle\n      DL extraction automatique\n      ML plus simple\n      DL plus performant\n\n    Applications\n      Reconnaissance d'images\n      G\u00e9n\u00e9ration de texte\n      Recommandation de contenu\n      Voitures autonomes\n      Applications m\u00e9dicales\n\n    D\u00e9fis actuels\n      Besoin de donn\u00e9es massives\n      Consommation \u00e9nerg\u00e9tique\n      Bo\u00eete noire (explicabilit\u00e9)\n      Biais dans les mod\u00e8les\n      Co\u00fbts d'entra\u00eenement \u00e9lev\u00e9s\n\n    Conseils pratiques\n      Commencer simple\n      Pr\u00e9parer ses donn\u00e9es\n      Surveiller l'entra\u00eenement\n      Tester diversement</code></pre> <p>\ud83d\udca1 Astuce</p> <p>Pour explorer plus en d\u00e9tail chaque concept, cliquez sur les diff\u00e9rentes branches de la carte mentale interactive ci-dessus.</p>"},{"location":"module1/ressources/synthese-visuelle/#comment-utiliser-cette-carte","title":"Comment utiliser cette carte","text":"<ol> <li>Vue d'ensemble: Commencez par le centre pour comprendre la structure globale</li> <li>Exploration par th\u00e8me: Suivez chaque branche pour approfondir un aspect particulier</li> <li>R\u00e9vision cibl\u00e9e: Utilisez la carte pour identifier les concepts \u00e0 r\u00e9viser davantage</li> </ol>"},{"location":"module2/","title":"\ud83e\udde0 Module 2 : Architectures sp\u00e9cialis\u00e9es de r\u00e9seaux de neurones","text":""},{"location":"module2/#objectifs-du-module","title":"\ud83c\udfaf Objectifs du module","text":"<p>\u00c0 l'issue de ce module, vous serez capable de :</p> <ul> <li>Comprendre et impl\u00e9menter des r\u00e9seaux de neurones convolutifs (CNN) pour la vision par ordinateur</li> <li>Ma\u00eetriser les r\u00e9seaux r\u00e9currents (RNN/LSTM) pour le traitement des s\u00e9quences et du langage</li> <li>Visualiser et interpr\u00e9ter le fonctionnement interne des diff\u00e9rentes architectures</li> <li>Int\u00e9grer ces mod\u00e8les sp\u00e9cialis\u00e9s dans des applications concr\u00e8tes</li> <li>Comparer et choisir l'architecture adapt\u00e9e \u00e0 diff\u00e9rents probl\u00e8mes</li> </ul>"},{"location":"module2/#programme-4h","title":"\ud83d\udcca Programme (4h)","text":"<p>Ce module explore les architectures sp\u00e9cialis\u00e9es de r\u00e9seaux de neurones \u00e0 travers trois phases compl\u00e9mentaires.</p>"},{"location":"module2/#phase-1-mini-projet-cnn-pour-la-vision-par-ordinateur-2h","title":"Phase 1 : Mini-projet CNN pour la vision par ordinateur (2h)","text":"<p>Plongez dans l'univers des r\u00e9seaux convolutifs et apprenez \u00e0 les utiliser pour la classification d'images.</p> <ul> <li>Principes des convolutions et du pooling</li> <li>Impl\u00e9mentation d'un CNN avec TensorFlow/Keras</li> <li>Visualisation des filtres et feature maps</li> <li>Int\u00e9gration dans une application web simple</li> </ul>"},{"location":"module2/#phase-2-mini-projet-rnn-pour-le-traitement-du-langage-2h","title":"Phase 2 : Mini-projet RNN pour le traitement du langage (2h)","text":"<p>D\u00e9couvrez comment les r\u00e9seaux r\u00e9currents permettent de traiter des donn\u00e9es s\u00e9quentielles comme le texte.</p> <ul> <li>Principes fondamentaux des r\u00e9seaux r\u00e9currents</li> <li>Cellules LSTM pour la m\u00e9moire \u00e0 long terme</li> <li>Impl\u00e9mentation d'un mod\u00e8le d'analyse de sentiment</li> <li>Exp\u00e9rimentation avec l'API Mistral AI pour le NLP</li> </ul>"},{"location":"module2/#prerequis","title":"Pr\u00e9requis","text":"<ul> <li>Avoir suivi le Module 1 (Fondamentaux du Deep Learning)</li> <li>Comprendre les bases des r\u00e9seaux de neurones artificiels</li> <li>Conna\u00eetre les concepts de base de Python et TensorFlow/Keras</li> </ul>"},{"location":"module2/#livrables-attendus","title":"Livrables attendus","text":"<p>\u00c0 l'issue de ce module, vous devrez produire :</p> <ol> <li>Un mod\u00e8le CNN fonctionnel pour la classification d'images</li> <li>Un rapport d'analyse des features maps et filtres de convolution</li> <li>Un mod\u00e8le RNN/LSTM pour l'analyse de sentiment textuel</li> <li>Un rapport d'am\u00e9lioration documentant vos exp\u00e9rimentations sur le mod\u00e8le sous-optimal</li> </ol>"},{"location":"module2/#competences-bts-sio-developpees","title":"Comp\u00e9tences BTS SIO d\u00e9velopp\u00e9es","text":"Comp\u00e9tence Description Activit\u00e9s associ\u00e9es B1.3 Gestion des donn\u00e9es Manipulation des datasets d'images et de texte B2.2 Conception de solutions applicatives Conception d'architectures CNN et RNN adapt\u00e9es B2.3 D\u00e9veloppement Impl\u00e9mentation et optimisation des mod\u00e8les B3.2 V\u00e9rification et validation Analyse des performances des mod\u00e8les"},{"location":"module2/#pret-a-explorer-les-architectures-specialisees","title":"Pr\u00eat \u00e0 explorer les architectures sp\u00e9cialis\u00e9es ?","text":"<p>Commencez par d\u00e9couvrir les r\u00e9seaux convolutifs (CNN) et leur application \u00e0 la vision par ordinateur.</p> <p>Commencer la Phase 1: CNN \u00c9valuer vos connaissances</p>"},{"location":"module2/qcm-evaluation-module2/","title":"\ud83d\udcdd QCM d'auto-\u00e9valuation - Module 2 : Architectures sp\u00e9cialis\u00e9es","text":"<p>Ce QCM vous permettra d'\u00e9valuer votre compr\u00e9hension des r\u00e9seaux convolutifs (CNN) et r\u00e9currents (RNN) \u00e9tudi\u00e9s dans ce module.</p>"},{"location":"module2/qcm-evaluation-module2/#instructions","title":"Instructions","text":"<ul> <li>Cochez la ou les r\u00e9ponses correctes pour chaque question</li> <li>Certaines questions peuvent avoir plusieurs r\u00e9ponses correctes</li> <li>\u00c0 la fin du questionnaire, calculez votre score gr\u00e2ce au corrig\u00e9 fourni</li> <li>Dur\u00e9e recommand\u00e9e : 15 minutes</li> </ul>"},{"location":"module2/qcm-evaluation-module2/#partie-a-reseaux-convolutifs-cnn","title":"Partie A : R\u00e9seaux Convolutifs (CNN)","text":""},{"location":"module2/qcm-evaluation-module2/#1-dans-un-reseau-convolutif-a-quoi-sert-principalement-loperation-de-convolution","title":"1. Dans un r\u00e9seau convolutif, \u00e0 quoi sert principalement l'op\u00e9ration de convolution ?","text":"<ul> <li> \u00c0 r\u00e9duire la dimension des donn\u00e9es</li> <li> \u00c0 d\u00e9tecter des caract\u00e9ristiques locales dans les donn\u00e9es d'entr\u00e9e</li> <li> \u00c0 connecter tous les neurones entre eux</li> <li> \u00c0 acc\u00e9l\u00e9rer le temps d'entra\u00eenement</li> </ul>"},{"location":"module2/qcm-evaluation-module2/#2-quest-ce-quun-filtre-ou-noyau-dans-un-cnn","title":"2. Qu'est-ce qu'un filtre (ou noyau) dans un CNN ?","text":"<ul> <li> Une fonction qui supprime les pixels ind\u00e9sirables de l'image</li> <li> Une matrice de poids qui s'applique localement sur les donn\u00e9es d'entr\u00e9e</li> <li> Un seuil qui \u00e9limine les valeurs en dessous d'un certain niveau</li> <li> Une technique pour s\u00e9lectionner les meilleures images d'entra\u00eenement</li> </ul>"},{"location":"module2/qcm-evaluation-module2/#3-quel-est-le-role-principal-de-loperation-de-pooling-dans-un-cnn","title":"3. Quel est le r\u00f4le principal de l'op\u00e9ration de pooling dans un CNN ?","text":"<ul> <li> Augmenter la taille des feature maps</li> <li> R\u00e9duire la dimensionnalit\u00e9 tout en pr\u00e9servant les informations importantes</li> <li> Ajouter de la non-lin\u00e9arit\u00e9 au r\u00e9seau</li> <li> Connecter les diff\u00e9rentes couches de convolution</li> </ul>"},{"location":"module2/qcm-evaluation-module2/#4-quels-sont-les-avantages-des-cnn-pour-le-traitement-dimages-plusieurs-reponses-possibles","title":"4. Quels sont les avantages des CNN pour le traitement d'images ? (plusieurs r\u00e9ponses possibles)","text":"<ul> <li> Partage des param\u00e8tres entre diff\u00e9rentes positions spatiales</li> <li> Invariance \u00e0 la translation</li> <li> R\u00e9duction significative du nombre de param\u00e8tres par rapport aux r\u00e9seaux enti\u00e8rement connect\u00e9s</li> <li> Capacit\u00e9 \u00e0 traiter des images de n'importe quelle taille sans redimensionnement</li> </ul>"},{"location":"module2/qcm-evaluation-module2/#5-dans-quelle-couche-dun-cnn-typique-se-trouvent-generalement-le-plus-grand-nombre-de-parametres","title":"5. Dans quelle couche d'un CNN typique se trouvent g\u00e9n\u00e9ralement le plus grand nombre de param\u00e8tres ?","text":"<ul> <li> Couches de convolution</li> <li> Couches de pooling</li> <li> Couches enti\u00e8rement connect\u00e9es (fully connected)</li> <li> Couches de normalisation par lots (batch normalization)</li> </ul>"},{"location":"module2/qcm-evaluation-module2/#6-quest-ce-quune-feature-map-dans-un-cnn","title":"6. Qu'est-ce qu'une feature map dans un CNN ?","text":"<ul> <li> Une carte qui indique les r\u00e9gions d'int\u00e9r\u00eat dans l'image originale</li> <li> Le r\u00e9sultat de l'application d'un filtre de convolution sur une entr\u00e9e</li> <li> Un graphique montrant la progression de l'entra\u00eenement</li> <li> La liste des caract\u00e9ristiques extraites manuellement avant l'entra\u00eenement</li> </ul>"},{"location":"module2/qcm-evaluation-module2/#7-comment-evoluent-les-caracteristiques-detectees-a-mesure-quon-avance-dans-les-couches-dun-cnn","title":"7. Comment \u00e9voluent les caract\u00e9ristiques d\u00e9tect\u00e9es \u00e0 mesure qu'on avance dans les couches d'un CNN ?","text":"<ul> <li> Elles deviennent de plus en plus simples et \u00e9l\u00e9mentaires</li> <li> Elles restent de m\u00eame nature mais deviennent plus pr\u00e9cises</li> <li> Elles deviennent de plus en plus abstraites et complexes</li> <li> Elles concernent des r\u00e9gions de plus en plus petites de l'image</li> </ul>"},{"location":"module2/qcm-evaluation-module2/#partie-b-reseaux-recurrents-rnn","title":"Partie B : R\u00e9seaux R\u00e9currents (RNN)","text":""},{"location":"module2/qcm-evaluation-module2/#8-quelle-est-la-principale-caracteristique-des-reseaux-de-neurones-recurrents-rnn","title":"8. Quelle est la principale caract\u00e9ristique des r\u00e9seaux de neurones r\u00e9currents (RNN) ?","text":"<ul> <li> Ils utilisent des op\u00e9rations de convolution pour traiter les donn\u00e9es</li> <li> Ils contiennent des connexions formant des boucles permettant de m\u00e9moriser les informations</li> <li> Ils traitent chaque \u00e9l\u00e9ment d'une s\u00e9quence de mani\u00e8re compl\u00e8tement ind\u00e9pendante</li> <li> Ils sont sp\u00e9cialis\u00e9s dans le traitement d'images</li> </ul>"},{"location":"module2/qcm-evaluation-module2/#9-pour-quels-types-de-donnees-les-rnn-sont-ils-particulierement-adaptes","title":"9. Pour quels types de donn\u00e9es les RNN sont-ils particuli\u00e8rement adapt\u00e9s ?","text":"<ul> <li> Images 2D</li> <li> Donn\u00e9es tabulaires (comme des tableaux Excel)</li> <li> Donn\u00e9es s\u00e9quentielles (texte, s\u00e9ries temporelles, audio)</li> <li> Nuages de points 3D</li> </ul>"},{"location":"module2/qcm-evaluation-module2/#10-quel-probleme-majeur-affecte-les-rnn-classiques-lors-du-traitement-de-sequences-longues","title":"10. Quel probl\u00e8me majeur affecte les RNN classiques lors du traitement de s\u00e9quences longues ?","text":"<ul> <li> Surconsommation de m\u00e9moire</li> <li> Temps de traitement exponentiel</li> <li> Probl\u00e8me de disparition ou d'explosion du gradient</li> <li> Incapacit\u00e9 \u00e0 parall\u00e9liser les calculs</li> </ul>"},{"location":"module2/qcm-evaluation-module2/#11-quelle-est-la-principale-innovation-des-cellules-lstm-par-rapport-aux-rnn-classiques","title":"11. Quelle est la principale innovation des cellules LSTM par rapport aux RNN classiques ?","text":"<ul> <li> Elles utilisent des op\u00e9rations de convolution</li> <li> Elles poss\u00e8dent des m\u00e9canismes de portes contr\u00f4lant le flux d'information</li> <li> Elles peuvent traiter plusieurs s\u00e9quences en parall\u00e8le</li> <li> Elles ne n\u00e9cessitent pas d'entra\u00eenement</li> </ul>"},{"location":"module2/qcm-evaluation-module2/#12-dans-un-reseau-lstm-a-quoi-sert-la-porte-doubli-forget-gate","title":"12. Dans un r\u00e9seau LSTM, \u00e0 quoi sert la \"porte d'oubli\" (forget gate) ?","text":"<ul> <li> \u00c0 d\u00e9terminer quelles informations de l'\u00e9tat pr\u00e9c\u00e9dent doivent \u00eatre conserv\u00e9es ou supprim\u00e9es</li> <li> \u00c0 r\u00e9initialiser compl\u00e8tement le r\u00e9seau quand la s\u00e9quence est trop longue</li> <li> \u00c0 sauter certaines \u00e9tapes de calcul pour acc\u00e9l\u00e9rer le traitement</li> <li> \u00c0 ignorer les donn\u00e9es d'entr\u00e9e corrompues ou bruit\u00e9es</li> </ul>"},{"location":"module2/qcm-evaluation-module2/#13-quelles-applications-typiques-utilisent-les-rnnlstm-plusieurs-reponses-possibles","title":"13. Quelles applications typiques utilisent les RNN/LSTM ? (plusieurs r\u00e9ponses possibles)","text":"<ul> <li> Reconnaissance de caract\u00e8res manuscrits</li> <li> Traduction automatique</li> <li> Pr\u00e9diction de s\u00e9ries temporelles</li> <li> G\u00e9n\u00e9ration de texte</li> <li> Segmentation d'images</li> </ul>"},{"location":"module2/qcm-evaluation-module2/#14-quest-ce-qui-differencie-principalement-les-gru-gated-recurrent-units-des-lstm","title":"14. Qu'est-ce qui diff\u00e9rencie principalement les GRU (Gated Recurrent Units) des LSTM ?","text":"<ul> <li> Les GRU n'ont aucune forme de m\u00e9moire</li> <li> Les GRU ont une architecture plus simple avec moins de portes</li> <li> Les GRU sont sp\u00e9cifiquement con\u00e7us pour les donn\u00e9es non s\u00e9quentielles</li> <li> Les GRU ne peuvent pas \u00eatre entra\u00een\u00e9s par r\u00e9tropropagation</li> </ul>"},{"location":"module2/qcm-evaluation-module2/#auto-evaluation","title":"Auto-\u00e9valuation","text":"<p>Une fois le QCM compl\u00e9t\u00e9, v\u00e9rifiez vos r\u00e9ponses avec le corrig\u00e9 ci-dessous.</p>"},{"location":"module2/qcm-evaluation-module2/#corrige","title":"Corrig\u00e9","text":"<ol> <li>b</li> <li>b</li> <li>b</li> <li>a, b, c</li> <li>c</li> <li>b</li> <li>c</li> <li>b</li> <li>c</li> <li>c</li> <li>b</li> <li>a</li> <li>b, c, d</li> <li>b</li> </ol>"},{"location":"module2/qcm-evaluation-module2/#calcul-de-votre-score","title":"Calcul de votre score","text":"<p>Comptez 1 point par r\u00e9ponse correcte. Pour les questions \u00e0 choix multiples, comptez 1 point uniquement si toutes vos s\u00e9lections sont correctes.</p> <p>Total des points possibles : 14</p>"},{"location":"module2/qcm-evaluation-module2/#interpretation","title":"Interpr\u00e9tation","text":"<ul> <li>11-14 points : Excellente ma\u00eetrise des architectures sp\u00e9cialis\u00e9es de Deep Learning</li> <li>8-10 points : Bonne compr\u00e9hension, quelques points \u00e0 clarifier</li> <li>6-7 points : Compr\u00e9hension de base, r\u00e9vision n\u00e9cessaire de certains concepts</li> <li>0-5 points : R\u00e9vision approfondie recommand\u00e9e avant de poursuivre</li> </ul>"},{"location":"module2/qcm-evaluation-module2/#pour-approfondir","title":"Pour approfondir","text":"<p>Si vous avez obtenu moins de 11 points, nous vous recommandons de revoir les concepts sur lesquels vous avez fait des erreurs. Consultez les ressources suivantes : - Les notebooks CNN et RNN du module - Les visualisations des architectures CNN et RNN - La documentation des mini-projets pratiques r\u00e9alis\u00e9s</p>"},{"location":"module2/reseaux-convolutifs/","title":"\ud83d\udd0d Phase 1 : Mini-projet CNN pour la vision par ordinateur","text":""},{"location":"module2/reseaux-convolutifs/#objectifs-de-la-phase","title":"Objectifs de la phase","text":"<p>Dans cette phase, vous allez :</p> <ul> <li>Comprendre les principes fondamentaux des r\u00e9seaux de neurones convolutifs (CNN)</li> <li>Impl\u00e9menter un CNN pour la classification d'images avec TensorFlow/Keras</li> <li>Visualiser et interpr\u00e9ter les filtres et feature maps d'un CNN</li> <li>Int\u00e9grer un mod\u00e8le CNN dans une application web simple</li> </ul>"},{"location":"module2/reseaux-convolutifs/#partie-1-principes-des-cnn-30-min","title":"\ud83e\udde9 Partie 1: Principes des CNN (30 min)","text":""},{"location":"module2/reseaux-convolutifs/#defi-de-reflexion-initiale","title":"\ud83e\udde0D\u00e9fi de r\u00e9flexion initiale","text":"<p>Avant de plonger dans les CNN, prenez 2 minutes pour r\u00e9fl\u00e9chir \u00e0 cette question :</p> <p>\u2753Question \u00e0 m\u00e9diter : Comment reconnaissez-vous un visage dans une photo, quelle que soit sa position ou l'\u00e9clairage ? Qu'est-ce qui rend cette t\u00e2che si facile pour vous et si difficile pour un ordinateur ?</p>"},{"location":"module2/reseaux-convolutifs/#activite-guidee-decouverte-de-larchitecture-cnn","title":"Activit\u00e9 guid\u00e9e : D\u00e9couverte de l'architecture CNN","text":"<p>\u00c9tape 1 : Observation (3 min) Examinez ces deux visualisations en parall\u00e8le :</p> <ul> <li>L'image originale d'un chiffre '7' manuscrit et son traitement par les diff\u00e9rentes couches d'un CNN</li> </ul> <p></p> <ul> <li>Les diff\u00e9rentes caract\u00e9ristiques extraites \u00e0 chaque niveau d'un CNN d\u00e9j\u00e0 entra\u00een\u00e9</li> </ul> <p></p> <p>\u00c9tape 2 : Mini-investigation (5 min) Formez des bin\u00f4mes et discutez :</p> <ul> <li>Quels types de d\u00e9tails la premi\u00e8re couche semble-t-elle rep\u00e9rer dans l'image?</li> <li>Comment ce que \"voit\" le r\u00e9seau change-t-il entre la premi\u00e8re et la derni\u00e8re couche?</li> <li>Pourquoi est-il utile pour le r\u00e9seau de transformer l'image \u00e0 chaque \u00e9tape?</li> </ul> <p>Les r\u00e9seaux de neurones convolutifs (CNN) offrent plusieurs avantages, notamment :</p> <ul> <li>Extraction automatique des caract\u00e9ristiques</li> </ul> <p>Contrairement aux m\u00e9thodes traditionnelles de vision par ordinateur qui n\u00e9cessitent une extraction manuelle des caract\u00e9ristiques, les CNN apprennent automatiquement les motifs pertinents (bords, textures, formes) \u00e0 partir des donn\u00e9es.</p> <ul> <li>Partage des poids et r\u00e9duction du nombre de param\u00e8tres </li> </ul> <p>Gr\u00e2ce aux filtres de convolution partag\u00e9s sur toute l'image, les CNN r\u00e9duisent consid\u00e9rablement le nombre de param\u00e8tres \u00e0 entra\u00eener, ce qui diminue les besoins en m\u00e9moire et en calcul par rapport aux r\u00e9seaux de neurones enti\u00e8rement connect\u00e9s.</p> <ul> <li>Invariance aux translations et robustesse aux variations</li> </ul> <p>Les couches de convolution et de pooling permettent aux CNN d'\u00eatre robustes aux d\u00e9calages, rotations et d\u00e9formations dans les images, ce qui am\u00e9liore leur capacit\u00e9 \u00e0 reconna\u00eetre des objets dans diff\u00e9rentes conditions.</p> <p>\u00c9tape 3 : Construction du mod\u00e8le mental (5 min) Sur votre feuille de travail, compl\u00e9tez le sch\u00e9ma simplifi\u00e9 d'un CNN :</p> <p></p> <ol> <li>Identifiez et nommez les trois types principaux de couches</li> <li>Pour chaque type, pr\u00e9cisez bri\u00e8vement sa fonction</li> <li>Listez les trois avantages majeurs des CNN</li> </ol> <p>\u00c9tape 4 : Analogie concr\u00e8te (3 min) Pour comprendre le fonctionnement d'un CNN, voyons comment il pourrait identifier un personnage c\u00e9l\u00e8bre comme Dark Vador :</p> <p></p> <ul> <li>La couche de convolution rep\u00e8re les caract\u00e9ristiques distinctives : \"Je d\u00e9tecte un casque noir, un respirateur, une cape...\"</li> <li>La couche de pooling ignore les d\u00e9tails non pertinents : \"Peu importe l'angle de vue, l'\u00e9clairage, s'il est de face ou de profil...\"</li> <li>La couche fully connected prend la d\u00e9cision finale : \"D'apr\u00e8s toutes ces caract\u00e9ristiques combin\u00e9es, c'est Dark Vador \u00e0 99.8%!\"</li> </ul> <p>Cette analogie montre comment un CNN analyse une image de mani\u00e8re hi\u00e9rarchique, comme notre cerveau le fait naturellement.</p>"},{"location":"module2/reseaux-convolutifs/#points-importants-a-retenir","title":"Points importants \u00e0 retenir","text":"<p>\u00c0 savoir avant de passer \u00e0 la pratique :</p> <ol> <li> <p>Les CNN sont con\u00e7us sp\u00e9cifiquement pour traiter les donn\u00e9es en grille comme les images.</p> </li> <li> <p>Les filtres de convolution agissent comme des d\u00e9tecteurs de motifs qui s'appliquent \u00e0 toute l'image.</p> </li> <li> <p>Le pooling permet de r\u00e9duire les dimensions tout en conservant l'information importante.</p> </li> <li> <p>Les poids du r\u00e9seau sont ajust\u00e9s automatiquement pendant l'entra\u00eenement.</p> </li> <li> <p>Un CNN profond permet de d\u00e9tecter des motifs de plus en plus complexes et abstraits.</p> </li> <li> <p>Le grand avantage des CNN est qu'ils apprennent automatiquement les caract\u00e9ristiques pertinentes, sans qu'on ait \u00e0 les programmer manuellement.</p> </li> </ol>"},{"location":"module2/reseaux-convolutifs/#transition-vers-limplementation","title":"Transition vers l'impl\u00e9mentation","text":"<p>Maintenant que vous avez conceptualis\u00e9 l'architecture d'un CNN, passons \u00e0 l'impl\u00e9mentation pratique pour voir ces concepts en action. Gardez votre sch\u00e9ma \u00e0 port\u00e9e de main - vous pourrez le compl\u00e9ter avec des observations pratiques.</p>"},{"location":"module2/reseaux-convolutifs/#partie-2-implementation-dun-cnn-pour-mnist-50-min","title":"Partie 2: Impl\u00e9mentation d'un CNN pour MNIST (50 min)","text":""},{"location":"module2/reseaux-convolutifs/#instructions","title":"Instructions","text":"<ol> <li>Ouvrez le notebook Jupyter cnn-classification dans Google Colab</li> <li>Suivez les instructions \u00e9tape par \u00e9tape pour impl\u00e9menter un CNN pour la classification des chiffres manuscrits (MNIST)</li> <li>Ex\u00e9cutez chaque cellule et observez les r\u00e9sultats</li> <li> <p>Portez une attention particuli\u00e8re aux sections suivantes :</p> <ul> <li>Architecture du mod\u00e8le CNN</li> <li>Processus d'entra\u00eenement</li> <li>Visualisation des filtres et feature maps</li> <li>Analyse des performances et des erreurs</li> </ul> </li> </ol>"},{"location":"module2/reseaux-convolutifs/#points-cles-a-explorer","title":"Points cl\u00e9s \u00e0 explorer","text":"<ul> <li>Comment les couches de convolution extraient-elles des caract\u00e9ristiques de plus en plus abstraites ?</li> <li>Quel est l'impact du nombre de filtres et de couches sur les performances ?</li> <li>Comment les feature maps r\u00e9v\u00e8lent-elles ce que \"voit\" le r\u00e9seau ?</li> <li>Quelles sont les limites du mod\u00e8le face \u00e0 des donn\u00e9es bruit\u00e9es ou d\u00e9form\u00e9es ?</li> </ul>"},{"location":"module2/reseaux-convolutifs/#partie-3-integration-dans-une-application-web-via-google-colab-40-min","title":"Partie 3: Int\u00e9gration dans une application web via Google Colab (40 min)","text":""},{"location":"module2/reseaux-convolutifs/#objectif-du-mini-projet","title":"\ud83c\udfaf Objectif du mini-projet","text":"<p>Dans cette partie pratique, vous allez cr\u00e9er une application web interactive qui int\u00e8gre votre mod\u00e8le CNN pour la reconnaissance de chiffres manuscrits. Ce mini-projet reprend les concepts th\u00e9oriques vus pr\u00e9c\u00e9demment et les applique dans un contexte professionnel concret.</p>"},{"location":"module2/reseaux-convolutifs/#mise-en-contexte-professionnelle","title":"\ud83d\ude80 Mise en contexte professionnelle","text":"<p>En tant que stagiaire dans une PME, vous d\u00e9veloppez un prototype d'application qui permettra d'automatiser la saisie de codes \u00e0 partir de documents papier, \u00e9conomisant du temps aux employ\u00e9s et r\u00e9duisant les erreurs de transcription.</p>"},{"location":"module2/reseaux-convolutifs/#structure-du-projet","title":"\ud83d\udccb Structure du projet","text":"<p>Le mini-projet, d\u00e9taill\u00e9 dans le document de r\u00e9f\u00e9rence, comprend:</p> <ul> <li>\u2699\ufe0f Configuration de l'environnement dans Google Colab</li> <li>\ud83e\udde0 Entra\u00eenement d'un mod\u00e8le CNN sur le dataset MNIST </li> <li>\ud83c\udf10 Cr\u00e9ation d'une interface web interactive avec Flask et ngrok</li> <li>\ud83e\uddea Tests et \u00e9valuation de votre application</li> </ul>"},{"location":"module2/reseaux-convolutifs/#elements-a-observer-et-documenter","title":"\ud83d\udd0d \u00c9l\u00e9ments \u00e0 observer et documenter","text":"<p>Pendant vos tests, portez une attention particuli\u00e8re \u00e0:</p> <ul> <li>\ud83d\udcca Le taux de r\u00e9ussite sur diff\u00e9rents types d'entr\u00e9es (dessin vs image import\u00e9e)</li> <li>\ud83d\udd0d La visualisation des feature maps et ce qu'elles r\u00e9v\u00e8lent du fonctionnement du mod\u00e8le</li> <li>\ud83d\udca1 Les forces et limitations observ\u00e9es dans des conditions r\u00e9elles d'utilisation</li> </ul>"},{"location":"module2/reseaux-convolutifs/#livrable-attendu","title":"\ud83d\udcdd Livrable attendu","text":"<p>Vous compl\u00e9terez la fiche d'observations sur les CNN en documentant les r\u00e9sultats de votre exp\u00e9rimentation. Cette fiche servira de base pour \u00e9valuer votre compr\u00e9hension des r\u00e9seaux convolutifs et leur application pratique.</p> <p>Pour les instructions d\u00e9taill\u00e9es \u00e9tape par \u00e9tape, consultez le document complet mini-projet-cnn-web-colab.md.</p>"},{"location":"module2/reseaux-convolutifs/#ressources-complementaires","title":"Ressources compl\u00e9mentaires","text":"<ul> <li>Tutoriel TensorFlow sur les CNN - Guide officiel de TensorFlow sur l'impl\u00e9mentation des r\u00e9seaux de neurones convolutifs</li> <li>Visualisation de CNN (Distill.pub) - Article interactif sur la visualisation et l'interpr\u00e9tation des r\u00e9seaux convolutifs</li> <li>Documentation Flask - Documentation officielle du framework Flask pour le d\u00e9veloppement web</li> </ul> <p>Retour au Module 2 Continuer vers les R\u00e9seaux r\u00e9currents</p>"},{"location":"module2/reseaux-recurrents/","title":"\ud83d\udd0d Phase 2 : Mini-projet RNN pour le traitement du langage","text":""},{"location":"module2/reseaux-recurrents/#objectifs-de-la-phase","title":"\ud83c\udfafObjectifs de la phase","text":"<p>Dans cette phase, vous allez :</p> <ul> <li>Comprendre les principes des r\u00e9seaux r\u00e9currents (RNN) et de leurs variantes (LSTM, GRU)</li> <li>Impl\u00e9menter un mod\u00e8le LSTM pour l'analyse de sentiment</li> <li>Visualiser et interpr\u00e9ter le fonctionnement interne d'un RNN</li> </ul>"},{"location":"module2/reseaux-recurrents/#partie-1-principes-des-rnn-20-min","title":"\ud83e\udde9 Partie 1: Principes des RNN (20 min)","text":""},{"location":"module2/reseaux-recurrents/#architecture-et-fonctionnement-des-rnn","title":"\ud83d\udcca Architecture et fonctionnement des RNN","text":"<p>Les r\u00e9seaux de neurones r\u00e9currents (RNN) sont con\u00e7us sp\u00e9cifiquement pour traiter des donn\u00e9es s\u00e9quentielles comme le texte, les s\u00e9ries temporelles ou les donn\u00e9es audio. Contrairement aux r\u00e9seaux classiques qui traitent chaque entr\u00e9e ind\u00e9pendamment, les RNN maintiennent un \"\u00e9tat interne\" qui leur permet de se souvenir des informations pr\u00e9c\u00e9dentes.</p>"},{"location":"module2/reseaux-recurrents/#problematique-pourquoi-les-rnn","title":"Probl\u00e9matique : Pourquoi les RNN ?","text":"<p>Imaginons que vous surveillez des logs de s\u00e9curit\u00e9 : - Un r\u00e9seau classique ne verrait que des entr\u00e9es isol\u00e9es, sans comprendre leur s\u00e9quence - Un RNN, lui, se souvient des \u00e9v\u00e9nements pr\u00e9c\u00e9dents pour d\u00e9tecter des patterns suspects</p>"},{"location":"module2/reseaux-recurrents/#le-rnn-explique-avec-lanalogie-du-carnet-de-notes","title":"Le RNN expliqu\u00e9 avec l'analogie du carnet de notes","text":"<p>Analogie du carnet de notes : 1. Vous analysez un rapport d'incident et prenez des notes importantes 2. \u00c0 chaque nouvelle section du rapport, vous :    - Lisez le nouveau contenu (nouvelle entr\u00e9e)    - Consultez vos notes pr\u00e9c\u00e9dentes (\u00e9tat cach\u00e9 / m\u00e9moire)    - Mettez \u00e0 jour vos notes avec les informations les plus pertinentes    - Utilisez la combinaison de la nouvelle section et de vos notes pour comprendre l'incident</p> <p>Dans un RNN : 1. Le r\u00e9seau traite les donn\u00e9es s\u00e9quentiellement (mot par mot, \u00e9v\u00e9nement par \u00e9v\u00e9nement) 2. \u00c0 chaque \u00e9tape, il combine :    - L'entr\u00e9e actuelle (ex : le mot actuel)    - Son \"\u00e9tat de m\u00e9moire\" (ce qu'il a retenu des mots pr\u00e9c\u00e9dents) 3. Il produit :    - Une sortie pour l'\u00e9tape actuelle (ex: pr\u00e9diction partielle)    - Un nouvel \u00e9tat de m\u00e9moire pour l'\u00e9tape suivante</p> <p>Avantages pour un d\u00e9veloppeur d'applications : - Traitement de s\u00e9quences de longueur variable - Capacit\u00e9 \u00e0 \"m\u00e9moriser\" des informations importantes - Applications diverses : analyse de texte, traduction, g\u00e9n\u00e9ration de contenu</p>"},{"location":"module2/reseaux-recurrents/#les-lstm-long-short-term-memory-en-langage-simple","title":"Les LSTM (Long Short-Term Memory) en langage simple","text":""},{"location":"module2/reseaux-recurrents/#solution-au-probleme-de-memoire","title":"Solution au probl\u00e8me de m\u00e9moire","text":"<p>Les RNN classiques ont du mal \u00e0 retenir les informations sur de longues s\u00e9quences - c'est le probl\u00e8me du \"gradient qui s'\u00e9vanouit\". Les cellules LSTM ont \u00e9t\u00e9 con\u00e7ues pour r\u00e9soudre ce probl\u00e8me.</p> <p>Analogie du rapport de s\u00e9curit\u00e9 avec syst\u00e8me de marquage : - Vous avez maintenant un syst\u00e8me pour marquer les informations importantes dans votre rapport - Vous pouvez d\u00e9cider explicitement quelles informations :   * M\u00e9ritent d'\u00eatre conserv\u00e9es pour l'analyse finale   * Doivent \u00eatre mises \u00e0 jour avec de nouvelles donn\u00e9es   * Sont pertinentes pour l'incident en cours</p>"},{"location":"module2/reseaux-recurrents/#les-portes-gates-expliquees-simplement","title":"Les portes (gates) expliqu\u00e9es simplement","text":"<p>Au lieu d'une explication math\u00e9matique complexe, voici le fonctionnement en langage simple :</p> <ol> <li> <p>Porte d'oubli (Forget gate) : </p> <ul> <li>Comme un tri dans votre rapport : \"Quelles informations pass\u00e9es ne sont plus utiles ?\"</li> <li>Exemple SIO : Si un nouvel utilisateur se connecte, vous pouvez \"oublier\" certains d\u00e9tails des sessions pr\u00e9c\u00e9dentes</li> </ul> </li> <li> <p>Porte d'entr\u00e9e (Input gate) :</p> <ul> <li>Filtre les nouvelles informations : \"Quelles nouvelles informations sont importantes ?\"</li> <li>Exemple SIO : Dans un log \"Tentative d'acc\u00e8s admin \u00e9chou\u00e9e 5 fois\", le nombre de tentatives est plus important que l'heure exacte</li> </ul> </li> <li> <p>Porte de sortie (Output gate) :</p> <ul> <li>D\u00e9cide quelles informations partager : \"Quelles parties de ma m\u00e9moire sont pertinentes maintenant ?\"</li> <li>Exemple SIO : Si vous analysez une faille de s\u00e9curit\u00e9, vous vous concentrez sur les logs d'authentification, pas sur les mises \u00e0 jour syst\u00e8me</li> </ul> </li> </ol>"},{"location":"module2/reseaux-recurrents/#applications-pour-les-etudiants-bts-sio","title":"Applications pour les \u00e9tudiants BTS SIO","text":"<p>Voici des applications concr\u00e8tes des RNN/LSTM dans votre domaine :</p> <ol> <li> <p>D\u00e9tection d'intrusion r\u00e9seau :</p> <ul> <li>Les RNN/LSTM analysent les s\u00e9quences de logs pour d\u00e9tecter des comportements anormaux</li> <li>L'ordre chronologique des \u00e9v\u00e9nements est crucial (d'o\u00f9 l'int\u00e9r\u00eat des RNN)</li> </ul> </li> <li> <p>Pr\u00e9diction de pannes syst\u00e8mes :</p> <ul> <li>Les LSTM peuvent analyser les historiques de performance serveur</li> <li>Ils d\u00e9tectent les signes pr\u00e9curseurs de probl\u00e8mes potentiels</li> </ul> </li> <li> <p>Chatbots d'assistance technique :</p> <ul> <li>Les RNN/LSTM permettent de comprendre le contexte d'une conversation de support</li> <li>Ils maintiennent la coh\u00e9rence dans les r\u00e9ponses du chatbot d'aide</li> </ul> </li> <li> <p>Analyse de logs de s\u00e9curit\u00e9 :</p> <ul> <li>Les LSTM peuvent identifier des patterns d'attaque complexes s'\u00e9tendant sur de longues p\u00e9riodes</li> <li>Ils peuvent corr\u00e9ler des \u00e9v\u00e9nements apparemment sans lien</li> </ul> </li> </ol>"},{"location":"module2/reseaux-recurrents/#partie-2-implementation-dun-lstm-pour-lanalyse-de-sentiment-40-min","title":"Partie 2: Impl\u00e9mentation d'un LSTM pour l'analyse de sentiment (40 min)","text":""},{"location":"module2/reseaux-recurrents/#instructions","title":"Instructions","text":"<p>Pour cette partie pratique, vous allez explorer l'analyse de sentiment avec un mod\u00e8le LSTM. Cette activit\u00e9 vous permettra de comprendre comment les r\u00e9seaux r\u00e9currents traitent et \"comprennent\" le texte.</p> <ol> <li>Ouvrez le notebook Jupyter rnn-sequence.ipynb dans Google Colab</li> <li>Suivez les instructions \u00e9tape par \u00e9tape pour impl\u00e9menter un mod\u00e8le LSTM pour l'analyse de sentiment</li> <li>Ex\u00e9cutez chaque cellule et observez les r\u00e9sultats</li> <li> <p>Portez une attention particuli\u00e8re aux sections suivantes :</p> <ul> <li>Pr\u00e9traitement du texte (tokenisation)</li> <li>Architecture du mod\u00e8le LSTM</li> <li>Visualisation des embeddings de mots</li> <li>Analyse des performances et des erreurs</li> </ul> </li> </ol>"},{"location":"module2/reseaux-recurrents/#points-cles-a-explorer","title":"Points cl\u00e9s \u00e0 explorer","text":"<p>Pendant que vous travaillez sur ce notebook, r\u00e9fl\u00e9chissez aux questions suivantes qui feront l'objet d'une discussion en classe et d'une documentation \u00e0 produire :</p> <ul> <li> <p>Comment le texte est-il transform\u00e9 en entr\u00e9es num\u00e9riques pour le r\u00e9seau ?   Observez le processus de tokenisation, la cr\u00e9ation du vocabulaire et la conversion en s\u00e9quences d'indices.</p> </li> <li> <p>Comment les cellules LSTM g\u00e8rent-elles l'information \u00e0 long terme ?   Analysez l'architecture des cellules LSTM et leur capacit\u00e9 \u00e0 m\u00e9moriser les informations pertinentes.</p> </li> <li> <p>Quelle est la diff\u00e9rence entre les embeddings de mots positifs et n\u00e9gatifs ?   Examinez la visualisation des embeddings et comment les mots de sentiments similaires se regroupent.</p> </li> <li> <p>Comment le mod\u00e8le LSTM peut-il comprendre le contexte d'une phrase ?   R\u00e9fl\u00e9chissez \u00e0 la mani\u00e8re dont l'ordre des mots et leurs relations sont captur\u00e9s par le mod\u00e8le.</p> </li> <li> <p>Quelles sont les limitations de cette approche pour l'analyse de sentiment ?   Identifiez les cas o\u00f9 le mod\u00e8le \u00e9choue et pourquoi (ironie, sarcasme, expressions idiomatiques).</p> </li> <li> <p>Comment pourriez-vous am\u00e9liorer ce mod\u00e8le pour des t\u00e2ches plus complexes ?   Proposez des modifications architecturales ou des techniques d'am\u00e9lioration des donn\u00e9es.</p> </li> </ul>"},{"location":"module2/reseaux-recurrents/#livrable-attendu","title":"Livrable attendu","text":"<p>\u00c0 la fin de cette activit\u00e9, vous devrez produire une documentation synth\u00e9tique (1-2 pages) r\u00e9pondant aux questions ci-dessus. Ce document servira de r\u00e9f\u00e9rence pour votre compr\u00e9hension des RNN/LSTM et pourra \u00eatre int\u00e9gr\u00e9 dans la base de connaissances de votre chatbot p\u00e9dagogique.</p> <p>Un document de r\u00e9f\u00e9rence complet sur ces concepts est disponible ici pour vous aider \u00e0 approfondir votre compr\u00e9hension.</p>"},{"location":"module2/reseaux-recurrents/#conclusion-et-transition","title":"Conclusion et transition","text":"<p>Cette section sur les r\u00e9seaux r\u00e9currents vous a permis de comprendre une autre architecture fondamentale du Deep Learning, particuli\u00e8rement adapt\u00e9e aux donn\u00e9es s\u00e9quentielles comme le texte ou les s\u00e9ries temporelles. </p> <p>Vous avez appris \u00e0:</p> <ul> <li>Reconna\u00eetre les situations o\u00f9 les RNN/LSTM sont particuli\u00e8rement adapt\u00e9s</li> <li>Comprendre les m\u00e9canismes de m\u00e9moire qui font la force de ces architectures</li> <li>Impl\u00e9menter un mod\u00e8le LSTM pour l'analyse de sentiment de texte</li> <li>Visualiser et interpr\u00e9ter les repr\u00e9sentations internes du mod\u00e8le</li> </ul> <p>Ces connaissances constitueront une base essentielle pour le d\u00e9veloppement de votre projet de chatbot p\u00e9dagogique dans les prochains modules.</p> <p>Retour \u00e0 la vue d'ensemble du Module 2 Continuer vers le Module 3</p>"},{"location":"module2/ressources/Partie2-Phase1-fiche-observationsCNN/","title":"Fiche d'oberservation du Mini-Projet CNN - Reconnaissance de chiffres manuscrits","text":""},{"location":"module2/ressources/Partie2-Phase1-fiche-observationsCNN/#informations-generales","title":"Informations g\u00e9n\u00e9rales","text":"<p>Nom et pr\u00e9nom: ______ Date: ________</p>"},{"location":"module2/ressources/Partie2-Phase1-fiche-observationsCNN/#partie-1-resultats-de-test-de-lapplication","title":"Partie 1 : R\u00e9sultats de test de l'application","text":""},{"location":"module2/ressources/Partie2-Phase1-fiche-observationsCNN/#tests-pratiques-avec-linterface","title":"Tests pratiques avec l'interface","text":"Type de test Nombre de tests Pr\u00e9dictions correctes Pr\u00e9dictions incorrectes Taux de r\u00e9ussite Dessin \u00e0 la souris Image import\u00e9e"},{"location":"module2/ressources/Partie2-Phase1-fiche-observationsCNN/#observations-sur-les-predictions","title":"Observations sur les pr\u00e9dictions","text":"<p>Chiffres les mieux reconnus: ____ Chiffres les plus difficiles \u00e0 reconna\u00eetre: __ Niveau de confiance moyen observ\u00e9: _______</p>"},{"location":"module2/ressources/Partie2-Phase1-fiche-observationsCNN/#partie-2-analyse-de-larchitecture-cnn","title":"Partie 2 : Analyse de l'architecture CNN","text":""},{"location":"module2/ressources/Partie2-Phase1-fiche-observationsCNN/#architecture-du-modele-utilise","title":"Architecture du mod\u00e8le utilis\u00e9","text":"<p>Nombre de couches de convolution: ____ Nombre de couches de pooling: ____ Nombre de couches enti\u00e8rement connect\u00e9es: ___ Fonction d'activation utilis\u00e9e: ______</p>"},{"location":"module2/ressources/Partie2-Phase1-fiche-observationsCNN/#analyse-des-visualisations","title":"Analyse des visualisations","text":"<p>Quelles caract\u00e9ristiques semblent \u00eatre d\u00e9tect\u00e9es par les premi\u00e8res couches de convolution? <pre><code>_________________________________________________________________\n_________________________________________________________________\n</code></pre></p> <p>Comment \u00e9voluent les feature maps dans les couches plus profondes? <pre><code>_________________________________________________________________\n_________________________________________________________________\n</code></pre></p> <p>L'observation des feature maps vous aide-t-elle \u00e0 comprendre les erreurs du mod\u00e8le? <pre><code>_________________________________________________________________\n_________________________________________________________________\n</code></pre></p>"},{"location":"module2/ressources/Partie2-Phase1-fiche-observationsCNN/#partie-3-avantages-et-limitations","title":"Partie 3 : Avantages et limitations","text":""},{"location":"module2/ressources/Partie2-Phase1-fiche-observationsCNN/#points-forts-de-lapplication","title":"Points forts de l'application","text":""},{"location":"module2/ressources/Partie2-Phase1-fiche-observationsCNN/#limitations-observees","title":"Limitations observ\u00e9es","text":""},{"location":"module2/ressources/Partie2-Phase1-fiche-observationsCNN/#propositions-damelioration","title":"Propositions d'am\u00e9lioration","text":""},{"location":"module2/ressources/Partie2-Phase1-fiche-observationsCNN/#partie-4-comprehension-des-concepts-cnn","title":"Partie 4 : Compr\u00e9hension des concepts CNN","text":""},{"location":"module2/ressources/Partie2-Phase1-fiche-observationsCNN/#concept-des-convolutions","title":"Concept des convolutions","text":"<p>Expliquez bri\u00e8vement comment fonctionnent les convolutions et leur avantage pour la reconnaissance d'images: <pre><code>_________________________________________________________________\n_________________________________________________________________\n_________________________________________________________________\n</code></pre></p>"},{"location":"module2/ressources/Partie2-Phase1-fiche-observationsCNN/#concept-du-pooling","title":"Concept du pooling","text":"<p>Quel est le r\u00f4le des couches de pooling dans le r\u00e9seau et pourquoi sont-elles importantes? <pre><code>_________________________________________________________________\n_________________________________________________________________\n_________________________________________________________________\n</code></pre></p>"},{"location":"module2/ressources/Partie2-Phase1-fiche-observationsCNN/#transfer-learning","title":"Transfer learning","text":"<p>Comment pourrait-on utiliser le transfer learning pour am\u00e9liorer cette application? <pre><code>_________________________________________________________________\n_________________________________________________________________\n</code></pre></p>"},{"location":"module2/ressources/Partie2-Phase1-fiche-observationsCNN/#partie-5-applications-potentielles","title":"Partie 5 : Applications potentielles","text":""},{"location":"module2/ressources/Partie2-Phase1-fiche-observationsCNN/#cas-dutilisation-possibles","title":"Cas d'utilisation possibles","text":"<p>Citez deux applications pratiques o\u00f9 cette technologie pourrait \u00eatre utilis\u00e9e en entreprise: 1. _________ 2. _________</p>"},{"location":"module2/ressources/Partie2-Phase1-fiche-observationsCNN/#extension-a-dautres-problemes","title":"Extension \u00e0 d'autres probl\u00e8mes","text":"<p>Comment adapteriez-vous ce mod\u00e8le pour reconna\u00eetre d'autres types d'objets que des chiffres? <pre><code>_________________________________________________________________\n_________________________________________________________________\n</code></pre></p>"},{"location":"module2/ressources/Partie2-Phase1-fiche-observationsCNN/#partie-6-conclusion","title":"Partie 6 : Conclusion","text":""},{"location":"module2/ressources/Partie2-Phase1-fiche-observationsCNN/#impact-sur-votre-comprehension","title":"Impact sur votre compr\u00e9hension","text":"<p>Qu'avez-vous appris de ce mini-projet concernant les CNN que vous ne saviez pas avant? <pre><code>_________________________________________________________________\n_________________________________________________________________\n</code></pre></p>"},{"location":"module2/ressources/Partie2-Phase1-fiche-observationsCNN/#pertinence-pour-le-projet-chatbot","title":"Pertinence pour le projet chatbot","text":"<p>Comment ces connaissances sur les CNN pourraient-elles \u00eatre utiles pour le projet final de chatbot p\u00e9dagogique? <pre><code>_________________________________________________________________\n_________________________________________________________________\n</code></pre></p>"},{"location":"module2/ressources/Partie2-Phase1-fiche-observationsCNN/#auto-evaluation","title":"Auto-\u00e9valuation","text":"Crit\u00e8re Points possibles Points obtenus Commentaires Qualit\u00e9 des observations 5 Compr\u00e9hension des concepts 8 Analyse critique 5 Propositions d'am\u00e9lioration 4 Qualit\u00e9 r\u00e9dactionnelle 3 TOTAL 25 <p>Feedback global: <pre><code>_________________________________________________________________\n_________________________________________________________________\n_________________________________________________________________\n</code></pre></p>"},{"location":"module2/ressources/cnn-classification/","title":"CNN pour la classification d'images - MNIST","text":"<p>Ce document contient le code et les explications pour le notebook de classification d'images MNIST avec un CNN. Vous pouvez copier-coller chaque section dans une cellule Google Colab.</p>"},{"location":"module2/ressources/cnn-classification/#cellule-1-markdown-introduction","title":"Cellule 1 (Markdown) - Introduction","text":"<pre><code># CNN pour la classification d'images - MNIST\n\n## BTS SIO  - S\u00e9ance 2: Types de r\u00e9seaux de neurones\n\nCe notebook vous guidera \u00e0 travers l'impl\u00e9mentation et l'utilisation d'un r\u00e9seau de neurones convolutif (CNN) pour la classification d'images, en utilisant le c\u00e9l\u00e8bre dataset MNIST des chiffres manuscrits.\n\n### Objectifs d'apprentissage:\n- Comprendre l'architecture d'un r\u00e9seau convolutif (CNN)\n- Impl\u00e9menter un CNN avec TensorFlow/Keras\n- Visualiser les filtres et feature maps\n- Analyser les performances du mod\u00e8le\n\n### Pr\u00e9requis:\n- Connaissances de base en Python\n- Avoir suivi la s\u00e9ance 1 d'introduction au Deep Learning\n</code></pre>"},{"location":"module2/ressources/cnn-classification/#1-configuration-et-imports","title":"1. Configuration et imports","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.datasets import mnist\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\n# Reproductibilit\u00e9\nnp.random.seed(42)\ntf.random.set_seed(42)\n</code></pre>"},{"location":"module2/ressources/cnn-classification/#2-chargement-et-preparation-des-donnees","title":"2. Chargement et pr\u00e9paration des donn\u00e9es","text":"<pre><code># Charger MNIST\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\n# Redimensionner et normaliser\nX_train = X_train.reshape(-1, 28, 28, 1) / 255.0\nX_test = X_test.reshape(-1, 28, 28, 1) / 255.0\n\n# Convertir les \u00e9tiquettes en one-hot\ny_train_onehot = to_categorical(y_train, 10)\ny_test_onehot = to_categorical(y_test, 10)\n\n# Visualiser quelques exemples\nplt.figure(figsize=(10, 4))\nfor i in range(10):\n    plt.subplot(2, 5, i+1)\n    plt.imshow(X_train[i].reshape(28, 28), cmap='gray')\n    plt.title(f\"Chiffre: {y_train[i]}\")\n    plt.axis('off')\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"module2/ressources/cnn-classification/#3-creation-du-modele-cnn","title":"3. Cr\u00e9ation du mod\u00e8le CNN","text":"<pre><code># Cr\u00e9er un mod\u00e8le CNN\nmodel = Sequential([\n    # Premi\u00e8re couche de convolution\n    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), name='conv1'),\n    MaxPooling2D((2, 2), name='pool1'),\n\n    # Deuxi\u00e8me couche de convolution\n    Conv2D(64, (3, 3), activation='relu', name='conv2'),\n    MaxPooling2D((2, 2), name='pool2'),\n\n    # Aplatissement et couches denses\n    Flatten(name='flatten'),\n    Dense(128, activation='relu', name='dense1'),\n    Dropout(0.5, name='dropout1'),\n    Dense(10, activation='softmax', name='output')\n])\n\n# Compiler le mod\u00e8le\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Afficher le r\u00e9sum\u00e9\nmodel.summary()\n</code></pre>"},{"location":"module2/ressources/cnn-classification/#4-entrainement-du-modele","title":"4. Entra\u00eenement du mod\u00e8le","text":"<pre><code># Entra\u00eener le mod\u00e8le\nhistory = model.fit(\n    X_train, y_train_onehot, \n    batch_size=128, \n    epochs=5,\n    validation_split=0.2,\n    verbose=1\n)\n\n# Visualiser les courbes d'apprentissage\nplt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Train')\nplt.plot(history.history['val_accuracy'], label='Validation')\nplt.title('Pr\u00e9cision')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Train')\nplt.plot(history.history['val_loss'], label='Validation')\nplt.title('Perte')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"module2/ressources/cnn-classification/#5-evaluation-du-modele","title":"5. \u00c9valuation du mod\u00e8le","text":"<pre><code># \u00c9valuer sur le jeu de test\ntest_loss, test_acc = model.evaluate(X_test, y_test_onehot, verbose=1)\nprint(f\"Pr\u00e9cision sur l'ensemble de test: {test_acc*100:.2f}%\")\n\n# Pr\u00e9dictions et matrice de confusion\ny_pred = model.predict(X_test)\ny_pred_classes = np.argmax(y_pred, axis=1)\n\n# Matrice de confusion\nplt.figure(figsize=(8, 6))\nsns.heatmap(confusion_matrix(y_test, y_pred_classes), annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Pr\u00e9dit')\nplt.ylabel('R\u00e9el')\nplt.title('Matrice de confusion')\nplt.show()\n\n# Afficher des erreurs\nmisclassified = np.where(y_pred_classes != y_test)[0]\nplt.figure(figsize=(10, 4))\nfor i, idx in enumerate(misclassified[:10]):\n    plt.subplot(2, 5, i+1)\n    plt.imshow(X_test[idx].reshape(28, 28), cmap='gray')\n    plt.title(f\"R:{y_test[idx]} P:{y_pred_classes[idx]}\")\n    plt.axis('off')\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"module2/ressources/cnn-classification/#6-visualisation-des-filtres-et-feature-maps","title":"6. Visualisation des filtres et feature maps","text":"<pre><code># Visualiser les filtres de la premi\u00e8re couche\n# Approche alternative compl\u00e8te pour la visualisation\nprint(\"Initialisation et visualisation avec une approche alternative...\")\n\n# 1. R\u00e9initialiser le mod\u00e8le pour s'assurer qu'il est correctement d\u00e9fini\nmodel = Sequential([\n    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), name='conv1'),\n    MaxPooling2D((2, 2), name='pool1'),\n    Conv2D(64, (3, 3), activation='relu', name='conv2'),\n    MaxPooling2D((2, 2), name='pool2'),\n    Flatten(name='flatten'),\n    Dense(128, activation='relu', name='dense1'),\n    Dropout(0.5, name='dropout1'),\n    Dense(10, activation='softmax', name='output')\n])\n\n# 2. Compiler le mod\u00e8le\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# 3. Forcer l'initialisation avec build ET un forward pass\nmodel.build(input_shape=(None, 28, 28, 1))\ndummy_input = np.zeros((1, 28, 28, 1))\n_ = model(dummy_input)\n\n# 4. V\u00e9rifier que les couches sont accessibles\nprint(f\"Couches dans le mod\u00e8le: {[layer.name for layer in model.layers]}\")\n\n# 5. Cr\u00e9er et visualiser des poids al\u00e9atoires puisque le mod\u00e8le n'est pas entra\u00een\u00e9\nfilters = np.random.normal(size=(3, 3, 1, 8))  # Simuler 8 filtres 3x3\nf_min, f_max = filters.min(), filters.max()\nfilters = (filters - f_min) / (f_max - f_min)\n\nplt.figure(figsize=(10, 4))\nfor i in range(8):\n    plt.subplot(2, 4, i+1)\n    plt.imshow(filters[:, :, 0, i], cmap='viridis')\n    plt.title(f'Filtre {i+1}')\n    plt.axis('off')\nplt.tight_layout()\nplt.show()\n\n# 6. Simuler des feature maps al\u00e9atoires\nsample_idx = 12\nsample_image = X_test[sample_idx]\nplt.figure(figsize=(3, 3))\nplt.imshow(sample_image.reshape(28, 28), cmap='gray')\nplt.title(f\"Chiffre: {y_test[sample_idx]}\")\nplt.axis('off')\nplt.show()\n\n# 7. G\u00e9n\u00e9rer des feature maps simul\u00e9es\nfeature_maps = np.random.rand(1, 26, 26, 8)  # Taille typique apr\u00e8s convolution 3x3\n\nplt.figure(figsize=(10, 4))\nfor i in range(8):\n    plt.subplot(2, 4, i+1)\n    plt.imshow(feature_maps[0, :, :, i], cmap='viridis')\n    plt.axis('off')\nplt.suptitle('Feature Maps - Couche 1 (Simul\u00e9es)')\nplt.tight_layout()\nplt.show()\n\nprint(\"Visualisation termin\u00e9e avec des donn\u00e9es simul\u00e9es.\")\nprint(\"Note: Pour voir les vrais filtres et feature maps, le mod\u00e8le doit \u00eatre entra\u00een\u00e9.\")\n</code></pre>"},{"location":"module2/ressources/cnn-classification/#7-test-avec-des-images-bruitees","title":"7. Test avec des images bruit\u00e9es","text":"<pre><code># Ajouter du bruit\ndef add_noise(images, noise_level=0.2):\n    noisy_images = images.copy()\n    noise = np.random.normal(0, noise_level, images.shape)\n    return np.clip(noisy_images + noise, 0, 1)\n\n# Tester avec quelques images bruit\u00e9es\ntest_samples = X_test[:5]\nnoisy_samples = add_noise(test_samples, noise_level=0.3)\n\n# Afficher les images originales et bruit\u00e9es\nplt.figure(figsize=(10, 4))\nfor i in range(5):\n    plt.subplot(2, 5, i+1)\n    plt.imshow(test_samples[i].reshape(28, 28), cmap='gray')\n    plt.title(f\"Original: {y_test[i]}\")\n    plt.axis('off')\n\n    plt.subplot(2, 5, i+6)\n    plt.imshow(noisy_samples[i].reshape(28, 28), cmap='gray')\n    plt.axis('off')\nplt.tight_layout()\nplt.show()\n\n# Pr\u00e9dictions sur les images bruit\u00e9es\npredictions = model.predict(noisy_samples)\npred_classes = np.argmax(predictions, axis=1)\n\nprint(\"R\u00e9sultats sur les images bruit\u00e9es:\")\nfor i in range(5):\n    print(f\"Image {i+1} - R\u00e9el: {y_test[i]}, Pr\u00e9dit: {pred_classes[i]}\")\n</code></pre>"},{"location":"module2/ressources/cnn-classification/#8-exercice-ameliorez-le-modele","title":"8. Exercice: Am\u00e9liorez le mod\u00e8le","text":"<p>Modifiez l'architecture pour am\u00e9liorer les performances:</p> <ol> <li>Essayez d'ajouter une couche de convolution suppl\u00e9mentaire</li> <li>Modifiez le nombre de filtres ou leur taille</li> <li>Ajustez les param\u00e8tres d'entra\u00eenement (epochs, batch_size)</li> </ol> <pre><code># VOTRE CODE ICI - Cr\u00e9ez un mod\u00e8le am\u00e9lior\u00e9\nimproved_model = Sequential([\n    # Ajoutez votre architecture am\u00e9lior\u00e9e ici\n])\n\n# Compiler et entra\u00eener votre mod\u00e8le\n# ...\n</code></pre>"},{"location":"module2/ressources/cnn-classification/#9-sauvegarde-du-modele-pour-lapplication-web","title":"9. Sauvegarde du mod\u00e8le pour l'application web","text":"<pre><code># Sauvegarder le mod\u00e8le pour l'int\u00e9gration web\nmodel.save('mnist_cnn_model.h5')\nprint(\"Mod\u00e8le sauvegard\u00e9 avec succ\u00e8s!\")\n\n# Si vous utilisez Google Colab, t\u00e9l\u00e9chargez le fichier\ntry:\n    from google.colab import files\n    files.download('mnist_cnn_model.h5')\n    print(\"T\u00e9l\u00e9chargement du fichier initi\u00e9...\")\nexcept:\n    print(\"Vous n'\u00eates pas sur Google Colab. Le mod\u00e8le est sauvegard\u00e9 localement.\")\n</code></pre>"},{"location":"module2/ressources/cnn-classification/#questions-de-reflexion","title":"Questions de r\u00e9flexion","text":"<ol> <li>Qu'est-ce qui rend les CNNs plus efficaces que les r\u00e9seaux denses pour les images?</li> <li>Comment les couches de convolution extraient-elles les caract\u00e9ristiques des images?</li> <li>Pourquoi utilisons-nous le pooling dans les CNNs?</li> <li>Quelles am\u00e9liorations pourriez-vous apporter pour rendre le mod\u00e8le plus robuste au bruit?</li> </ol>"},{"location":"module2/ressources/create_model/","title":"Create model","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.utils import to_categorical\n</pre> import numpy as np import tensorflow as tf from tensorflow.keras.datasets import mnist from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout from tensorflow.keras.utils import to_categorical In\u00a0[\u00a0]: Copied! <pre>print(\"TensorFlow version:\", tf.__version__)\n</pre> print(\"TensorFlow version:\", tf.__version__) In\u00a0[\u00a0]: Copied! <pre># Pour la reproductibilit\u00e9\nnp.random.seed(42)\ntf.random.set_seed(42)\n</pre> # Pour la reproductibilit\u00e9 np.random.seed(42) tf.random.set_seed(42) In\u00a0[\u00a0]: Copied! <pre># Charger les donn\u00e9es MNIST\nprint(\"Chargement des donn\u00e9es MNIST...\")\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n</pre> # Charger les donn\u00e9es MNIST print(\"Chargement des donn\u00e9es MNIST...\") (X_train, y_train), (X_test, y_test) = mnist.load_data() In\u00a0[\u00a0]: Copied! <pre># Pr\u00e9traitement des donn\u00e9es\nX_train = X_train.reshape(-1, 28, 28, 1) / 255.0\nX_test = X_test.reshape(-1, 28, 28, 1) / 255.0\n</pre> # Pr\u00e9traitement des donn\u00e9es X_train = X_train.reshape(-1, 28, 28, 1) / 255.0 X_test = X_test.reshape(-1, 28, 28, 1) / 255.0 In\u00a0[\u00a0]: Copied! <pre># Conversion des \u00e9tiquettes en format one-hot\ny_train_onehot = to_categorical(y_train, 10)\ny_test_onehot = to_categorical(y_test, 10)\n</pre> # Conversion des \u00e9tiquettes en format one-hot y_train_onehot = to_categorical(y_train, 10) y_test_onehot = to_categorical(y_test, 10) In\u00a0[\u00a0]: Copied! <pre># Cr\u00e9ation du mod\u00e8le CNN\nmodel = Sequential([\n    # Premi\u00e8re couche de convolution\n    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), name='conv1'),\n    MaxPooling2D((2, 2), name='pool1'),\n    \n    # Deuxi\u00e8me couche de convolution\n    Conv2D(64, (3, 3), activation='relu', name='conv2'),\n    MaxPooling2D((2, 2), name='pool2'),\n    \n    # Aplatissement pour passer aux couches denses\n    Flatten(name='flatten'),\n    \n    # Couches denses (fully connected)\n    Dense(128, activation='relu', name='dense1'),\n    Dropout(0.5, name='dropout1'),  # \u00c9vite le surapprentissage\n    Dense(10, activation='softmax', name='output')  # 10 classes (chiffres 0-9)\n])\n</pre> # Cr\u00e9ation du mod\u00e8le CNN model = Sequential([     # Premi\u00e8re couche de convolution     Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), name='conv1'),     MaxPooling2D((2, 2), name='pool1'),          # Deuxi\u00e8me couche de convolution     Conv2D(64, (3, 3), activation='relu', name='conv2'),     MaxPooling2D((2, 2), name='pool2'),          # Aplatissement pour passer aux couches denses     Flatten(name='flatten'),          # Couches denses (fully connected)     Dense(128, activation='relu', name='dense1'),     Dropout(0.5, name='dropout1'),  # \u00c9vite le surapprentissage     Dense(10, activation='softmax', name='output')  # 10 classes (chiffres 0-9) ]) In\u00a0[\u00a0]: Copied! <pre># Compiler le mod\u00e8le\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n</pre> # Compiler le mod\u00e8le model.compile(     optimizer='adam',     loss='categorical_crossentropy',     metrics=['accuracy'] ) In\u00a0[\u00a0]: Copied! <pre># Afficher le r\u00e9sum\u00e9 de l'architecture\nmodel.summary()\n</pre> # Afficher le r\u00e9sum\u00e9 de l'architecture model.summary() In\u00a0[\u00a0]: Copied! <pre># Entra\u00eenement du mod\u00e8le\nprint(\"Entra\u00eenement du mod\u00e8le...\")\nhistory = model.fit(\n    X_train, \n    y_train_onehot, \n    batch_size=128, \n    epochs=5,  # Entra\u00eenement court pour l'exemple\n    validation_split=0.2,\n    verbose=1\n)\n</pre> # Entra\u00eenement du mod\u00e8le print(\"Entra\u00eenement du mod\u00e8le...\") history = model.fit(     X_train,      y_train_onehot,      batch_size=128,      epochs=5,  # Entra\u00eenement court pour l'exemple     validation_split=0.2,     verbose=1 ) In\u00a0[\u00a0]: Copied! <pre># \u00c9valuer le mod\u00e8le\ntest_loss, test_acc = model.evaluate(X_test, y_test_onehot, verbose=1)\nprint(f\"Pr\u00e9cision sur l'ensemble de test: {test_acc*100:.2f}%\")\n</pre> # \u00c9valuer le mod\u00e8le test_loss, test_acc = model.evaluate(X_test, y_test_onehot, verbose=1) print(f\"Pr\u00e9cision sur l'ensemble de test: {test_acc*100:.2f}%\") In\u00a0[\u00a0]: Copied! <pre># Sauvegarder le mod\u00e8le\nmodel.save('mnist_cnn_model.h5')\nprint(\"Mod\u00e8le sauvegard\u00e9 avec succ\u00e8s sous 'mnist_cnn_model.h5'\")\n</pre> # Sauvegarder le mod\u00e8le model.save('mnist_cnn_model.h5') print(\"Mod\u00e8le sauvegard\u00e9 avec succ\u00e8s sous 'mnist_cnn_model.h5'\")"},{"location":"module2/ressources/evaluationCNN-correction/","title":"\u00c9valuation du Mini-Projet CNN - Reconnaissance de chiffres manuscrits","text":""},{"location":"module2/ressources/evaluationCNN-correction/#corrige-guide-de-levaluateur","title":"CORRIG\u00c9 - GUIDE DE L'\u00c9VALUATEUR","text":"<p>Ce document contient les \u00e9l\u00e9ments de r\u00e9ponse attendus et le bar\u00e8me d\u00e9taill\u00e9 pour l'\u00e9valuation du mini-projet CNN.</p>"},{"location":"module2/ressources/evaluationCNN-correction/#partie-1-resultats-de-test-de-lapplication","title":"Partie 1 : R\u00e9sultats de test de l'application","text":""},{"location":"module2/ressources/evaluationCNN-correction/#tests-pratiques-avec-linterface","title":"Tests pratiques avec l'interface","text":"Type de test Nombre de tests Pr\u00e9dictions correctes Pr\u00e9dictions incorrectes Taux de r\u00e9ussite Dessin \u00e0 la souris 10 (recommand\u00e9) ~7-8 (typique) ~2-3 (typique) ~70-80% (attendu) Image import\u00e9e 5 (recommand\u00e9) ~3-4 (typique) ~1-2 (typique) ~60-80% (attendu) <p>Remarque \u00e9valuateur: Les r\u00e9sultats peuvent varier selon la qualit\u00e9 des dessins et des images. L'important est que l'\u00e9tudiant ait effectu\u00e9 suffisamment de tests (&gt;5 par m\u00e9thode) et qu'il ait correctement calcul\u00e9 les taux de r\u00e9ussite.</p>"},{"location":"module2/ressources/evaluationCNN-correction/#observations-sur-les-predictions","title":"Observations sur les pr\u00e9dictions","text":"<p>\u00c9l\u00e9ments de r\u00e9ponse attendus:</p> <p>Chiffres les mieux reconnus:  - Les chiffres avec des structures claires et distinctes comme 0, 1 et 7 sont g\u00e9n\u00e9ralement mieux reconnus.</p> <p>Chiffres les plus difficiles \u00e0 reconna\u00eetre:  - Les chiffres qui peuvent \u00eatre confondus comme 4/9, 3/8, ou 5/6 sont typiquement plus difficiles.</p> <p>Niveau de confiance moyen observ\u00e9:  - Un bon mod\u00e8le devrait montrer ~90% de confiance sur les pr\u00e9dictions correctes.  - L'\u00e9tudiant devrait mentionner la diff\u00e9rence de confiance entre pr\u00e9dictions correctes et incorrectes.</p>"},{"location":"module2/ressources/evaluationCNN-correction/#partie-2-analyse-de-larchitecture-cnn","title":"Partie 2 : Analyse de l'architecture CNN","text":""},{"location":"module2/ressources/evaluationCNN-correction/#architecture-du-modele-utilise","title":"Architecture du mod\u00e8le utilis\u00e9","text":"<p>R\u00e9ponses correctes: - Nombre de couches de convolution: 2 (dans le mod\u00e8le de base) - Nombre de couches de pooling: 2 (dans le mod\u00e8le de base) - Nombre de couches enti\u00e8rement connect\u00e9es: 2 (dans le mod\u00e8le de base) - Fonction d'activation utilis\u00e9e: ReLU pour les couches interm\u00e9diaires, Softmax pour la couche de sortie</p> <p>Note pour l'\u00e9valuateur: Si l'\u00e9tudiant a exp\u00e9riment\u00e9 avec une architecture diff\u00e9rente, \u00e9valuez la coh\u00e9rence de sa description.</p>"},{"location":"module2/ressources/evaluationCNN-correction/#analyse-des-visualisations","title":"Analyse des visualisations","text":"<p>Quelles caract\u00e9ristiques semblent \u00eatre d\u00e9tect\u00e9es par les premi\u00e8res couches de convolution?</p> <p>\u00c9l\u00e9ments attendus: - D\u00e9tection de caract\u00e9ristiques de bas niveau: contours, bords, lignes simples - Mention de l'orientation des d\u00e9tecteurs (horizontaux, verticaux, diagonaux) - Observation que diff\u00e9rents filtres se sp\u00e9cialisent dans diff\u00e9rentes caract\u00e9ristiques</p> <p>Comment \u00e9voluent les feature maps dans les couches plus profondes?</p> <p>\u00c9l\u00e9ments attendus: - Caract\u00e9ristiques de plus haut niveau et plus abstraites dans les couches profondes - Combinaison des caract\u00e9ristiques simples en motifs plus complexes - Diminution de la r\u00e9solution spatiale mais augmentation de la profondeur s\u00e9mantique - Sp\u00e9cialisation progressive des filtres pour des parties sp\u00e9cifiques des chiffres</p> <p>L'observation des feature maps vous aide-t-elle \u00e0 comprendre les erreurs du mod\u00e8le?</p> <p>\u00c9l\u00e9ments attendus: - Identification de caract\u00e9ristiques similaires entre chiffres souvent confondus - Observation des zones d'activation fortes/faibles sur les exemples mal classifi\u00e9s - Mention que certains filtres peuvent ne pas s'activer correctement sur des entr\u00e9es ambigu\u00ebs - Explication de l'impact de la r\u00e9solution r\u00e9duite sur la distinction de d\u00e9tails fins</p>"},{"location":"module2/ressources/evaluationCNN-correction/#partie-3-avantages-et-limitations","title":"Partie 3 : Avantages et limitations","text":""},{"location":"module2/ressources/evaluationCNN-correction/#points-forts-de-lapplication","title":"Points forts de l'application","text":"<p>Exemples de r\u00e9ponses pertinentes: 1. Interface intuitive permettant de tester facilement le mod\u00e8le avec diff\u00e9rentes entr\u00e9es 2. Bonne pr\u00e9cision sur les chiffres clairement \u00e9crits (&gt;70-80%) 3. Temps de r\u00e9ponse rapide pour les pr\u00e9dictions en temps r\u00e9el 4. Visualisation des feature maps qui aide \u00e0 comprendre le fonctionnement interne du mod\u00e8le 5. Robustesse relative aux variations mineures dans l'\u00e9criture</p>"},{"location":"module2/ressources/evaluationCNN-correction/#limitations-observees","title":"Limitations observ\u00e9es","text":"<p>Exemples de r\u00e9ponses pertinentes: 1. Sensibilit\u00e9 \u00e0 l'\u00e9paisseur des traits et au positionnement du chiffre 2. Difficult\u00e9 avec les styles d'\u00e9criture tr\u00e8s diff\u00e9rents des donn\u00e9es d'entra\u00eenement 3. Confusion entre certains chiffres visuellement similaires (4/9, 3/8) 4. Performance r\u00e9duite sur les chiffres mal centr\u00e9s ou de taille inappropri\u00e9e 5. Absence de feedback en temps r\u00e9el pendant le dessin 6. Mod\u00e8le entra\u00een\u00e9 uniquement sur MNIST, limitant sa g\u00e9n\u00e9ralisation</p>"},{"location":"module2/ressources/evaluationCNN-correction/#propositions-damelioration","title":"Propositions d'am\u00e9lioration","text":"<p>Exemples de r\u00e9ponses pertinentes: 1. Augmentation des donn\u00e9es d'entra\u00eenement avec des rotations, translations et d\u00e9formations 2. Pr\u00e9traitement am\u00e9lior\u00e9 (centrage automatique, normalisation) 3. Architecture plus profonde ou utilisation de techniques avanc\u00e9es (ResNet, attention) 4. Interface avec guide visuel pour aider l'utilisateur \u00e0 dessiner dans la zone optimale 5. Feedback en temps r\u00e9el pendant le dessin 6. Syst\u00e8me d'apprentissage continu qui s'am\u00e9liore avec les nouveaux exemples fournis</p>"},{"location":"module2/ressources/evaluationCNN-correction/#partie-4-comprehension-des-concepts-cnn","title":"Partie 4 : Compr\u00e9hension des concepts CNN","text":""},{"location":"module2/ressources/evaluationCNN-correction/#concept-des-convolutions","title":"Concept des convolutions","text":"<p>\u00c9l\u00e9ments essentiels attendus: - Description de la convolution comme une op\u00e9ration de filtrage local - Explication du partage de poids et de la d\u00e9tection de caract\u00e9ristiques ind\u00e9pendamment de leur position - Mention des avantages par rapport aux r\u00e9seaux enti\u00e8rement connect\u00e9s (moins de param\u00e8tres, meilleure g\u00e9n\u00e9ralisation) - Explication de l'extraction hi\u00e9rarchique des caract\u00e9ristiques - R\u00e9f\u00e9rence \u00e0 l'inspiration biologique (champ r\u00e9ceptif du syst\u00e8me visuel)</p>"},{"location":"module2/ressources/evaluationCNN-correction/#concept-du-pooling","title":"Concept du pooling","text":"<p>\u00c9l\u00e9ments essentiels attendus: - D\u00e9finition du pooling comme m\u00e9thode de sous-\u00e9chantillonnage - Explication de la r\u00e9duction de dimensionnalit\u00e9 et des avantages computationnels - Mention de l'invariance \u00e0 de petites translations/d\u00e9formations - Distinction entre Max-Pooling et Average-Pooling - Explication du r\u00f4le dans la hi\u00e9rarchie des caract\u00e9ristiques (augmentation du champ r\u00e9ceptif)</p>"},{"location":"module2/ressources/evaluationCNN-correction/#transfer-learning","title":"Transfer learning","text":"<p>\u00c9l\u00e9ments essentiels attendus: - D\u00e9finition correcte du transfer learning (r\u00e9utilisation d'un mod\u00e8le pr\u00e9-entra\u00een\u00e9) - Suggestion d'utiliser un mod\u00e8le plus large pr\u00e9-entra\u00een\u00e9 sur ImageNet - Explication du gel des couches de base et r\u00e9entra\u00eenement des couches sup\u00e9rieures - Mention des avantages (moins de donn\u00e9es n\u00e9cessaires, convergence plus rapide) - Adaptations n\u00e9cessaires pour les images de chiffres (conversion en RGB, redimensionnement)</p>"},{"location":"module2/ressources/evaluationCNN-correction/#partie-5-applications-potentielles","title":"Partie 5 : Applications potentielles","text":""},{"location":"module2/ressources/evaluationCNN-correction/#cas-dutilisation-possibles","title":"Cas d'utilisation possibles","text":"<p>Exemples de r\u00e9ponses pertinentes: 1. Num\u00e9risation automatique de formulaires manuscrits (assurance, administration) 2. Tri automatique de courrier bas\u00e9 sur les codes postaux manuscrits 3. V\u00e9rification automatique des ch\u00e8ques bancaires 4. Aide \u00e0 la saisie pour personnes \u00e0 mobilit\u00e9 r\u00e9duite 5. OCR pour archives historiques et documents manuscrits 6. Saisie de donn\u00e9es \u00e0 partir de formulaires de recensement ou d'enqu\u00eates manuscrits</p>"},{"location":"module2/ressources/evaluationCNN-correction/#extension-a-dautres-problemes","title":"Extension \u00e0 d'autres probl\u00e8mes","text":"<p>\u00c9l\u00e9ments essentiels attendus: - N\u00e9cessit\u00e9 d'un nouveau jeu de donn\u00e9es d'entra\u00eenement sp\u00e9cifique aux objets cibles - Possible ajustement de l'architecture (plus de couches/filtres pour des objets plus complexes) - Mention des techniques d'augmentation de donn\u00e9es pour compenser les donn\u00e9es limit\u00e9es - R\u00e9f\u00e9rence au transfer learning \u00e0 partir de mod\u00e8les pr\u00e9-entra\u00een\u00e9s sur ImageNet - Adaptation des couches d'entr\u00e9e pour g\u00e9rer des images couleur et de plus haute r\u00e9solution - Consid\u00e9ration des classes d\u00e9s\u00e9quilibr\u00e9es et des strat\u00e9gies pour y rem\u00e9dier</p>"},{"location":"module2/ressources/evaluationCNN-correction/#partie-6-conclusion","title":"Partie 6 : Conclusion","text":""},{"location":"module2/ressources/evaluationCNN-correction/#impact-sur-votre-comprehension","title":"Impact sur votre compr\u00e9hension","text":"<p>\u00c9l\u00e9ments de r\u00e9ponse valoris\u00e9s: - Compr\u00e9hension concr\u00e8te du fonctionnement interne des CNN gr\u00e2ce \u00e0 la visualisation - Appr\u00e9ciation de l'importance du pr\u00e9traitement des donn\u00e9es - Prise de conscience des forces et limites des CNN pour la reconnaissance d'images - Compr\u00e9hension pratique de l'impact des hyperparam\u00e8tres sur les performances - R\u00e9alisation du foss\u00e9 entre les exemples acad\u00e9miques et les applications r\u00e9elles</p>"},{"location":"module2/ressources/evaluationCNN-correction/#pertinence-pour-le-projet-chatbot","title":"Pertinence pour le projet chatbot","text":"<p>\u00c9l\u00e9ments de r\u00e9ponse valoris\u00e9s: - Int\u00e9gration possible des concepts CNN dans la base de connaissances du chatbot - Capacit\u00e9 \u00e0 expliquer visuellement le fonctionnement des CNN dans le chatbot - Possibilit\u00e9 d'int\u00e9grer des visualisations interactives pour l'apprentissage - Compr\u00e9hension approfondie permettant de mieux structurer les explications du chatbot - Identification des concepts difficiles n\u00e9cessitant des explications plus d\u00e9taill\u00e9es</p>"},{"location":"module2/ressources/evaluationCNN-correction/#auto-evaluation","title":"Auto-\u00e9valuation","text":"<p>La note attribu\u00e9e par l'\u00e9tudiant doit \u00eatre coh\u00e9rente avec le reste de ses r\u00e9ponses.</p>"},{"location":"module2/ressources/evaluationCNN-correction/#bareme-detaille-pour-levaluateur","title":"Bar\u00e8me d\u00e9taill\u00e9 pour l'\u00e9valuateur","text":"Crit\u00e8re Points possibles R\u00e9partition des points Qualit\u00e9 des observations 5 \u2022 R\u00e9sultats des tests (1 pt)\u2022 Observations sur les pr\u00e9dictions (1 pt)\u2022 Analyse des visualisations (3 pts) Compr\u00e9hension des concepts 8 \u2022 Architecture CNN (2 pts)\u2022 Convolutions (2 pts)\u2022 Pooling (2 pts)\u2022 Transfer learning (2 pts) Analyse critique 5 \u2022 Points forts identifi\u00e9s (1.5 pts)\u2022 Limitations identifi\u00e9es (1.5 pts)\u2022 Coh\u00e9rence globale de l'analyse (2 pts) Propositions d'am\u00e9lioration 4 \u2022 Pertinence des am\u00e9liorations (2 pts)\u2022 Faisabilit\u00e9 des propositions (1 pt)\u2022 Applications potentielles (1 pt) Qualit\u00e9 r\u00e9dactionnelle 3 \u2022 Clart\u00e9 des explications (1 pt)\u2022 Structure des r\u00e9ponses (1 pt)\u2022 Terminologie appropri\u00e9e (1 pt) TOTAL 25 <p>Guide d'attribution des notes:</p> <p>Excellente r\u00e9ponse (100% des points): - D\u00e9monstration compl\u00e8te de compr\u00e9hension - R\u00e9f\u00e9rences sp\u00e9cifiques \u00e0 l'exp\u00e9rience pratique - Analyse nuanc\u00e9e et r\u00e9fl\u00e9chie - Terminologie technique correcte</p> <p>Bonne r\u00e9ponse (75% des points): - Compr\u00e9hension solide des concepts cl\u00e9s - Quelques observations pratiques sp\u00e9cifiques - Analyse coh\u00e9rente mais pas toujours approfondie - Terminologie g\u00e9n\u00e9ralement correcte</p> <p>R\u00e9ponse moyenne (50% des points): - Compr\u00e9hension basique des concepts - Observations g\u00e9n\u00e9riques sans sp\u00e9cificit\u00e9 - Analyse superficielle - Quelques erreurs terminologiques</p> <p>R\u00e9ponse insuffisante (25% des points): - Compr\u00e9hension limit\u00e9e ou erron\u00e9e des concepts - Peu ou pas d'observations pratiques - Analyse incoh\u00e9rente ou tr\u00e8s limit\u00e9e - Erreurs terminologiques importantes</p> <p>R\u00e9ponse absente ou incorrect (0 points): - Absence de r\u00e9ponse - Contenu hors sujet - Incompr\u00e9hension fondamentale des concepts</p>"},{"location":"module2/ressources/lstm-sentiment-analyse/","title":"Points cl\u00e9s \u00e0 explorer - LSTM pour l'analyse de sentiment","text":""},{"location":"module2/ressources/lstm-sentiment-analyse/#introduction","title":"Introduction","text":"<p>Ce document approfondit les aspects essentiels des r\u00e9seaux LSTM (Long Short-Term Memory) pour l'analyse de sentiment. Il est con\u00e7u pour accompagner le mini-projet RNN du Module 2 et vous aider \u00e0 mieux comprendre le fonctionnement interne de ces r\u00e9seaux.</p>"},{"location":"module2/ressources/lstm-sentiment-analyse/#1-transformation-du-texte-en-entrees-numeriques","title":"1. Transformation du texte en entr\u00e9es num\u00e9riques","text":""},{"location":"module2/ressources/lstm-sentiment-analyse/#processus-de-tokenisation","title":"Processus de tokenisation","text":"<p>Le texte brut doit \u00eatre converti en valeurs num\u00e9riques pour \u00eatre trait\u00e9 par un r\u00e9seau de neurones. Cette conversion se fait g\u00e9n\u00e9ralement en plusieurs \u00e9tapes :</p> <ol> <li> <p>Nettoyage du texte : Suppression de la ponctuation, conversion en minuscules, \u00e9limination des mots vides (stopwords)    <pre><code># Exemple de nettoyage\nimport re\nimport string\n\ndef clean_text(text):\n    text = text.lower()  # Conversion en minuscules\n    text = re.sub(f'[{string.punctuation}]', ' ', text)  # Suppression de la ponctuation\n    text = re.sub(r'\\s+', ' ', text)  # Remplacement des espaces multiples\n    return text.strip()\n</code></pre></p> </li> <li> <p>Tokenisation : D\u00e9coupage du texte en mots individuels (tokens)    <pre><code># Exemple simple de tokenisation\ndef tokenize(text):\n    return text.split()\n</code></pre></p> </li> <li> <p>Cr\u00e9ation d'un vocabulaire : Attribution d'un index unique \u00e0 chaque mot unique    <pre><code># Cr\u00e9ation d'un vocabulaire\ndef build_vocab(texts):\n    vocab = {}\n    idx = 1  # R\u00e9server 0 pour le padding\n    for text in texts:\n        for word in tokenize(clean_text(text)):\n            if word not in vocab:\n                vocab[word] = idx\n                idx += 1\n    return vocab\n</code></pre></p> </li> <li> <p>Conversion en s\u00e9quences num\u00e9riques : Remplacement de chaque mot par son index    <pre><code># Conversion texte \u2192 s\u00e9quence num\u00e9rique\ndef text_to_sequence(text, vocab):\n    return [vocab.get(word, 0) for word in tokenize(clean_text(text))]\n</code></pre></p> </li> <li> <p>Padding : Uniformisation de la longueur des s\u00e9quences    <pre><code># Padding des s\u00e9quences\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nsequences = [text_to_sequence(text, vocab) for text in texts]\npadded_sequences = pad_sequences(sequences, maxlen=100, padding='post')\n</code></pre></p> </li> </ol>"},{"location":"module2/ressources/lstm-sentiment-analyse/#embeddings-de-mots","title":"Embeddings de mots","text":"<p>Une fois les mots convertis en indices, ils sont transform\u00e9s en vecteurs denses via une couche d'embedding :</p> <pre><code># Cr\u00e9ation d'une couche d'embedding\nfrom tensorflow.keras.layers import Embedding\n\nembedding_dim = 100\nvocab_size = len(vocab) + 1  # +1 pour l'index de padding (0)\n\nembedding_layer = Embedding(\n    input_dim=vocab_size,\n    output_dim=embedding_dim,\n    input_length=max_sequence_length,\n    mask_zero=True  # Pour ignorer les tokens de padding\n)\n</code></pre> <p>Ces embeddings repr\u00e9sentent les mots dans un espace vectoriel o\u00f9 des mots s\u00e9mantiquement proches ont des vecteurs similaires.</p>"},{"location":"module2/ressources/lstm-sentiment-analyse/#2-gestion-de-linformation-a-long-terme-par-les-cellules-lstm","title":"2. Gestion de l'information \u00e0 long terme par les cellules LSTM","text":""},{"location":"module2/ressources/lstm-sentiment-analyse/#architecture-dune-cellule-lstm","title":"Architecture d'une cellule LSTM","text":"<p>Une cellule LSTM utilise trois \"portes\" pour contr\u00f4ler le flux d'information :</p> <ol> <li>Porte d'oubli (Forget Gate) : D\u00e9termine quelles informations de l'\u00e9tat pr\u00e9c\u00e9dent doivent \u00eatre conserv\u00e9es ou supprim\u00e9es</li> <li>Formule : f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)</li> <li> <p>O\u00f9 \\sigma est la fonction sigmoid qui produit des valeurs entre 0 et 1</p> </li> <li> <p>Porte d'entr\u00e9e (Input Gate) : Contr\u00f4le quelles nouvelles informations sont ajout\u00e9es \u00e0 l'\u00e9tat de la cellule</p> </li> <li>Formule : i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)</li> <li> <p>Nouvelles valeurs candidates : \\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)</p> </li> <li> <p>Porte de sortie (Output Gate) : D\u00e9termine quelle partie de l'\u00e9tat de la cellule sera transmise \u00e0 la sortie</p> </li> <li>Formule : o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)</li> </ol>"},{"location":"module2/ressources/lstm-sentiment-analyse/#mecanisme-de-memoire","title":"M\u00e9canisme de m\u00e9moire","text":"<p>La force des LSTM r\u00e9side dans leur capacit\u00e9 \u00e0 maintenir une m\u00e9moire \u00e0 long terme gr\u00e2ce \u00e0 l'\u00e9tat de la cellule (C_t) :</p> <ol> <li>Mise \u00e0 jour de l'\u00e9tat : </li> <li>C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t</li> <li>L'\u00e9tat pr\u00e9c\u00e9dent est partiellement oubli\u00e9 (multipli\u00e9 par f_t)</li> <li> <p>Les nouvelles informations sont ajout\u00e9es (multipli\u00e9es par i_t)</p> </li> <li> <p>Calcul de la sortie :</p> </li> <li>h_t = o_t * \\tanh(C_t)</li> <li>La sortie est une version filtr\u00e9e de l'\u00e9tat de la cellule</li> </ol> <p>Ce m\u00e9canisme permet aux LSTM de : - M\u00e9moriser des informations importantes sur de longues s\u00e9quences - Oublier les informations non pertinentes - Mettre \u00e0 jour leur m\u00e9moire de mani\u00e8re s\u00e9lective</p>"},{"location":"module2/ressources/lstm-sentiment-analyse/#3-difference-entre-embeddings-de-mots-positifs-et-negatifs","title":"3. Diff\u00e9rence entre embeddings de mots positifs et n\u00e9gatifs","text":""},{"location":"module2/ressources/lstm-sentiment-analyse/#proprietes-des-embeddings","title":"Propri\u00e9t\u00e9s des embeddings","text":"<p>Apr\u00e8s entra\u00eenement, les embeddings de mots similaires se rapprochent dans l'espace vectoriel. Pour l'analyse de sentiment, cela signifie que :</p> <ol> <li>Mots positifs : Les embeddings de mots comme \"excellent\", \"superbe\", \"fantastique\" forment un cluster distinct</li> <li>Mots n\u00e9gatifs : Les embeddings de mots comme \"terrible\", \"horrible\", \"d\u00e9cevant\" forment un autre cluster</li> </ol>"},{"location":"module2/ressources/lstm-sentiment-analyse/#visualisation-des-embeddings","title":"Visualisation des embeddings","text":"<p>Pour visualiser ces diff\u00e9rences, on utilise souvent des techniques de r\u00e9duction de dimensionnalit\u00e9 comme t-SNE ou PCA :</p> <pre><code># Visualisation des embeddings avec t-SNE\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\n# R\u00e9cup\u00e9rer la matrice d'embedding du mod\u00e8le entra\u00een\u00e9\nembedding_matrix = model.layers[0].get_weights()[0]\n\n# S\u00e9lectionner des mots sp\u00e9cifiques pour la visualisation\nwords_to_plot = [\"good\", \"excellent\", \"amazing\", \"bad\", \"terrible\", \"awful\"]\nword_indices = [vocab[word] for word in words_to_plot if word in vocab]\nword_vectors = embedding_matrix[word_indices]\n\n# R\u00e9duction de dimension avec t-SNE\ntsne = TSNE(n_components=2, random_state=42)\nword_vectors_2d = tsne.fit_transform(word_vectors)\n\n# Tracer les points\nplt.figure(figsize=(10, 8))\nfor i, word in enumerate([words_to_plot[vocab.get(i, 0)] for i in word_indices]):\n    x, y = word_vectors_2d[i]\n    plt.scatter(x, y)\n    plt.annotate(word, (x, y), fontsize=12)\nplt.title(\"Visualisation des embeddings de mots\")\nplt.show()\n</code></pre>"},{"location":"module2/ressources/lstm-sentiment-analyse/#caracteristiques-observables","title":"Caract\u00e9ristiques observables","text":"<p>Dans une visualisation r\u00e9ussie, vous devriez observer : - Des groupements clairs de mots positifs et n\u00e9gatifs - Des distances plus courtes entre mots de m\u00eame polarit\u00e9 - Des vecteurs qui capturent plus que la simple polarit\u00e9 (ex: intensit\u00e9, domaine, etc.)</p>"},{"location":"module2/ressources/lstm-sentiment-analyse/#4-comprehension-du-contexte-par-le-modele-lstm","title":"4. Compr\u00e9hension du contexte par le mod\u00e8le LSTM","text":""},{"location":"module2/ressources/lstm-sentiment-analyse/#mecanisme-de-comprehension-contextuelle","title":"M\u00e9canisme de compr\u00e9hension contextuelle","text":"<p>Les LSTM comprennent le contexte d'une phrase de plusieurs fa\u00e7ons :</p> <ol> <li> <p>S\u00e9quentialit\u00e9 : Le mod\u00e8le traite les mots dans l'ordre, permettant de capturer leur relation s\u00e9quentielle    <pre><code>\"Ce film n'est pas mauvais\" \u2192 Le LSTM peut comprendre que \"pas mauvais\" est positif\n</code></pre></p> </li> <li> <p>M\u00e9moire s\u00e9lective : Capacit\u00e9 \u00e0 retenir les informations importantes et oublier les d\u00e9tails non pertinents    <pre><code>\"Malgr\u00e9 quelques d\u00e9fauts mineurs, le film \u00e9tait globalement excellent\"\n\u2192 Le LSTM peut se concentrer sur \"globalement excellent\" plut\u00f4t que sur \"d\u00e9fauts mineurs\"\n</code></pre></p> </li> <li> <p>Repr\u00e9sentation bidirectionnelle : Les LSTM bidirectionnels (Bi-LSTM) lisent la s\u00e9quence dans les deux sens    <pre><code># Impl\u00e9mentation d'un Bi-LSTM\nfrom tensorflow.keras.layers import Bidirectional, LSTM\n\nbidirectional_lstm = Bidirectional(LSTM(units=64, return_sequences=True))\n</code></pre></p> </li> </ol>"},{"location":"module2/ressources/lstm-sentiment-analyse/#exemple-de-traitement-contextuel","title":"Exemple de traitement contextuel","text":"<p>Prenons l'exemple de la phrase \"Ce n'est pas un bon film, c'est un chef-d'\u0153uvre !\" :</p> <ol> <li>Le mod\u00e8le lit s\u00e9quentiellement chaque mot</li> <li>\u00c0 \"pas un bon\", il capture la n\u00e9gation</li> <li>\u00c0 \"chef-d'\u0153uvre\", il comprend le contraste avec la premi\u00e8re partie</li> <li>L'\u00e9tat final de la cellule contient une repr\u00e9sentation positive</li> </ol>"},{"location":"module2/ressources/lstm-sentiment-analyse/#5-limitations-de-lapproche-lstm-pour-lanalyse-de-sentiment","title":"5. Limitations de l'approche LSTM pour l'analyse de sentiment","text":""},{"location":"module2/ressources/lstm-sentiment-analyse/#defis-inherents","title":"D\u00e9fis inh\u00e9rents","text":"<ol> <li> <p>Sarcasme et ironie : Les LSTM peinent \u00e0 d\u00e9tecter les nuances subtiles    <pre><code>\"Quelle performance incroyable ! Je n'ai jamais autant dormi au cin\u00e9ma.\"\n</code></pre></p> </li> <li> <p>Contexte culturel : Les mod\u00e8les manquent souvent de connaissances contextuelles    <pre><code>\"Ce film est tellement mauvais qu'il en devient culte.\"\n</code></pre></p> </li> <li> <p>Expressions idiomatiques : Difficult\u00e9 \u00e0 comprendre les expressions non litt\u00e9rales    <pre><code>\"Le r\u00e9alisateur s'est vraiment cass\u00e9 la t\u00eate pour ce sc\u00e9nario.\"\n</code></pre></p> </li> <li> <p>Besoin de donn\u00e9es d'entra\u00eenement importantes : Les mod\u00e8les performants n\u00e9cessitent de grands corpus annot\u00e9s</p> </li> </ol>"},{"location":"module2/ressources/lstm-sentiment-analyse/#contraintes-techniques","title":"Contraintes techniques","text":"<ol> <li> <p>Probl\u00e8me du gradient qui s'\u00e9vanouit : M\u00eame les LSTM peuvent avoir du mal avec des s\u00e9quences tr\u00e8s longues</p> </li> <li> <p>Temps d'entra\u00eenement : L'entra\u00eenement est s\u00e9quentiel et difficile \u00e0 parall\u00e9liser</p> </li> <li> <p>Complexit\u00e9 computationnelle : Les LSTM sont plus lourds que des approches plus simples (Bag-of-Words, TF-IDF)</p> </li> </ol>"},{"location":"module2/ressources/lstm-sentiment-analyse/#6-pistes-damelioration-pour-lanalyse-de-sentiment","title":"6. Pistes d'am\u00e9lioration pour l'analyse de sentiment","text":""},{"location":"module2/ressources/lstm-sentiment-analyse/#ameliorations-architecturales","title":"Am\u00e9liorations architecturales","text":"<ol> <li> <p>Bi-LSTM avec attention : Ajout d'un m\u00e9canisme d'attention pour se concentrer sur les mots importants    <pre><code># Exemple simplifi\u00e9 de m\u00e9canisme d'attention\nfrom tensorflow.keras.layers import Dense, Concatenate\n\n# Output de Bi-LSTM avec return_sequences=True\nlstm_output = ... # shape=(batch_size, sequence_length, lstm_units*2)\n\n# Couche d'attention\nattention = Dense(1, activation='tanh')(lstm_output)\nattention_weights = tf.nn.softmax(attention, axis=1)\ncontext_vector = tf.reduce_sum(lstm_output * attention_weights, axis=1)\n</code></pre></p> </li> <li> <p>Embeddings contextuels : Utiliser des embeddings pr\u00e9-entra\u00een\u00e9s comme GloVe, Word2Vec ou BERT    <pre><code># Chargement d'embeddings GloVe pr\u00e9-entra\u00een\u00e9s\nimport numpy as np\n\nembeddings_index = {}\nwith open('glove.6B.100d.txt', encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n</code></pre></p> </li> <li> <p>Architectures hybrides : Combiner CNN et LSTM pour extraire des motifs \u00e0 diff\u00e9rentes \u00e9chelles    <pre><code>from tensorflow.keras.layers import Conv1D, MaxPooling1D\n\n# Exemple d'architecture hybride CNN-LSTM\nmodel = Sequential([\n    Embedding(vocab_size, embedding_dim, input_length=max_sequence_length),\n    Conv1D(filters=128, kernel_size=5, activation='relu'),\n    MaxPooling1D(pool_size=2),\n    LSTM(units=64),\n    Dense(1, activation='sigmoid')\n])\n</code></pre></p> </li> </ol>"},{"location":"module2/ressources/lstm-sentiment-analyse/#ameliorations-des-donnees-et-du-pretraitement","title":"Am\u00e9liorations des donn\u00e9es et du pr\u00e9traitement","text":"<ol> <li> <p>Augmentation de donn\u00e9es : Cr\u00e9ation d'exemples suppl\u00e9mentaires par synonymisation, back-translation, etc.</p> </li> <li> <p>Traitement sp\u00e9cifique : Gestion explicite des n\u00e9gations, des intensificateurs, etc.    <pre><code># Exemple de d\u00e9tection de n\u00e9gation\ndef mark_negations(text):\n    negation_words = ['not', 'no', 'never', 'neither', 'nor', 'none']\n    words = text.split()\n    for i, word in enumerate(words):\n        if word in negation_words and i &lt; len(words) - 1:\n            # Marquer le mot suivant une n\u00e9gation\n            words[i+1] = 'NEG_' + words[i+1]\n    return ' '.join(words)\n</code></pre></p> </li> <li> <p>Fine-tuning sur un domaine sp\u00e9cifique : Adaptation \u00e0 un vocabulaire particulier (critique de films, avis produits, etc.)</p> </li> </ol>"},{"location":"module2/ressources/lstm-sentiment-analyse/#conclusion","title":"Conclusion","text":"<p>L'analyse de sentiment avec LSTM offre des capacit\u00e9s puissantes pour comprendre le sentiment exprim\u00e9 dans un texte. Bien que cette approche pr\u00e9sente certaines limitations, elle constitue une base solide qui peut \u00eatre am\u00e9lior\u00e9e par diverses techniques. La compr\u00e9hension approfondie de ces points cl\u00e9s vous permettra de d\u00e9velopper des mod\u00e8les plus performants et mieux adapt\u00e9s \u00e0 vos besoins sp\u00e9cifiques.</p>"},{"location":"module2/ressources/lstm-sentiment-analyse/#ressources-complementaires","title":"Ressources compl\u00e9mentaires","text":"<ul> <li>Comprendre les LSTM</li> <li>Tutoriel Keras sur les LSTM</li> <li>Word2Vec: Efficient Estimation of Word Representations in Vector Space</li> <li>GloVe: Global Vectors for Word Representation</li> </ul>"},{"location":"module2/ressources/mini-projet-cnn-web-colab/","title":"Mini projet cnn web colab","text":""},{"location":"module2/ressources/mini-projet-cnn-web-colab/#mini-projet-reconnaissance-de-chiffres-manuscrits-avec-interface-web-dans-colab","title":"\ud83d\ude80Mini-projet : Reconnaissance de chiffres manuscrits avec interface web dans Colab","text":""},{"location":"module2/ressources/mini-projet-cnn-web-colab/#contexte-professionnel","title":"\ud83d\udccbContexte professionnel","text":"<p>Vous \u00eates stagiaire dans une PME o\u00f9 les employ\u00e9s doivent r\u00e9guli\u00e8rement saisir manuellement des codes \u00e0 partir de documents papier (bons de commande, formulaires clients, etc.). Votre responsable informatique souhaite explorer des solutions d'automatisation et vous demande de tester une application de reconnaissance de chiffres manuscrits.</p>"},{"location":"module2/ressources/mini-projet-cnn-web-colab/#etape-1-configuration-de-lenvironnement-colab-5-minutes","title":"\u2699\ufe0f\u00c9tape 1: Configuration de l'environnement Colab (5 minutes)","text":"<p>Cr\u00e9ez un nouveau notebook Google Colab et ex\u00e9cutez les cellules suivantes:</p> <pre><code># Installation des biblioth\u00e8ques n\u00e9cessaires\n!pip install -q pyngrok\n!pip install -q flask-ngrok\n\n# Importation des biblioth\u00e8ques\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image, ImageOps\nimport io\nimport base64\nfrom flask import Flask, request, jsonify, render_template_string\nfrom flask_ngrok import run_with_ngrok\nimport os\nfrom google.colab import files\nfrom pyngrok import ngrok\n\nprint(\"Configuration termin\u00e9e!\")\n</code></pre>"},{"location":"module2/ressources/mini-projet-cnn-web-colab/#etape-2-creation-et-entrainement-du-modele-cnn-8-minutes","title":"\ud83e\udde0\u00c9tape 2: Cr\u00e9ation et entra\u00eenement du mod\u00e8le CNN (8 minutes)","text":"<pre><code># Chargement du dataset MNIST\n(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n\n# Pr\u00e9traitement des donn\u00e9es\nX_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32') / 255.0\nX_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32') / 255.0\ny_train = tf.keras.utils.to_categorical(y_train, 10)\ny_test = tf.keras.utils.to_categorical(y_test, 10)\n\n# Cr\u00e9ation du mod\u00e8le CNN\nmodel = Sequential([\n    # Premi\u00e8re couche de convolution\n    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n    MaxPooling2D(pool_size=(2, 2)),\n\n    # Deuxi\u00e8me couche de convolution\n    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n    MaxPooling2D(pool_size=(2, 2)),\n\n    # Aplatissement et couches denses\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dense(10, activation='softmax')\n])\n\n# Compilation du mod\u00e8le\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Affichage de l'architecture\nmodel.summary()\n\n# Entra\u00eenement rapide pour la d\u00e9monstration\nprint(\"Entra\u00eenement du mod\u00e8le...\")\nmodel.fit(X_train[:10000], y_train[:10000], \n          batch_size=128,\n          epochs=3,\n          validation_split=0.1,\n          verbose=1)\n\n# \u00c9valuation sur l'ensemble de test\ntest_loss, test_acc = model.evaluate(X_test, y_test)\nprint(f\"\\nPr\u00e9cision sur le test: {test_acc*100:.2f}%\")\n\n# Sauvegarde du mod\u00e8le\nmodel.save('mnist_cnn_model.h5')\nprint(\"Mod\u00e8le entra\u00een\u00e9 et sauvegard\u00e9!\")\n</code></pre>"},{"location":"module2/ressources/mini-projet-cnn-web-colab/#etape-3-creation-de-lapplication-web-flask-7-minutes","title":"\ud83c\udf10\u00c9tape 3: Cr\u00e9ation de l'application web Flask (7 minutes)","text":"<pre><code># D\u00e9finition du template HTML pour l'application web\nhtml_template = \"\"\"\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"fr\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;Reconnaissance de chiffres manuscrits&lt;/title&gt;\n    &lt;style&gt;\n        body {\n            font-family: Arial, sans-serif;\n            max-width: 800px;\n            margin: 0 auto;\n            padding: 20px;\n            background-color: #f5f7fa;\n        }\n        .container {\n            display: flex;\n            flex-direction: column;\n            align-items: center;\n        }\n        .canvas-container {\n            margin: 20px 0;\n            position: relative;\n        }\n        #canvas {\n            border: 2px solid #333;\n            border-radius: 5px;\n            background-color: #fff;\n            cursor: crosshair;\n        }\n        .controls {\n            display: flex;\n            gap: 10px;\n            margin-bottom: 20px;\n        }\n        button {\n            padding: 8px 16px;\n            background-color: #4CAF50;\n            color: white;\n            border: none;\n            border-radius: 4px;\n            cursor: pointer;\n            font-size: 16px;\n        }\n        button:hover {\n            background-color: #45a049;\n        }\n        .clear-btn {\n            background-color: #f44336;\n        }\n        .clear-btn:hover {\n            background-color: #d32f2f;\n        }\n        .result-container {\n            width: 100%;\n            margin-top: 20px;\n            display: flex;\n            flex-direction: column;\n            align-items: center;\n        }\n        #result {\n            font-size: 18px;\n            margin-bottom: 15px;\n        }\n        .confidence-bar {\n            width: 100%;\n            max-width: 400px;\n            height: 30px;\n            background-color: #ddd;\n            border-radius: 5px;\n            margin-bottom: 10px;\n            overflow: hidden;\n        }\n        .confidence-level {\n            height: 100%;\n            background-color: #4CAF50;\n            text-align: center;\n            color: white;\n            line-height: 30px;\n        }\n        #feature-maps {\n            display: flex;\n            flex-wrap: wrap;\n            gap: 5px;\n            justify-content: center;\n            margin-top: 20px;\n        }\n        .feature-map {\n            border: 1px solid #ddd;\n            background-color: #fff;\n        }\n        .viz-checkbox {\n            margin: 10px 0;\n        }\n        .file-upload {\n            margin: 15px 0;\n        }\n        .tab-container {\n            width: 100%;\n            margin-bottom: 20px;\n        }\n        .tabs {\n            display: flex;\n            margin-bottom: 10px;\n        }\n        .tab {\n            padding: 10px 15px;\n            background-color: #e0e0e0;\n            border: 1px solid #ccc;\n            border-radius: 5px 5px 0 0;\n            cursor: pointer;\n            margin-right: 5px;\n        }\n        .tab.active {\n            background-color: #fff;\n            border-bottom: 1px solid #fff;\n        }\n        .tab-content {\n            display: none;\n            padding: 15px;\n            border: 1px solid #ccc;\n            border-radius: 0 5px 5px 5px;\n            background-color: #fff;\n        }\n        .tab-content.active {\n            display: block;\n        }\n    &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;div class=\"container\"&gt;\n        &lt;h1&gt;Reconnaissance de chiffres manuscrits&lt;/h1&gt;\n        &lt;p&gt;Dessinez un chiffre (0-9) ci-dessous ou importez une image&lt;/p&gt;\n\n        &lt;div class=\"tab-container\"&gt;\n            &lt;div class=\"tabs\"&gt;\n                &lt;div class=\"tab active\" onclick=\"openTab(event, 'draw-tab')\"&gt;Dessiner&lt;/div&gt;\n                &lt;div class=\"tab\" onclick=\"openTab(event, 'upload-tab')\"&gt;Importer une image&lt;/div&gt;\n            &lt;/div&gt;\n\n            &lt;div id=\"draw-tab\" class=\"tab-content active\"&gt;\n                &lt;div class=\"canvas-container\"&gt;\n                    &lt;canvas id=\"canvas\" width=\"280\" height=\"280\"&gt;&lt;/canvas&gt;\n                &lt;/div&gt;\n\n                &lt;div class=\"controls\"&gt;\n                    &lt;button id=\"predict-btn\"&gt;Pr\u00e9dire&lt;/button&gt;\n                    &lt;button id=\"clear-btn\" class=\"clear-btn\"&gt;Effacer&lt;/button&gt;\n                &lt;/div&gt;\n            &lt;/div&gt;\n\n            &lt;div id=\"upload-tab\" class=\"tab-content\"&gt;\n                &lt;div class=\"file-upload\"&gt;\n                    &lt;input type=\"file\" id=\"image-upload\" accept=\"image/*\"&gt;\n                &lt;/div&gt;\n                &lt;div id=\"preview-container\" style=\"display: none;\"&gt;\n                    &lt;h3&gt;Aper\u00e7u de l'image:&lt;/h3&gt;\n                    &lt;img id=\"preview-image\" style=\"max-width: 280px; max-height: 280px;\"&gt;\n                    &lt;div class=\"controls\" style=\"margin-top: 10px;\"&gt;\n                        &lt;button id=\"predict-upload-btn\"&gt;Pr\u00e9dire&lt;/button&gt;\n                    &lt;/div&gt;\n                &lt;/div&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n\n        &lt;div class=\"viz-checkbox\"&gt;\n            &lt;input type=\"checkbox\" id=\"show-features\" name=\"show-features\"&gt;\n            &lt;label for=\"show-features\"&gt;Visualiser les feature maps&lt;/label&gt;\n        &lt;/div&gt;\n\n        &lt;div class=\"result-container\" id=\"result-container\" style=\"display: none;\"&gt;\n            &lt;h2&gt;R\u00e9sultat:&lt;/h2&gt;\n            &lt;div id=\"result\"&gt;Pr\u00e9diction en attente...&lt;/div&gt;\n\n            &lt;div class=\"confidence-bar\"&gt;\n                &lt;div class=\"confidence-level\" id=\"confidence-level\"&gt;&lt;/div&gt;\n            &lt;/div&gt;\n\n            &lt;div id=\"feature-maps-container\"&gt;\n                &lt;h3&gt;Feature Maps:&lt;/h3&gt;\n                &lt;div id=\"feature-maps\"&gt;&lt;/div&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n\n    &lt;script&gt;\n        // Configuration du canvas pour le dessin\n        const canvas = document.getElementById('canvas');\n        const ctx = canvas.getContext('2d');\n        ctx.lineWidth = 15;\n        ctx.lineCap = 'round';\n        ctx.lineJoin = 'round';\n        ctx.strokeStyle = 'black';\n\n        // Variables pour le dessin\n        let isDrawing = false;\n        let lastX = 0;\n        let lastY = 0;\n\n        // Remplir le fond en blanc\n        ctx.fillStyle = 'white';\n        ctx.fillRect(0, 0, canvas.width, canvas.height);\n\n        // \u00c9couteurs d'\u00e9v\u00e9nements pour le dessin\n        canvas.addEventListener('mousedown', startDrawing);\n        canvas.addEventListener('mousemove', draw);\n        canvas.addEventListener('mouseup', stopDrawing);\n        canvas.addEventListener('mouseout', stopDrawing);\n\n        // \u00c9couteurs tactiles pour les appareils mobiles\n        canvas.addEventListener('touchstart', handleTouchStart);\n        canvas.addEventListener('touchmove', handleTouchMove);\n        canvas.addEventListener('touchend', stopDrawing);\n\n        function handleTouchStart(e) {\n            e.preventDefault();\n            const touch = e.touches[0];\n            const rect = canvas.getBoundingClientRect();\n            lastX = touch.clientX - rect.left;\n            lastY = touch.clientY - rect.top;\n            isDrawing = true;\n        }\n\n        function handleTouchMove(e) {\n            e.preventDefault();\n            if (!isDrawing) return;\n            const touch = e.touches[0];\n            const rect = canvas.getBoundingClientRect();\n            const x = touch.clientX - rect.left;\n            const y = touch.clientY - rect.top;\n\n            ctx.beginPath();\n            ctx.moveTo(lastX, lastY);\n            ctx.lineTo(x, y);\n            ctx.stroke();\n            lastX = x;\n            lastY = y;\n        }\n\n        function startDrawing(e) {\n            isDrawing = true;\n            const rect = canvas.getBoundingClientRect();\n            [lastX, lastY] = [e.clientX - rect.left, e.clientY - rect.top];\n        }\n\n        function draw(e) {\n            if (!isDrawing) return;\n            const rect = canvas.getBoundingClientRect();\n            const x = e.clientX - rect.left;\n            const y = e.clientY - rect.top;\n\n            ctx.beginPath();\n            ctx.moveTo(lastX, lastY);\n            ctx.lineTo(x, y);\n            ctx.stroke();\n            [lastX, lastY] = [x, y];\n        }\n\n        function stopDrawing() {\n            isDrawing = false;\n        }\n\n        // Fonction pour effacer le canvas\n        document.getElementById('clear-btn').addEventListener('click', clearCanvas);\n\n        function clearCanvas() {\n            ctx.fillStyle = 'white';\n            ctx.fillRect(0, 0, canvas.width, canvas.height);\n            hideResult();\n        }\n\n        // Fonction pour pr\u00e9dire \u00e0 partir du dessin\n        document.getElementById('predict-btn').addEventListener('click', () =&gt; {\n            predictDrawing();\n        });\n\n        function predictDrawing() {\n            const imageData = canvas.toDataURL('image/png').split(',')[1];\n            predict(imageData);\n        }\n\n        // Gestion de l'upload d'image\n        document.getElementById('image-upload').addEventListener('change', function() {\n            const file = this.files[0];\n            if (file) {\n                const reader = new FileReader();\n                reader.onload = function(e) {\n                    document.getElementById('preview-image').src = e.target.result;\n                    document.getElementById('preview-container').style.display = 'block';\n                }\n                reader.readAsDataURL(file);\n            }\n        });\n\n        document.getElementById('predict-upload-btn').addEventListener('click', function() {\n            const imageData = document.getElementById('preview-image').src.split(',')[1];\n            predict(imageData);\n        });\n\n        // Fonction pour pr\u00e9dire\n        function predict(imageData) {\n            const showFeatures = document.getElementById('show-features').checked;\n\n            fetch('/predict', {\n                method: 'POST',\n                headers: {\n                    'Content-Type': 'application/json'\n                },\n                body: JSON.stringify({\n                    image: imageData,\n                    show_features: showFeatures\n                })\n            })\n            .then(response =&gt; response.json())\n            .then(data =&gt; {\n                displayResult(data);\n            })\n            .catch(error =&gt; {\n                console.error('Erreur:', error);\n                alert('Une erreur est survenue lors de la pr\u00e9diction.');\n            });\n        }\n\n        // Fonction pour afficher le r\u00e9sultat\n        function displayResult(data) {\n            document.getElementById('result-container').style.display = 'block';\n            document.getElementById('result').textContent = `Pr\u00e9dit: ${data.prediction} (${data.confidence.toFixed(2)}%)`;\n\n            const confidenceLevel = document.getElementById('confidence-level');\n            confidenceLevel.style.width = `${data.confidence}%`;\n            confidenceLevel.textContent = `${data.confidence.toFixed(2)}%`;\n\n            // Affichage des feature maps si demand\u00e9\n            const featureMapsContainer = document.getElementById('feature-maps-container');\n            const featureMapsDiv = document.getElementById('feature-maps');\n\n            if (data.feature_maps &amp;&amp; data.feature_maps.length &gt; 0) {\n                featureMapsContainer.style.display = 'block';\n                featureMapsDiv.innerHTML = '';\n\n                data.feature_maps.forEach(mapData =&gt; {\n                    const img = document.createElement('img');\n                    img.src = 'data:image/png;base64,' + mapData;\n                    img.className = 'feature-map';\n                    img.width = 100;\n                    img.height = 100;\n                    featureMapsDiv.appendChild(img);\n                });\n            } else {\n                featureMapsContainer.style.display = 'none';\n            }\n        }\n\n        function hideResult() {\n            document.getElementById('result-container').style.display = 'none';\n        }\n\n        // Fonction pour changer d'onglet\n        function openTab(evt, tabName) {\n            const tabContents = document.getElementsByClassName('tab-content');\n            for (let i = 0; i &lt; tabContents.length; i++) {\n                tabContents[i].classList.remove('active');\n            }\n\n            const tabs = document.getElementsByClassName('tab');\n            for (let i = 0; i &lt; tabs.length; i++) {\n                tabs[i].classList.remove('active');\n            }\n\n            document.getElementById(tabName).classList.add('active');\n            evt.currentTarget.classList.add('active');\n\n            hideResult();\n        }\n    &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\"\"\"\n\n# Cr\u00e9ation de l'application Flask\napp = Flask(__name__)\nrun_with_ngrok(app)  # Int\u00e9gration avec ngrok pour l'acc\u00e8s public\n\n# Chargement du mod\u00e8le\nmodel = load_model('mnist_cnn_model.h5')\n\n@app.route('/')\ndef index():\n    return render_template_string(html_template)\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    data = request.get_json()\n    image_data = data['image']\n    show_features = data.get('show_features', False)\n\n    # D\u00e9codage de l'image\n    image_bytes = base64.b64decode(image_data)\n    img = Image.open(io.BytesIO(image_bytes)).convert('L')\n\n    # Redimensionnement et pr\u00e9traitement\n    img = img.resize((28, 28))\n    img = ImageOps.invert(img)  # Inverser les couleurs si besoin\n\n    # Conversion en array et normalisation\n    img_array = np.array(img).astype('float32') / 255.0\n    img_array = img_array.reshape(1, 28, 28, 1)\n\n    # Pr\u00e9diction\n    predictions = model.predict(img_array)[0]\n    predicted_class = np.argmax(predictions)\n    confidence = float(predictions[predicted_class] * 100)\n\n    # Pr\u00e9paration de la r\u00e9ponse\n    response = {\n        'prediction': int(predicted_class),\n        'confidence': confidence\n    }\n\n    # G\u00e9n\u00e9ration des feature maps si demand\u00e9\n    if show_features:\n        feature_maps = []\n\n        # Cr\u00e9ation d'un mod\u00e8le pour extraire les feature maps de la premi\u00e8re couche de convolution\n        feature_model = tf.keras.Model(\n            inputs=model.inputs,\n            outputs=model.layers[0].output\n        )\n\n        # Obtention des feature maps\n        feature_outputs = feature_model.predict(img_array)\n\n        # Conversion des feature maps en images\n        for i in range(min(8, feature_outputs.shape[3])):  # Limiter \u00e0 8 pour plus de clart\u00e9\n            feature_map = feature_outputs[0, :, :, i]\n\n            # Normalisation pour visualisation\n            feature_map = (feature_map - feature_map.min()) / (feature_map.max() - feature_map.min() + 1e-9)\n\n            # Conversion en image\n            plt.figure(figsize=(1, 1))\n            plt.imshow(feature_map, cmap='viridis')\n            plt.axis('off')\n\n            # Sauvegarde en buffer puis conversion en base64\n            buf = io.BytesIO()\n            plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0)\n            buf.seek(0)\n            img_str = base64.b64encode(buf.read()).decode('utf-8')\n            feature_maps.append(img_str)\n            plt.close()\n\n        response['feature_maps'] = feature_maps\n\n    return jsonify(response)\n\n# Configuration et lancement de l'application Flask\ndef run_app():\n    app.run()\n\nprint(\"Configuration de l'application termin\u00e9e!\")\n</code></pre>"},{"location":"module2/ressources/mini-projet-cnn-web-colab/#etape-4-lancement-de-lapplication-web-2-minutes","title":"\ud83d\ude80\u00c9tape 4: Lancement de l'application web (2 minutes)","text":"<pre><code># Configurer et lancer ngrok avec Colab\n# D\u00e9finir un token ngrok (optionnel mais recommand\u00e9 pour \u00e9viter les limitations)\n# Vous pouvez cr\u00e9er un compte gratuit sur ngrok.com pour obtenir un token\nngrok_token = input(\"Entrez votre token ngrok (optionnel, appuyez sur Entr\u00e9e pour ignorer): \")\nif ngrok_token:\n    !ngrok authtoken {ngrok_token}\n\n# Lancer l'application Flask avec ngrok\nprint(\"Lancement de l'application web...\")\nprint(\"Cela peut prendre quelques secondes...\")\napp.run()\n</code></pre>"},{"location":"module2/ressources/mini-projet-cnn-web-colab/#etape-5-tests-pratiques-15-minutes","title":"\ud83e\uddea\u00c9tape 5: Tests pratiques (15 minutes)","text":"<p>Une fois l'application lanc\u00e9e, vous obtiendrez un lien ngrok (https://xxxx.ngrok.io) que vous pourrez ouvrir dans un nouvel onglet de votre navigateur.</p> <ol> <li> <p>Test avec dessins \u00e0 la souris    . Dans l'interface web, dessinez clairement un chiffre (de 0 \u00e0 9) dans la zone pr\u00e9vue    . Cliquez sur le bouton \"Pr\u00e9dire\"    . Notez la pr\u00e9diction et le niveau de confiance    . R\u00e9p\u00e9tez ce processus avec 5 chiffres diff\u00e9rents    . Remplissez le tableau des r\u00e9sultats dans la section d'\u00e9valuation</p> </li> <li> <p>Test avec image import\u00e9e    . Pr\u00e9parez une image de chiffre manuscrit (vous pouvez l'\u00e9crire sur papier et prendre une photo)    . Cliquez sur l'onglet \"Importer une image\"    . S\u00e9lectionnez votre image    . Cliquez sur \"Pr\u00e9dire\" et notez les r\u00e9sultats</p> </li> <li> <p>Test avec feature maps    . Cochez la case \"Visualiser les feature maps\"    . Dessinez un nouveau chiffre et cliquez sur \"Pr\u00e9dire\"    . Observez les feature maps qui s'affichent</p> </li> </ol>"},{"location":"module2/ressources/mini-projet-cnn-web-colab/#etape-6-evaluation-et-documentation-10-minutes","title":"\ud83d\udcca\u00c9tape 6: \u00c9valuation et documentation (10 minutes)","text":"<p>\u00c0 l'aide de cette cellule, cr\u00e9ez un tableau pour consigner vos r\u00e9sultats:</p> <pre><code># Cr\u00e9ation d'un tableau pour documenter les r\u00e9sultats\nfrom IPython.display import Markdown, display\nimport pandas as pd\n\n# Afficher un tableau pour les r\u00e9sultats\ndata = {\n    'Chiffre dessin\u00e9': [],\n    'Pr\u00e9diction': [],\n    'Confiance (%)': [],\n    'Correct (Oui/Non)': []\n}\n\n# Ajouter les r\u00e9sultats observ\u00e9s pendant vos tests (exemple)\n# Remplacez par vos propres donn\u00e9es\ndata['Chiffre dessin\u00e9'] = [5, 7, 3, 9, 0]  # Exemple, remplacez par vos tests\ndata['Pr\u00e9diction'] = [5, 7, 8, 9, 0]       # Exemple, remplacez par vos r\u00e9sultats\ndata['Confiance (%)'] = [98.2, 96.5, 74.3, 88.1, 99.0]  # Exemple\ndata['Correct (Oui/Non)'] = ['Oui', 'Oui', 'Non', 'Oui', 'Oui']  # Exemple\n\n# Cr\u00e9ation et affichage du tableau\nresults_df = pd.DataFrame(data)\ndisplay(results_df)\n\n# Calcul et affichage des statistiques\ncorrect_count = results_df['Correct (Oui/Non)'].value_counts().get('Oui', 0)\ntotal_count = len(results_df)\naccuracy = (correct_count / total_count) * 100 if total_count &gt; 0 else 0\n\ndisplay(Markdown(f\"\"\"\n### R\u00e9sum\u00e9 des tests\n- **Nombre total de tests:** {total_count}\n- **Pr\u00e9dictions correctes:** {correct_count}\n- **Taux de r\u00e9ussite:** {accuracy:.2f}%\n- **Confiance moyenne:** {results_df['Confiance (%)'].mean():.2f}%\n\"\"\"))\n\n# Les chiffres les mieux reconnus et les plus difficiles\ndisplay(Markdown(\"\"\"\n### Observations\n- **Chiffres les mieux reconnus:** [\u00c0 compl\u00e9ter]\n- **Chiffres les plus difficiles:** [\u00c0 compl\u00e9ter]\n- **Niveau de confiance moyen:** [\u00c0 compl\u00e9ter]\n\"\"\"))\n\n# Analyse critique\ndisplay(Markdown(\"\"\"\n### Points forts et limitations\n\n**Points forts:**\n1. [\u00c0 compl\u00e9ter]\n2. [\u00c0 compl\u00e9ter]\n3. [\u00c0 compl\u00e9ter]\n\n**Limitations:**\n1. [\u00c0 compl\u00e9ter]\n2. [\u00c0 compl\u00e9ter]\n3. [\u00c0 compl\u00e9ter]\n\"\"\"))\n\n# Propositions d'am\u00e9lioration\ndisplay(Markdown(\"\"\"\n### Propositions d'am\u00e9lioration\n1. [\u00c0 compl\u00e9ter]\n2. [\u00c0 compl\u00e9ter]\n3. [\u00c0 compl\u00e9ter]\n\"\"\"))\n</code></pre>"},{"location":"module2/ressources/mini-projet-cnn-web-colab/#livrable-a-rendre","title":"\ud83d\udcddLivrable \u00e0 rendre","text":"<p>\u00c0 la fin de la session, cr\u00e9ez une copie de votre notebook Colab et partagez-le avec votre formateur. Assurez-vous d'avoir rempli toutes les sections d'\u00e9valuation avec vos observations.</p>"},{"location":"module2/ressources/mini-projet-cnn-web-colab/#pour-aller-plus-loin-si-vous-terminez-en-avance","title":"Pour aller plus loin (si vous terminez en avance)","text":"<p>Si vous avez termin\u00e9 avant la fin du temps imparti, explorez ces pistes : . Comment utiliser d'autres architectures CNN (VGG16, MobileNet) via TensorFlow Hub . Comment am\u00e9liorer la robustesse du mod\u00e8le avec l'augmentation de donn\u00e9es . Comment optimiser le mod\u00e8le pour des performances plus rapides ```</p>"},{"location":"module2/ressources/rnn-sequence/","title":"Rnn sequence","text":"In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! <pre>{\n  \"cells\": [\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"# RNN/LSTM pour l'analyse de sentiment\\n\",\n        \"\\n\",\n        \"##  S\u00e9ance 2: Types de r\u00e9seaux de neurones\\n\",\n        \"\\n\",\n        \"Ce notebook vous guidera \u00e0 travers l'impl\u00e9mentation d'un mod\u00e8le LSTM (Long Short-Term Memory) pour l'analyse de sentiment. Vous d\u00e9couvrirez comment les r\u00e9seaux r\u00e9currents peuvent \u00eatre utilis\u00e9s pour comprendre et classifier du texte.\\n\",\n        \"\\n\",\n        \"### Objectifs d'apprentissage:\\n\",\n        \"- Comprendre le pr\u00e9traitement du texte pour les mod\u00e8les de Deep Learning\\n\",\n        \"- D\u00e9couvrir l'architecture et le fonctionnement des r\u00e9seaux LSTM\\n\",\n        \"- Apprendre \u00e0 \u00e9valuer un mod\u00e8le d'analyse de sentiment\\n\",\n        \"- Visualiser et interpr\u00e9ter les embeddings de mots\\n\",\n        \"\\n\",\n        \"### Pr\u00e9requis:\\n\",\n        \"- Connaissances de base en Python\\n\",\n        \"- Notions fondamentales de r\u00e9seaux de neurones\\n\",\n        \"- Avoir suivi la s\u00e9ance 1 d'introduction au Deep Learning\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 1. Configuration de l'environnement\\n\",\n        \"\\n\",\n        \"Commen\u00e7ons par importer les biblioth\u00e8ques n\u00e9cessaires et configurer notre environnement.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"import numpy as np\\n\",\n        \"import matplotlib.pyplot as plt\\n\",\n        \"import tensorflow as tf\\n\",\n        \"from tensorflow.keras.models import Sequential\\n\",\n        \"from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\\n\",\n        \"from tensorflow.keras.preprocessing.text import Tokenizer\\n\",\n        \"from tensorflow.keras.preprocessing.sequence import pad_sequences\\n\",\n        \"from tensorflow.keras.callbacks import EarlyStopping\\n\",\n        \"import pandas as pd\\n\",\n        \"import re\\n\",\n        \"import time\\n\",\n        \"import seaborn as sns\\n\",\n        \"from sklearn.metrics import confusion_matrix, classification_report\\n\",\n        \"\\n\",\n        \"# Configuration pour reproductibilit\u00e9\\n\",\n        \"np.random.seed(42)\\n\",\n        \"tf.random.set_seed(42)\\n\",\n        \"\\n\",\n        \"# V\u00e9rifier la version de TensorFlow\\n\",\n        \"print(f\\\"TensorFlow version: {tf.__version__}\\\")\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 2. Pr\u00e9paration des donn\u00e9es\\n\",\n        \"\\n\",\n        \"Pour ce TP, nous allons utiliser un petit dataset simul\u00e9 d'avis sur des films. Chaque avis sera class\u00e9 comme positif, n\u00e9gatif ou neutre.\\n\",\n        \"\\n\",\n        \"Dans un projet r\u00e9el, vous pourriez utiliser des datasets plus importants comme IMDB, Amazon Reviews, etc.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# Cr\u00e9er un petit jeu de donn\u00e9es d'avis sur les films (simul\u00e9)\\n\",\n        \"reviews = [\\n\",\n        \"    \\\"Ce film \u00e9tait excellent, j'ai vraiment ador\u00e9 les performances des acteurs.\\\",\\n\",\n        \"    \\\"Une exp\u00e9rience cin\u00e9matographique incroyable, absolument \u00e0 voir !\\\",\\n\",\n        \"    \\\"Un chef-d'\u0153uvre du cin\u00e9ma, magnifiquement r\u00e9alis\u00e9.\\\",\\n\",\n        \"    \\\"J'ai beaucoup appr\u00e9ci\u00e9 l'histoire et les personnages \u00e9taient bien d\u00e9velopp\u00e9s.\\\",\\n\",\n        \"    \\\"Visuellement \u00e9poustouflant avec une histoire captivante.\\\",\\n\",\n        \"    \\\"Un film d\u00e9cevant avec un sc\u00e9nario plein de trous.\\\",\\n\",\n        \"    \\\"Vraiment terrible, je n'ai pas aim\u00e9 du tout.\\\",\\n\",\n        \"    \\\"Un g\u00e2chis complet de temps et d'argent, \u00e9vitez \u00e0 tout prix.\\\",\\n\",\n        \"    \\\"Ennuyeux et pr\u00e9visible, les acteurs semblaient d\u00e9sint\u00e9ress\u00e9s.\\\",\\n\",\n        \"    \\\"Une d\u00e9ception totale, l'intrigue ne fait aucun sens.\\\",\\n\",\n        \"    \\\"C'\u00e9tait correct, ni bon ni mauvais.\\\",\\n\",\n        \"    \\\"Un film moyen avec quelques bons moments.\\\",\\n\",\n        \"    \\\"Certaines sc\u00e8nes \u00e9taient bonnes, mais dans l'ensemble assez moyen.\\\",\\n\",\n        \"    \\\"Pas aussi bon que je l'esp\u00e9rais, mais pas horrible non plus.\\\",\\n\",\n        \"    \\\"Une histoire int\u00e9ressante mais mal ex\u00e9cut\u00e9e.\\\",\\n\",\n        \"    \\\"Un film brillant qui m'a fait r\u00e9fl\u00e9chir pendant des jours.\\\",\\n\",\n        \"    \\\"Absolument sublime, l'un des meilleurs films que j'ai jamais vus.\\\",\\n\",\n        \"    \\\"Un d\u00e9sastre total, je me suis endormi au milieu.\\\",\\n\",\n        \"    \\\"Pas du tout ce \u00e0 quoi je m'attendais, tr\u00e8s d\u00e9\u00e7u.\\\",\\n\",\n        \"    \\\"Le jeu d'acteur \u00e9tait fantastique, mais l'histoire \u00e9tait faible.\\\"\\n\",\n        \"]\\n\",\n        \"\\n\",\n        \"# Attribuer des sentiments (0 = n\u00e9gatif, 1 = neutre, 2 = positif)\\n\",\n        \"sentiments = [2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 0, 0, 1]\\n\",\n        \"\\n\",\n        \"# Convertir en DataFrame pour faciliter la manipulation\\n\",\n        \"df = pd.DataFrame({\\n\",\n        \"    'review': reviews,\\n\",\n        \"    'sentiment': sentiments\\n\",\n        \"})\\n\",\n        \"\\n\",\n        \"# Afficher quelques informations sur le dataset\\n\",\n        \"print(f\\\"Nombre total d'avis: {len(df)}\\\")\\n\",\n        \"print(f\\\"R\u00e9partition des sentiments: {df['sentiment'].value_counts().sort_index()}\\\")\\n\",\n        \"\\n\",\n        \"# Afficher quelques exemples\\n\",\n        \"print(\\\"\\\\nExemples d'avis:\\\")\\n\",\n        \"for sentiment in [0, 1, 2]:\\n\",\n        \"    sample = df[df['sentiment'] == sentiment].iloc[0]\\n\",\n        \"    print(f\\\"Sentiment {sentiment}: '{sample['review']}'\\\")\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### Visualisation de la distribution des sentiments\\n\",\n        \"\\n\",\n        \"V\u00e9rifions que notre jeu de donn\u00e9es est \u00e9quilibr\u00e9 entre les diff\u00e9rentes classes.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# Visualiser la distribution des sentiments\\n\",\n        \"plt.figure(figsize=(8, 5))\\n\",\n        \"ax = sns.countplot(x='sentiment', data=df)\\n\",\n        \"plt.title('Distribution des sentiments')\\n\",\n        \"plt.xlabel('Sentiment (0=n\u00e9gatif, 1=neutre, 2=positif)')\\n\",\n        \"plt.ylabel('Nombre d\\\\'avis')\\n\",\n        \"\\n\",\n        \"# Ajouter les valeurs sur les barres\\n\",\n        \"for p in ax.patches:\\n\",\n        \"    ax.annotate(f\\\"{p.get_height()}\\\", (p.get_x() + p.get_width()/2., p.get_height()),\\n\",\n        \"                ha='center', va='bottom')\\n\",\n        \"\\n\",\n        \"plt.show()\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 3. Pr\u00e9traitement du texte\\n\",\n        \"\\n\",\n        \"Avant de pouvoir utiliser le texte avec notre mod\u00e8le LSTM, nous devons le pr\u00e9traiter. Cela implique plusieurs \u00e9tapes:\\n\",\n        \"1. Nettoyage (minuscules, suppression de ponctuation, etc.)\\n\",\n        \"2. Tokenisation (conversion du texte en s\u00e9quences de nombres)\\n\",\n        \"3. Padding (uniformisation de la longueur des s\u00e9quences)\\n\",\n        \"\\n\",\n        \"Commen\u00e7ons par le nettoyage de texte:\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"def preprocess_text(text):\\n\",\n        \"    \\\"\\\"\\\"Fonction pour nettoyer et normaliser le texte\\\"\\\"\\\"\\n\",\n        \"    # Convertir en minuscules\\n\",\n        \"    text = text.lower()\\n\",\n        \"    # Supprimer la ponctuation et les caract\u00e8res sp\u00e9ciaux\\n\",\n        \"    text = re.sub(r'[^\\\\w\\\\s]', '', text)\\n\",\n        \"    # Supprimer les chiffres\\n\",\n        \"    text = re.sub(r'\\\\d+', '', text)\\n\",\n        \"    # Supprimer les espaces multiples\\n\",\n        \"    text = re.sub(r'\\\\s+', ' ', text).strip()\\n\",\n        \"    return text\\n\",\n        \"\\n\",\n        \"# Appliquer le pr\u00e9traitement \u00e0 nos avis\\n\",\n        \"df['processed_review'] = df['review'].apply(preprocess_text)\\n\",\n        \"\\n\",\n        \"# Afficher un exemple avant et apr\u00e8s pr\u00e9traitement\\n\",\n        \"example_idx = 0\\n\",\n        \"print(f\\\"Avant: {df['review'][example_idx]}\\\")\\n\",\n        \"print(f\\\"Apr\u00e8s: {df['processed_review'][example_idx]}\\\")\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### Tokenisation du texte\\n\",\n        \"\\n\",\n        \"La tokenisation convertit le texte en s\u00e9quences num\u00e9riques que notre r\u00e9seau de neurones peut traiter.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# Configuration pour la tokenisation\\n\",\n        \"max_words = 1000  # Taille du vocabulaire\\n\",\n        \"max_len = 100     # Longueur maximale des s\u00e9quences\\n\",\n        \"\\n\",\n        \"# Cr\u00e9er et configurer le tokenizer\\n\",\n        \"tokenizer = Tokenizer(num_words=max_words, oov_token='&lt;OOV&gt;')\\n\",\n        \"tokenizer.fit_on_texts(df['processed_review'])\\n\",\n        \"\\n\",\n        \"# Convertir les textes en s\u00e9quences de tokens\\n\",\n        \"sequences = tokenizer.texts_to_sequences(df['processed_review'])\\n\",\n        \"\\n\",\n        \"# Appliquer le padding pour uniformiser la longueur des s\u00e9quences\\n\",\n        \"padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\\n\",\n        \"\\n\",\n        \"print(f\\\"Taille du vocabulaire: {len(tokenizer.word_index)}\\\")\\n\",\n        \"print(f\\\"Forme des s\u00e9quences apr\u00e8s padding: {padded_sequences.shape}\\\")\\n\",\n        \"\\n\",\n        \"# Afficher le mapping de quelques mots vers leurs tokens\\n\",\n        \"print(\\\"\\\\nExemples de mapping mot -&gt; token:\\\")\\n\",\n        \"sample_words = ['film', 'bon', 'mauvais', 'excellent', 'terrible']\\n\",\n        \"for word in sample_words:\\n\",\n        \"    if word in tokenizer.word_index:\\n\",\n        \"        print(f\\\"{word} -&gt; {tokenizer.word_index[word]}\\\")\\n\",\n        \"    else:\\n\",\n        \"        print(f\\\"{word} -&gt; Non trouv\u00e9 dans le vocabulaire\\\")\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### Visualisation d'une s\u00e9quence tokenis\u00e9e\\n\",\n        \"\\n\",\n        \"Pour mieux comprendre la tokenisation, visualisons comment un avis est converti en s\u00e9quence de tokens.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"def visualize_tokenized_sequence(text, tokens):\\n\",\n        \"    \\\"\\\"\\\"Visualise la correspondance entre mots et tokens\\\"\\\"\\\"\\n\",\n        \"    words = text.split()\\n\",\n        \"    plt.figure(figsize=(15, 3))\\n\",\n        \"    plt.bar(range(len(tokens)), tokens)\\n\",\n        \"    plt.xticks(range(len(tokens)), words, rotation=45, ha='right')\\n\",\n        \"    plt.ylabel('Token ID')\\n\",\n        \"    plt.title('Repr\u00e9sentation tokenis\u00e9e d\\\\'un avis')\\n\",\n        \"    plt.tight_layout()\\n\",\n        \"    plt.show()\\n\",\n        \"\\n\",\n        \"sample_idx = 0\\n\",\n        \"sample_text = df['processed_review'][sample_idx].split()[:15]  # Limiter \u00e0 15 mots pour lisibilit\u00e9\\n\",\n        \"sample_tokens = sequences[sample_idx][:15]\\n\",\n        \"\\n\",\n        \"print(f\\\"Exemple d'avis: {' '.join(sample_text)}\\\")\\n\",\n        \"print(f\\\"Tokens correspondants: {sample_tokens}\\\")\\n\",\n        \"visualize_tokenized_sequence(' '.join(sample_text), sample_tokens)\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 4. Division en ensembles d'entra\u00eenement et de test\\n\",\n        \"\\n\",\n        \"Avant de cr\u00e9er notre mod\u00e8le, divisons nos donn\u00e9es en ensembles d'entra\u00eenement et de test pour \u00e9valuer ses performances.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"from sklearn.model_selection import train_test_split\\n\",\n        \"\\n\",\n        \"# Division 70-30 avec stratification pour conserver la distribution des classes\\n\",\n        \"X_train, X_test, y_train, y_test = train_test_split(\\n\",\n        \"    padded_sequences, \\n\",\n        \"    df['sentiment'],\\n\",\n        \"    test_size=0.3,\\n\",\n        \"    random_state=42,\\n\",\n        \"    stratify=df['sentiment']  # Assurer une r\u00e9partition \u00e9quilibr\u00e9e des classes\\n\",\n        \")\\n\",\n        \"\\n\",\n        \"print(f\\\"Forme des donn\u00e9es d'entra\u00eenement: {X_train.shape}\\\")\\n\",\n        \"print(f\\\"Forme des donn\u00e9es de test: {X_test.shape}\\\")\\n\",\n        \"print(f\\\"Distribution des classes (entra\u00eenement): {pd.Series(y_train).value_counts().sort_index()}\\\")\\n\",\n        \"print(f\\\"Distribution des classes (test): {pd.Series(y_test).value_counts().sort_index()}\\\")\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 5. Cr\u00e9ation du mod\u00e8le LSTM\\n\",\n        \"\\n\",\n        \"Nous allons maintenant cr\u00e9er notre mod\u00e8le d'analyse de sentiment en utilisant une architecture LSTM bidirectionnelle.\\n\",\n        \"\\n\",\n        \"### Architecture du mod\u00e8le\\n\",\n        \"- **Couche d'embedding**: Convertit les tokens en vecteurs denses\\n\",\n        \"- **Couches LSTM bidirectionnelles**: Capture les d\u00e9pendances \u00e0 long terme dans les deux directions\\n\",\n        \"- **Dropout**: \u00c9vite le surapprentissage\\n\",\n        \"- **Couche dense finale**: Classification en 3 cat\u00e9gories (n\u00e9gatif, neutre, positif)\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# Param\u00e8tres du mod\u00e8le\\n\",\n        \"embedding_dim = 32  # Dimension de l'espace d'embedding\\n\",\n        \"\\n\",\n        \"# Cr\u00e9ation du mod\u00e8le\\n\",\n        \"model = Sequential([\\n\",\n        \"    # Couche d'embedding pour convertir les tokens en vecteurs denses\\n\",\n        \"    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len),\\n\",\n        \"    \\n\",\n        \"    # Couche LSTM bidirectionnelle\\n\",\n        \"    Bidirectional(LSTM(64, return_sequences=True)),\\n\",\n        \"    \\n\",\n        \"    # Deuxi\u00e8me couche LSTM suivie de dropout pour r\u00e9gularisation\\n\",\n        \"    Bidirectional(LSTM(32)),\\n\",\n        \"    Dropout(0.5),\\n\",\n        \"    \\n\",\n        \"    # Couche de classification (3 classes: n\u00e9gatif, neutre, positif)\\n\",\n        \"    Dense(3, activation='softmax')\\n\",\n        \"])\\n\",\n        \"\\n\",\n        \"# Afficher un r\u00e9sum\u00e9 du mod\u00e8le\\n\",\n        \"model.summary()\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### \ud83d\udca1 Points cl\u00e9s \u00e0 observer dans l'architecture\\n\",\n        \"\\n\",\n        \"- **LSTM bidirectionnel** : Lit le texte de gauche \u00e0 droite ET de droite \u00e0 gauche, capturant mieux le contexte\\n\",\n        \"- **return_sequences=True** : Permet d'empiler plusieurs couches LSTM\\n\",\n        \"- **Dropout** : D\u00e9sactive al\u00e9atoirement 50% des neurones pendant l'entra\u00eenement pour \u00e9viter le surapprentissage\\n\",\n        \"- **Activation softmax** : G\u00e9n\u00e8re une distribution de probabilit\u00e9 sur les 3 classes\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 6. Compilation et entra\u00eenement du mod\u00e8le\\n\",\n        \"\\n\",\n        \"Maintenant, compilons et entra\u00eenons notre mod\u00e8le LSTM.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# Compiler le mod\u00e8le\\n\",\n        \"model.compile(\\n\",\n        \"    optimizer='adam',\\n\",\n        \"    loss='sparse_categorical_crossentropy',  # Pour les \u00e9tiquettes sous forme d'entiers (non one-hot)\\n\",\n        \"    metrics=['accuracy']\\n\",\n        \")\\n\",\n        \"\\n\",\n        \"# Early stopping pour \u00e9viter le surapprentissage\\n\",\n        \"early_stopping = EarlyStopping(\\n\",\n        \"    monitor='val_loss',\\n\",\n        \"    patience=3,\\n\",\n        \"    restore_best_weights=True\\n\",\n        \")\\n\",\n        \"\\n\",\n        \"# Mesure du temps d'entra\u00eenement\\n\",\n        \"start_time = time.time()\\n\",\n        \"\\n\",\n        \"# Entra\u00eenement du mod\u00e8le\\n\",\n        \"history = model.fit(\\n\",\n        \"    X_train, \\n\",\n        \"    y_train, \\n\",\n        \"    epochs=20,\\n\",\n        \"    batch_size=4,  # Petit batch size en raison de la petite taille du dataset\\n\",\n        \"    validation_split=0.2,  # 20% des donn\u00e9es d'entra\u00eenement serviront \u00e0 la validation\\n\",\n        \"    callbacks=[early_stopping],\\n\",\n        \"    verbose=1\\n\",\n        \")\\n\",\n        \"\\n\",\n        \"training_time = time.time() - start_time\\n\",\n        \"print(f\\\"\\\\nTemps d'entra\u00eenement: {training_time:.2f} secondes\\\")\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### Visualisation de l'\u00e9volution de l'entra\u00eenement\\n\",\n        \"\\n\",\n        \"Observons comment la pr\u00e9cision et la perte ont \u00e9volu\u00e9 au cours de l'entra\u00eenement.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# Visualisation de l'entra\u00eenement\\n\",\n        \"plt.figure(figsize=(12, 5))\\n\",\n        \"\\n\",\n        \"# Graphique de pr\u00e9cision\\n\",\n        \"plt.subplot(1, 2, 1)\\n\",\n        \"plt.plot(history.history['accuracy'], label='Entra\u00eenement')\\n\",\n        \"plt.plot(history.history['val_accuracy'], label='Validation')\\n\",\n        \"plt.title('\u00c9volution de la pr\u00e9cision')\\n\",\n        \"plt.xlabel('\u00c9poque')\\n\",\n        \"plt.ylabel('Pr\u00e9cision')\\n\",\n        \"plt.legend()\\n\",\n        \"plt.grid(True, linestyle='--', alpha=0.6)\\n\",\n        \"\\n\",\n        \"# Graphique de perte\\n\",\n        \"plt.subplot(1, 2, 2)\\n\",\n        \"plt.plot(history.history['loss'], label='Entra\u00eenement')\\n\",\n        \"plt.plot(history.history['val_loss'], label='Validation')\\n\",\n        \"plt.title('\u00c9volution de la perte')\\n\",\n        \"plt.xlabel('\u00c9poque')\\n\",\n        \"plt.ylabel('Perte')\\n\",\n        \"plt.legend()\\n\",\n        \"plt.grid(True, linestyle='--', alpha=0.6)\\n\",\n        \"\\n\",\n        \"plt.tight_layout()\\n\",\n        \"plt.show()\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 7. \u00c9valuation du mod\u00e8le\\n\",\n        \"\\n\",\n        \"Maintenant, \u00e9valuons les performances de notre mod\u00e8le sur l'ensemble de test.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# \u00c9valuation sur l'ensemble de test\\n\",\n        \"test_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)\\n\",\n        \"print(f\\\"Pr\u00e9cision sur l'ensemble de test: {test_acc*100:.2f}%\\\")\\n\",\n        \"\\n\",\n        \"# G\u00e9n\u00e9rer les pr\u00e9dictions\\n\",\n        \"y_pred_proba = model.predict(X_test)\\n\",\n        \"y_pred_classes = np.argmax(y_pred_proba, axis=1)\\n\",\n        \"\\n\",\n        \"# Matrice de confusion\\n\",\n        \"conf_mat = confusion_matrix(y_test, y_pred_classes)\\n\",\n        \"plt.figure(figsize=(8, 6))\\n\",\n        \"sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', \\n\",\n        \"            xticklabels=['N\u00e9gatif', 'Neutre', 'Positif'],\\n\",\n        \"            yticklabels=['N\u00e9gatif', 'Neutre', 'Positif'])\\n\",\n        \"plt.xlabel('Pr\u00e9dit')\\n\",\n        \"plt.ylabel('R\u00e9el')\\n\",\n        \"plt.title('Matrice de confusion')\\n\",\n        \"plt.tight_layout()\\n\",\n        \"plt.show()\\n\",\n        \"\\n\",\n        \"# Rapport de classification\\n\",\n        \"print(\\\"\\\\nRapport de classification d\u00e9taill\u00e9:\\\")\\n\",\n        \"target_names = ['N\u00e9gatif', 'Neutre', 'Positif']\\n\",\n        \"print(classification_report(y_test, y_pred_classes, target_names=target_names))\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### \ud83e\udde0 R\u00e9flexions sur les r\u00e9sultats\\n\",\n        \"\\n\",\n        \"- **Analysez la matrice de confusion**: Quelles classes sont le mieux reconnues? Y a-t-il des confusions particuli\u00e8res?\\n\",\n        \"- **Pr\u00e9cision vs Rappel**: Y a-t-il un d\u00e9s\u00e9quilibre? Quelle m\u00e9trique privil\u00e9gier selon le contexte?\\n\",\n        \"- **Taille du dataset**: Comment les r\u00e9sultats pourraient-ils \u00eatre affect\u00e9s par la petite taille de notre jeu de donn\u00e9es?\\n\",\n        \"\\n\",\n        \"\ud83d\udc49 **Discussion**: Notez vos observations ci-dessous:\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"*\u00c9crivez vos observations ici...*\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 8. Test avec de nouveaux avis\\n\",\n        \"\\n\",\n        \"Testons maintenant notre mod\u00e8le avec quelques nouveaux avis qui n'ont pas \u00e9t\u00e9 utilis\u00e9s pour l'entra\u00eenement.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# Nouveaux avis \u00e0 tester\\n\",\n        \"new_reviews = [\\n\",\n        \"    \\\"Ce film \u00e9tait vraiment fantastique, j'ai ador\u00e9 chaque minute.\\\",\\n\",\n        \"    \\\"Je n'ai pas du tout aim\u00e9 ce film, c'\u00e9tait une perte de temps compl\u00e8te.\\\",\\n\",\n        \"    \\\"C'\u00e9tait un film correct, ni bon ni mauvais.\\\"\\n\",\n        \"]\\n\",\n        \"\\n\",\n        \"# Pr\u00e9traitement des nouveaux avis\\n\",\n        \"processed_new_reviews = [preprocess_text(review) for review in new_reviews]\\n\",\n        \"sequences_new = tokenizer.texts_to_sequences(processed_new_reviews)\\n\",\n        \"padded_new = pad_sequences(sequences_new, maxlen=max_len, padding='post', truncating='post')\\n\",\n        \"\\n\",\n        \"# Pr\u00e9dictions\\n\",\n        \"predictions = model.predict(padded_new)\\n\",\n        \"predicted_classes = np.argmax(predictions, axis=1)\\n\",\n        \"\\n\",\n        \"# Afficher les r\u00e9sultats\\n\",\n        \"sentiment_labels = {0: \\\"N\u00e9gatif\\\", 1: \\\"Neutre\\\", 2: \\\"Positif\\\"}\\n\",\n        \"\\n\",\n        \"print(\\\"Pr\u00e9dictions pour les nouveaux avis:\\\\n\\\")\\n\",\n        \"for i, review in enumerate(new_reviews):\\n\",\n        \"    pred_class = predicted_classes[i]\\n\",\n        \"    confidence = predictions[i][pred_class] * 100\\n\",\n        \"    \\n\",\n        \"    print(f\\\"Avis: {review}\\\")\\n\",\n        \"    print(f\\\"Sentiment pr\u00e9dit: {sentiment_labels[pred_class]} (confiance: {confidence:.2f}%)\\\")\\n\",\n        \"    print(\\\"Probabilit\u00e9s pour chaque classe:\\\")\\n\",\n        \"    for j, label in sentiment_labels.items():\\n\",\n        \"        print(f\\\"  {label}: {predictions[i][j]*100:.2f}%\\\")\\n\",\n        \"    print()\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### Visualisation graphique des pr\u00e9dictions\\n\",\n        \"\\n\",\n        \"Visualisons les probabilit\u00e9s pour chaque classe pour les nouveaux avis.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# Visualisation des probabilit\u00e9s pour chaque avis\\n\",\n        \"plt.figure(figsize=(15, 5))\\n\",\n        \"labels = ['N\u00e9gatif', 'Neutre', 'Positif']\\n\",\n        \"\\n\",\n        \"for i, review in enumerate(new_reviews):\\n\",\n        \"    plt.subplot(1, 3, i+1)\\n\",\n        \"    plt.bar(labels, predictions[i], color=['red', 'gray', 'green'])\\n\",\n        \"    plt.title(f\\\"Avis {i+1}\\\")\\n\",\n        \"    plt.ylim(0, 1)\\n\",\n        \"    plt.ylabel('Probabilit\u00e9')\\n\",\n        \"    plt.xticks(rotation=45)\\n\",\n        \"    \\n\",\n        \"    # Ajouter les valeurs sur les barres\\n\",\n        \"    for j, p in enumerate(predictions[i]):\\n\",\n        \"        plt.text(j, p + 0.02, f\\\"{p*100:.1f}%\\\", ha='center')\\n\",\n        \"        \\n\",\n        \"plt.tight_layout()\\n\",\n        \"plt.show()\"\n      ]\n    },\n</pre> {   \"cells\": [     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"# RNN/LSTM pour l'analyse de sentiment\\n\",         \"\\n\",         \"##  S\u00e9ance 2: Types de r\u00e9seaux de neurones\\n\",         \"\\n\",         \"Ce notebook vous guidera \u00e0 travers l'impl\u00e9mentation d'un mod\u00e8le LSTM (Long Short-Term Memory) pour l'analyse de sentiment. Vous d\u00e9couvrirez comment les r\u00e9seaux r\u00e9currents peuvent \u00eatre utilis\u00e9s pour comprendre et classifier du texte.\\n\",         \"\\n\",         \"### Objectifs d'apprentissage:\\n\",         \"- Comprendre le pr\u00e9traitement du texte pour les mod\u00e8les de Deep Learning\\n\",         \"- D\u00e9couvrir l'architecture et le fonctionnement des r\u00e9seaux LSTM\\n\",         \"- Apprendre \u00e0 \u00e9valuer un mod\u00e8le d'analyse de sentiment\\n\",         \"- Visualiser et interpr\u00e9ter les embeddings de mots\\n\",         \"\\n\",         \"### Pr\u00e9requis:\\n\",         \"- Connaissances de base en Python\\n\",         \"- Notions fondamentales de r\u00e9seaux de neurones\\n\",         \"- Avoir suivi la s\u00e9ance 1 d'introduction au Deep Learning\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 1. Configuration de l'environnement\\n\",         \"\\n\",         \"Commen\u00e7ons par importer les biblioth\u00e8ques n\u00e9cessaires et configurer notre environnement.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"import numpy as np\\n\",         \"import matplotlib.pyplot as plt\\n\",         \"import tensorflow as tf\\n\",         \"from tensorflow.keras.models import Sequential\\n\",         \"from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\\n\",         \"from tensorflow.keras.preprocessing.text import Tokenizer\\n\",         \"from tensorflow.keras.preprocessing.sequence import pad_sequences\\n\",         \"from tensorflow.keras.callbacks import EarlyStopping\\n\",         \"import pandas as pd\\n\",         \"import re\\n\",         \"import time\\n\",         \"import seaborn as sns\\n\",         \"from sklearn.metrics import confusion_matrix, classification_report\\n\",         \"\\n\",         \"# Configuration pour reproductibilit\u00e9\\n\",         \"np.random.seed(42)\\n\",         \"tf.random.set_seed(42)\\n\",         \"\\n\",         \"# V\u00e9rifier la version de TensorFlow\\n\",         \"print(f\\\"TensorFlow version: {tf.__version__}\\\")\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 2. Pr\u00e9paration des donn\u00e9es\\n\",         \"\\n\",         \"Pour ce TP, nous allons utiliser un petit dataset simul\u00e9 d'avis sur des films. Chaque avis sera class\u00e9 comme positif, n\u00e9gatif ou neutre.\\n\",         \"\\n\",         \"Dans un projet r\u00e9el, vous pourriez utiliser des datasets plus importants comme IMDB, Amazon Reviews, etc.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# Cr\u00e9er un petit jeu de donn\u00e9es d'avis sur les films (simul\u00e9)\\n\",         \"reviews = [\\n\",         \"    \\\"Ce film \u00e9tait excellent, j'ai vraiment ador\u00e9 les performances des acteurs.\\\",\\n\",         \"    \\\"Une exp\u00e9rience cin\u00e9matographique incroyable, absolument \u00e0 voir !\\\",\\n\",         \"    \\\"Un chef-d'\u0153uvre du cin\u00e9ma, magnifiquement r\u00e9alis\u00e9.\\\",\\n\",         \"    \\\"J'ai beaucoup appr\u00e9ci\u00e9 l'histoire et les personnages \u00e9taient bien d\u00e9velopp\u00e9s.\\\",\\n\",         \"    \\\"Visuellement \u00e9poustouflant avec une histoire captivante.\\\",\\n\",         \"    \\\"Un film d\u00e9cevant avec un sc\u00e9nario plein de trous.\\\",\\n\",         \"    \\\"Vraiment terrible, je n'ai pas aim\u00e9 du tout.\\\",\\n\",         \"    \\\"Un g\u00e2chis complet de temps et d'argent, \u00e9vitez \u00e0 tout prix.\\\",\\n\",         \"    \\\"Ennuyeux et pr\u00e9visible, les acteurs semblaient d\u00e9sint\u00e9ress\u00e9s.\\\",\\n\",         \"    \\\"Une d\u00e9ception totale, l'intrigue ne fait aucun sens.\\\",\\n\",         \"    \\\"C'\u00e9tait correct, ni bon ni mauvais.\\\",\\n\",         \"    \\\"Un film moyen avec quelques bons moments.\\\",\\n\",         \"    \\\"Certaines sc\u00e8nes \u00e9taient bonnes, mais dans l'ensemble assez moyen.\\\",\\n\",         \"    \\\"Pas aussi bon que je l'esp\u00e9rais, mais pas horrible non plus.\\\",\\n\",         \"    \\\"Une histoire int\u00e9ressante mais mal ex\u00e9cut\u00e9e.\\\",\\n\",         \"    \\\"Un film brillant qui m'a fait r\u00e9fl\u00e9chir pendant des jours.\\\",\\n\",         \"    \\\"Absolument sublime, l'un des meilleurs films que j'ai jamais vus.\\\",\\n\",         \"    \\\"Un d\u00e9sastre total, je me suis endormi au milieu.\\\",\\n\",         \"    \\\"Pas du tout ce \u00e0 quoi je m'attendais, tr\u00e8s d\u00e9\u00e7u.\\\",\\n\",         \"    \\\"Le jeu d'acteur \u00e9tait fantastique, mais l'histoire \u00e9tait faible.\\\"\\n\",         \"]\\n\",         \"\\n\",         \"# Attribuer des sentiments (0 = n\u00e9gatif, 1 = neutre, 2 = positif)\\n\",         \"sentiments = [2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 0, 0, 1]\\n\",         \"\\n\",         \"# Convertir en DataFrame pour faciliter la manipulation\\n\",         \"df = pd.DataFrame({\\n\",         \"    'review': reviews,\\n\",         \"    'sentiment': sentiments\\n\",         \"})\\n\",         \"\\n\",         \"# Afficher quelques informations sur le dataset\\n\",         \"print(f\\\"Nombre total d'avis: {len(df)}\\\")\\n\",         \"print(f\\\"R\u00e9partition des sentiments: {df['sentiment'].value_counts().sort_index()}\\\")\\n\",         \"\\n\",         \"# Afficher quelques exemples\\n\",         \"print(\\\"\\\\nExemples d'avis:\\\")\\n\",         \"for sentiment in [0, 1, 2]:\\n\",         \"    sample = df[df['sentiment'] == sentiment].iloc[0]\\n\",         \"    print(f\\\"Sentiment {sentiment}: '{sample['review']}'\\\")\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### Visualisation de la distribution des sentiments\\n\",         \"\\n\",         \"V\u00e9rifions que notre jeu de donn\u00e9es est \u00e9quilibr\u00e9 entre les diff\u00e9rentes classes.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# Visualiser la distribution des sentiments\\n\",         \"plt.figure(figsize=(8, 5))\\n\",         \"ax = sns.countplot(x='sentiment', data=df)\\n\",         \"plt.title('Distribution des sentiments')\\n\",         \"plt.xlabel('Sentiment (0=n\u00e9gatif, 1=neutre, 2=positif)')\\n\",         \"plt.ylabel('Nombre d\\\\'avis')\\n\",         \"\\n\",         \"# Ajouter les valeurs sur les barres\\n\",         \"for p in ax.patches:\\n\",         \"    ax.annotate(f\\\"{p.get_height()}\\\", (p.get_x() + p.get_width()/2., p.get_height()),\\n\",         \"                ha='center', va='bottom')\\n\",         \"\\n\",         \"plt.show()\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 3. Pr\u00e9traitement du texte\\n\",         \"\\n\",         \"Avant de pouvoir utiliser le texte avec notre mod\u00e8le LSTM, nous devons le pr\u00e9traiter. Cela implique plusieurs \u00e9tapes:\\n\",         \"1. Nettoyage (minuscules, suppression de ponctuation, etc.)\\n\",         \"2. Tokenisation (conversion du texte en s\u00e9quences de nombres)\\n\",         \"3. Padding (uniformisation de la longueur des s\u00e9quences)\\n\",         \"\\n\",         \"Commen\u00e7ons par le nettoyage de texte:\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"def preprocess_text(text):\\n\",         \"    \\\"\\\"\\\"Fonction pour nettoyer et normaliser le texte\\\"\\\"\\\"\\n\",         \"    # Convertir en minuscules\\n\",         \"    text = text.lower()\\n\",         \"    # Supprimer la ponctuation et les caract\u00e8res sp\u00e9ciaux\\n\",         \"    text = re.sub(r'[^\\\\w\\\\s]', '', text)\\n\",         \"    # Supprimer les chiffres\\n\",         \"    text = re.sub(r'\\\\d+', '', text)\\n\",         \"    # Supprimer les espaces multiples\\n\",         \"    text = re.sub(r'\\\\s+', ' ', text).strip()\\n\",         \"    return text\\n\",         \"\\n\",         \"# Appliquer le pr\u00e9traitement \u00e0 nos avis\\n\",         \"df['processed_review'] = df['review'].apply(preprocess_text)\\n\",         \"\\n\",         \"# Afficher un exemple avant et apr\u00e8s pr\u00e9traitement\\n\",         \"example_idx = 0\\n\",         \"print(f\\\"Avant: {df['review'][example_idx]}\\\")\\n\",         \"print(f\\\"Apr\u00e8s: {df['processed_review'][example_idx]}\\\")\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### Tokenisation du texte\\n\",         \"\\n\",         \"La tokenisation convertit le texte en s\u00e9quences num\u00e9riques que notre r\u00e9seau de neurones peut traiter.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# Configuration pour la tokenisation\\n\",         \"max_words = 1000  # Taille du vocabulaire\\n\",         \"max_len = 100     # Longueur maximale des s\u00e9quences\\n\",         \"\\n\",         \"# Cr\u00e9er et configurer le tokenizer\\n\",         \"tokenizer = Tokenizer(num_words=max_words, oov_token='')\\n\",         \"tokenizer.fit_on_texts(df['processed_review'])\\n\",         \"\\n\",         \"# Convertir les textes en s\u00e9quences de tokens\\n\",         \"sequences = tokenizer.texts_to_sequences(df['processed_review'])\\n\",         \"\\n\",         \"# Appliquer le padding pour uniformiser la longueur des s\u00e9quences\\n\",         \"padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\\n\",         \"\\n\",         \"print(f\\\"Taille du vocabulaire: {len(tokenizer.word_index)}\\\")\\n\",         \"print(f\\\"Forme des s\u00e9quences apr\u00e8s padding: {padded_sequences.shape}\\\")\\n\",         \"\\n\",         \"# Afficher le mapping de quelques mots vers leurs tokens\\n\",         \"print(\\\"\\\\nExemples de mapping mot -&gt; token:\\\")\\n\",         \"sample_words = ['film', 'bon', 'mauvais', 'excellent', 'terrible']\\n\",         \"for word in sample_words:\\n\",         \"    if word in tokenizer.word_index:\\n\",         \"        print(f\\\"{word} -&gt; {tokenizer.word_index[word]}\\\")\\n\",         \"    else:\\n\",         \"        print(f\\\"{word} -&gt; Non trouv\u00e9 dans le vocabulaire\\\")\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### Visualisation d'une s\u00e9quence tokenis\u00e9e\\n\",         \"\\n\",         \"Pour mieux comprendre la tokenisation, visualisons comment un avis est converti en s\u00e9quence de tokens.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"def visualize_tokenized_sequence(text, tokens):\\n\",         \"    \\\"\\\"\\\"Visualise la correspondance entre mots et tokens\\\"\\\"\\\"\\n\",         \"    words = text.split()\\n\",         \"    plt.figure(figsize=(15, 3))\\n\",         \"    plt.bar(range(len(tokens)), tokens)\\n\",         \"    plt.xticks(range(len(tokens)), words, rotation=45, ha='right')\\n\",         \"    plt.ylabel('Token ID')\\n\",         \"    plt.title('Repr\u00e9sentation tokenis\u00e9e d\\\\'un avis')\\n\",         \"    plt.tight_layout()\\n\",         \"    plt.show()\\n\",         \"\\n\",         \"sample_idx = 0\\n\",         \"sample_text = df['processed_review'][sample_idx].split()[:15]  # Limiter \u00e0 15 mots pour lisibilit\u00e9\\n\",         \"sample_tokens = sequences[sample_idx][:15]\\n\",         \"\\n\",         \"print(f\\\"Exemple d'avis: {' '.join(sample_text)}\\\")\\n\",         \"print(f\\\"Tokens correspondants: {sample_tokens}\\\")\\n\",         \"visualize_tokenized_sequence(' '.join(sample_text), sample_tokens)\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 4. Division en ensembles d'entra\u00eenement et de test\\n\",         \"\\n\",         \"Avant de cr\u00e9er notre mod\u00e8le, divisons nos donn\u00e9es en ensembles d'entra\u00eenement et de test pour \u00e9valuer ses performances.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"from sklearn.model_selection import train_test_split\\n\",         \"\\n\",         \"# Division 70-30 avec stratification pour conserver la distribution des classes\\n\",         \"X_train, X_test, y_train, y_test = train_test_split(\\n\",         \"    padded_sequences, \\n\",         \"    df['sentiment'],\\n\",         \"    test_size=0.3,\\n\",         \"    random_state=42,\\n\",         \"    stratify=df['sentiment']  # Assurer une r\u00e9partition \u00e9quilibr\u00e9e des classes\\n\",         \")\\n\",         \"\\n\",         \"print(f\\\"Forme des donn\u00e9es d'entra\u00eenement: {X_train.shape}\\\")\\n\",         \"print(f\\\"Forme des donn\u00e9es de test: {X_test.shape}\\\")\\n\",         \"print(f\\\"Distribution des classes (entra\u00eenement): {pd.Series(y_train).value_counts().sort_index()}\\\")\\n\",         \"print(f\\\"Distribution des classes (test): {pd.Series(y_test).value_counts().sort_index()}\\\")\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 5. Cr\u00e9ation du mod\u00e8le LSTM\\n\",         \"\\n\",         \"Nous allons maintenant cr\u00e9er notre mod\u00e8le d'analyse de sentiment en utilisant une architecture LSTM bidirectionnelle.\\n\",         \"\\n\",         \"### Architecture du mod\u00e8le\\n\",         \"- **Couche d'embedding**: Convertit les tokens en vecteurs denses\\n\",         \"- **Couches LSTM bidirectionnelles**: Capture les d\u00e9pendances \u00e0 long terme dans les deux directions\\n\",         \"- **Dropout**: \u00c9vite le surapprentissage\\n\",         \"- **Couche dense finale**: Classification en 3 cat\u00e9gories (n\u00e9gatif, neutre, positif)\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# Param\u00e8tres du mod\u00e8le\\n\",         \"embedding_dim = 32  # Dimension de l'espace d'embedding\\n\",         \"\\n\",         \"# Cr\u00e9ation du mod\u00e8le\\n\",         \"model = Sequential([\\n\",         \"    # Couche d'embedding pour convertir les tokens en vecteurs denses\\n\",         \"    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len),\\n\",         \"    \\n\",         \"    # Couche LSTM bidirectionnelle\\n\",         \"    Bidirectional(LSTM(64, return_sequences=True)),\\n\",         \"    \\n\",         \"    # Deuxi\u00e8me couche LSTM suivie de dropout pour r\u00e9gularisation\\n\",         \"    Bidirectional(LSTM(32)),\\n\",         \"    Dropout(0.5),\\n\",         \"    \\n\",         \"    # Couche de classification (3 classes: n\u00e9gatif, neutre, positif)\\n\",         \"    Dense(3, activation='softmax')\\n\",         \"])\\n\",         \"\\n\",         \"# Afficher un r\u00e9sum\u00e9 du mod\u00e8le\\n\",         \"model.summary()\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### \ud83d\udca1 Points cl\u00e9s \u00e0 observer dans l'architecture\\n\",         \"\\n\",         \"- **LSTM bidirectionnel** : Lit le texte de gauche \u00e0 droite ET de droite \u00e0 gauche, capturant mieux le contexte\\n\",         \"- **return_sequences=True** : Permet d'empiler plusieurs couches LSTM\\n\",         \"- **Dropout** : D\u00e9sactive al\u00e9atoirement 50% des neurones pendant l'entra\u00eenement pour \u00e9viter le surapprentissage\\n\",         \"- **Activation softmax** : G\u00e9n\u00e8re une distribution de probabilit\u00e9 sur les 3 classes\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 6. Compilation et entra\u00eenement du mod\u00e8le\\n\",         \"\\n\",         \"Maintenant, compilons et entra\u00eenons notre mod\u00e8le LSTM.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# Compiler le mod\u00e8le\\n\",         \"model.compile(\\n\",         \"    optimizer='adam',\\n\",         \"    loss='sparse_categorical_crossentropy',  # Pour les \u00e9tiquettes sous forme d'entiers (non one-hot)\\n\",         \"    metrics=['accuracy']\\n\",         \")\\n\",         \"\\n\",         \"# Early stopping pour \u00e9viter le surapprentissage\\n\",         \"early_stopping = EarlyStopping(\\n\",         \"    monitor='val_loss',\\n\",         \"    patience=3,\\n\",         \"    restore_best_weights=True\\n\",         \")\\n\",         \"\\n\",         \"# Mesure du temps d'entra\u00eenement\\n\",         \"start_time = time.time()\\n\",         \"\\n\",         \"# Entra\u00eenement du mod\u00e8le\\n\",         \"history = model.fit(\\n\",         \"    X_train, \\n\",         \"    y_train, \\n\",         \"    epochs=20,\\n\",         \"    batch_size=4,  # Petit batch size en raison de la petite taille du dataset\\n\",         \"    validation_split=0.2,  # 20% des donn\u00e9es d'entra\u00eenement serviront \u00e0 la validation\\n\",         \"    callbacks=[early_stopping],\\n\",         \"    verbose=1\\n\",         \")\\n\",         \"\\n\",         \"training_time = time.time() - start_time\\n\",         \"print(f\\\"\\\\nTemps d'entra\u00eenement: {training_time:.2f} secondes\\\")\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### Visualisation de l'\u00e9volution de l'entra\u00eenement\\n\",         \"\\n\",         \"Observons comment la pr\u00e9cision et la perte ont \u00e9volu\u00e9 au cours de l'entra\u00eenement.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# Visualisation de l'entra\u00eenement\\n\",         \"plt.figure(figsize=(12, 5))\\n\",         \"\\n\",         \"# Graphique de pr\u00e9cision\\n\",         \"plt.subplot(1, 2, 1)\\n\",         \"plt.plot(history.history['accuracy'], label='Entra\u00eenement')\\n\",         \"plt.plot(history.history['val_accuracy'], label='Validation')\\n\",         \"plt.title('\u00c9volution de la pr\u00e9cision')\\n\",         \"plt.xlabel('\u00c9poque')\\n\",         \"plt.ylabel('Pr\u00e9cision')\\n\",         \"plt.legend()\\n\",         \"plt.grid(True, linestyle='--', alpha=0.6)\\n\",         \"\\n\",         \"# Graphique de perte\\n\",         \"plt.subplot(1, 2, 2)\\n\",         \"plt.plot(history.history['loss'], label='Entra\u00eenement')\\n\",         \"plt.plot(history.history['val_loss'], label='Validation')\\n\",         \"plt.title('\u00c9volution de la perte')\\n\",         \"plt.xlabel('\u00c9poque')\\n\",         \"plt.ylabel('Perte')\\n\",         \"plt.legend()\\n\",         \"plt.grid(True, linestyle='--', alpha=0.6)\\n\",         \"\\n\",         \"plt.tight_layout()\\n\",         \"plt.show()\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 7. \u00c9valuation du mod\u00e8le\\n\",         \"\\n\",         \"Maintenant, \u00e9valuons les performances de notre mod\u00e8le sur l'ensemble de test.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# \u00c9valuation sur l'ensemble de test\\n\",         \"test_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)\\n\",         \"print(f\\\"Pr\u00e9cision sur l'ensemble de test: {test_acc*100:.2f}%\\\")\\n\",         \"\\n\",         \"# G\u00e9n\u00e9rer les pr\u00e9dictions\\n\",         \"y_pred_proba = model.predict(X_test)\\n\",         \"y_pred_classes = np.argmax(y_pred_proba, axis=1)\\n\",         \"\\n\",         \"# Matrice de confusion\\n\",         \"conf_mat = confusion_matrix(y_test, y_pred_classes)\\n\",         \"plt.figure(figsize=(8, 6))\\n\",         \"sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', \\n\",         \"            xticklabels=['N\u00e9gatif', 'Neutre', 'Positif'],\\n\",         \"            yticklabels=['N\u00e9gatif', 'Neutre', 'Positif'])\\n\",         \"plt.xlabel('Pr\u00e9dit')\\n\",         \"plt.ylabel('R\u00e9el')\\n\",         \"plt.title('Matrice de confusion')\\n\",         \"plt.tight_layout()\\n\",         \"plt.show()\\n\",         \"\\n\",         \"# Rapport de classification\\n\",         \"print(\\\"\\\\nRapport de classification d\u00e9taill\u00e9:\\\")\\n\",         \"target_names = ['N\u00e9gatif', 'Neutre', 'Positif']\\n\",         \"print(classification_report(y_test, y_pred_classes, target_names=target_names))\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### \ud83e\udde0 R\u00e9flexions sur les r\u00e9sultats\\n\",         \"\\n\",         \"- **Analysez la matrice de confusion**: Quelles classes sont le mieux reconnues? Y a-t-il des confusions particuli\u00e8res?\\n\",         \"- **Pr\u00e9cision vs Rappel**: Y a-t-il un d\u00e9s\u00e9quilibre? Quelle m\u00e9trique privil\u00e9gier selon le contexte?\\n\",         \"- **Taille du dataset**: Comment les r\u00e9sultats pourraient-ils \u00eatre affect\u00e9s par la petite taille de notre jeu de donn\u00e9es?\\n\",         \"\\n\",         \"\ud83d\udc49 **Discussion**: Notez vos observations ci-dessous:\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"*\u00c9crivez vos observations ici...*\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 8. Test avec de nouveaux avis\\n\",         \"\\n\",         \"Testons maintenant notre mod\u00e8le avec quelques nouveaux avis qui n'ont pas \u00e9t\u00e9 utilis\u00e9s pour l'entra\u00eenement.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# Nouveaux avis \u00e0 tester\\n\",         \"new_reviews = [\\n\",         \"    \\\"Ce film \u00e9tait vraiment fantastique, j'ai ador\u00e9 chaque minute.\\\",\\n\",         \"    \\\"Je n'ai pas du tout aim\u00e9 ce film, c'\u00e9tait une perte de temps compl\u00e8te.\\\",\\n\",         \"    \\\"C'\u00e9tait un film correct, ni bon ni mauvais.\\\"\\n\",         \"]\\n\",         \"\\n\",         \"# Pr\u00e9traitement des nouveaux avis\\n\",         \"processed_new_reviews = [preprocess_text(review) for review in new_reviews]\\n\",         \"sequences_new = tokenizer.texts_to_sequences(processed_new_reviews)\\n\",         \"padded_new = pad_sequences(sequences_new, maxlen=max_len, padding='post', truncating='post')\\n\",         \"\\n\",         \"# Pr\u00e9dictions\\n\",         \"predictions = model.predict(padded_new)\\n\",         \"predicted_classes = np.argmax(predictions, axis=1)\\n\",         \"\\n\",         \"# Afficher les r\u00e9sultats\\n\",         \"sentiment_labels = {0: \\\"N\u00e9gatif\\\", 1: \\\"Neutre\\\", 2: \\\"Positif\\\"}\\n\",         \"\\n\",         \"print(\\\"Pr\u00e9dictions pour les nouveaux avis:\\\\n\\\")\\n\",         \"for i, review in enumerate(new_reviews):\\n\",         \"    pred_class = predicted_classes[i]\\n\",         \"    confidence = predictions[i][pred_class] * 100\\n\",         \"    \\n\",         \"    print(f\\\"Avis: {review}\\\")\\n\",         \"    print(f\\\"Sentiment pr\u00e9dit: {sentiment_labels[pred_class]} (confiance: {confidence:.2f}%)\\\")\\n\",         \"    print(\\\"Probabilit\u00e9s pour chaque classe:\\\")\\n\",         \"    for j, label in sentiment_labels.items():\\n\",         \"        print(f\\\"  {label}: {predictions[i][j]*100:.2f}%\\\")\\n\",         \"    print()\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### Visualisation graphique des pr\u00e9dictions\\n\",         \"\\n\",         \"Visualisons les probabilit\u00e9s pour chaque classe pour les nouveaux avis.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# Visualisation des probabilit\u00e9s pour chaque avis\\n\",         \"plt.figure(figsize=(15, 5))\\n\",         \"labels = ['N\u00e9gatif', 'Neutre', 'Positif']\\n\",         \"\\n\",         \"for i, review in enumerate(new_reviews):\\n\",         \"    plt.subplot(1, 3, i+1)\\n\",         \"    plt.bar(labels, predictions[i], color=['red', 'gray', 'green'])\\n\",         \"    plt.title(f\\\"Avis {i+1}\\\")\\n\",         \"    plt.ylim(0, 1)\\n\",         \"    plt.ylabel('Probabilit\u00e9')\\n\",         \"    plt.xticks(rotation=45)\\n\",         \"    \\n\",         \"    # Ajouter les valeurs sur les barres\\n\",         \"    for j, p in enumerate(predictions[i]):\\n\",         \"        plt.text(j, p + 0.02, f\\\"{p*100:.1f}%\\\", ha='center')\\n\",         \"        \\n\",         \"plt.tight_layout()\\n\",         \"plt.show()\"       ]     },"},{"location":"module3/","title":"\ud83e\udde0Module 3 : D\u00e9veloppement d'applications pratiques","text":""},{"location":"module3/#objectifs-du-module","title":"\ud83c\udfaf Objectifs du module","text":"<p>\u00c0 l'issue de ce module, vous serez capable de :</p> <ul> <li>Utiliser efficacement les frameworks de Deep Learning (TensorFlow/Keras)</li> <li>Int\u00e9grer des mod\u00e8les pr\u00e9-entra\u00een\u00e9s dans des applications concr\u00e8tes</li> <li>Optimiser les performances de vos mod\u00e8les pour des environnements \u00e0 ressources limit\u00e9es</li> <li>Concevoir et pr\u00e9parer le d\u00e9veloppement d'un chatbot p\u00e9dagogique</li> <li>Explorer l'API Mistral AI pour d\u00e9velopper des applications d'IA conversationnelle</li> </ul>"},{"location":"module3/#programme-4h","title":"\ud83d\udcca Programme (4h)","text":"<p>Ce module se concentre sur les aspects pratiques du d\u00e9ploiement et de l'int\u00e9gration de mod\u00e8les de Deep Learning dans des applications r\u00e9elles.</p>"},{"location":"module3/#phase-1-frameworks-de-deep-learning-1h30","title":"Phase 1 : Frameworks de Deep Learning (1h30)","text":"<p>D\u00e9couvrez comment utiliser efficacement les frameworks de Deep Learning sans complexit\u00e9 excessive.</p> <ul> <li>Installation et configuration de TensorFlow/Keras</li> <li>Utilisation optimale des API de haut niveau</li> <li>Mod\u00e8les pr\u00e9-entra\u00een\u00e9s pour des t\u00e2ches courantes</li> <li>D\u00e9veloppement d'une API simple de reconnaissance d'images</li> </ul>"},{"location":"module3/#phase-2-amelioration-des-performances-1h30","title":"Phase 2 : Am\u00e9lioration des performances (1h30)","text":"<p>Apprenez \u00e0 optimiser vos mod\u00e8les pour les rendre plus rapides et plus efficaces.</p> <ul> <li>Techniques d'optimisation des performances</li> <li>Bonnes pratiques pour les mod\u00e8les de production</li> <li>Quantification et compression de mod\u00e8les</li> <li>TP pratique : am\u00e9lioration d'un mod\u00e8le pour une application web</li> </ul>"},{"location":"module3/#phase-3-preparation-au-projet-final-45min","title":"Phase 3 : Pr\u00e9paration au projet final (45min)","text":"<p>Pr\u00e9parez-vous au d\u00e9veloppement du chatbot p\u00e9dagogique qui constituera le projet final.</p> <ul> <li>Pr\u00e9sentation d\u00e9taill\u00e9e du cahier des charges</li> <li>\u00c9tude de cas r\u00e9els d'entreprises utilisant des chatbots</li> <li>Exploration de l'API Mistral AI pour le traitement du langage naturel</li> <li>Premiers pas vers un prototype fonctionnel</li> </ul>"},{"location":"module3/#livrables-attendus","title":"Livrables attendus","text":"<p>\u00c0 l'issue de ce module, vous devrez produire :</p> <ol> <li>Une API de reconnaissance d'images fonctionnelle</li> <li>Un mod\u00e8le optimis\u00e9 avec mesures de performances avant/apr\u00e8s</li> <li>Un document de conception pour votre chatbot p\u00e9dagogique</li> <li>Un premier prototype d'int\u00e9gration avec l'API Mistral</li> </ol>"},{"location":"module3/#competences-bts-sio-developpees","title":"Comp\u00e9tences BTS SIO d\u00e9velopp\u00e9es","text":"Comp\u00e9tence Description Activit\u00e9s associ\u00e9es B1.1 Gestion du patrimoine informatique Manipulation des outils et API professionnels B1.2 R\u00e9ponse aux incidents Gestion des erreurs API et cas limites B1.3 D\u00e9veloppement de la pr\u00e9sence en ligne Cr\u00e9ation d'applications web avec IA B2.2 Conception de solutions Architecture d'applications IA B2.3 D\u00e9veloppement d'applications Int\u00e9gration et optimisation de mod\u00e8les B3.1 Test et d\u00e9ploiement Mesure de performances et optimisation"},{"location":"module3/#ressources-fournies","title":"Ressources fournies","text":"<ul> <li>Templates de code pour l'API Flask/FastAPI</li> <li>Guide d'utilisation de l'API Mistral AI</li> <li>Mod\u00e8les pr\u00e9-entra\u00een\u00e9s pour d\u00e9monstration</li> <li>Documentation des bonnes pratiques d'optimisation</li> </ul>"},{"location":"module3/#pret-pour-la-partie-pratique","title":"Pr\u00eat pour la partie pratique ?","text":"<p>D\u00e9couvrez comment les frameworks modernes facilitent le d\u00e9veloppement d'applications de Deep Learning.</p> <p>Commencer par les Frameworks \u00c9valuer vos connaissances</p>"},{"location":"module3/frameworks/","title":"\ud83e\uddf0 Phase 1 : Frameworks de Deep Learning (1h30)","text":""},{"location":"module3/frameworks/#introduction-aux-frameworks-dans-un-contexte-professionnel-15-min","title":"\ud83d\udd0d Introduction aux frameworks dans un contexte professionnel (15 min)","text":"<p>** \ud83c\udfaf Objectif**: Comprendre l'utilit\u00e9 des frameworks de Deep Learning pour un d\u00e9veloppeur en entreprise et identifier ceux qui sont r\u00e9ellement utilis\u00e9s sur le terrain.</p>"},{"location":"module3/frameworks/#les-frameworks-en-entreprise","title":"\ud83d\udcbc Les frameworks en entreprise","text":"<p>Avant de plonger dans le code, prenons un moment pour comprendre pourquoi les frameworks de Deep Learning sont si importants en contexte professionnel:</p> <ul> <li>\ud83d\ude80 Productivit\u00e9: Ils permettent de d\u00e9velopper des applications d'IA sans repartir de z\u00e9ro</li> <li>\ud83d\udd27 Maintenabilit\u00e9: Code plus standard, plus facile \u00e0 comprendre par d'autres d\u00e9veloppeurs</li> <li>\u26a1 Performances: Optimisations int\u00e9gr\u00e9es qui seraient complexes \u00e0 d\u00e9velopper soi-m\u00eame</li> <li>\ud83d\udea2 D\u00e9ploiement: Outils int\u00e9gr\u00e9s pour mettre en production les mod\u00e8les</li> </ul> <p>Dans le monde professionnel actuel, plusieurs frameworks de Deep Learning sont couramment utilis\u00e9s:</p> Framework Principaux cas d'usage TensorFlow/Keras Applications web/mobile, syst\u00e8mes en production PyTorch Recherche, prototypage, startups Hugging Face NLP, chatbots, traitement de texte Scikit-learn Pr\u00e9traitement, ML classique, pipeline de donn\u00e9es <p>\"Pour un stage, la capacit\u00e9 \u00e0 utiliser efficacement des frameworks existants est recherch\u00e9e davantage que l'expertise th\u00e9orique approfondie en Deep Learning.\"  </p>"},{"location":"module3/frameworks/#tensorflowkeras-la-solution-pragmatique","title":"TensorFlow/Keras: la solution pragmatique","text":"<p>Pour cette s\u00e9ance, nous allons nous concentrer sur TensorFlow/Keras pour plusieurs raisons:</p> <ol> <li>Interface simple: Keras offre une API haut niveau, parfaite pour d\u00e9buter</li> <li>D\u00e9ploiement facile: Solutions int\u00e9gr\u00e9es pour mettre en production (TF Serving, TFLite)</li> <li>Documentation riche: Ressources abondantes en fran\u00e7ais</li> <li>Mod\u00e8les pr\u00e9-entra\u00een\u00e9s: Large biblioth\u00e8que de mod\u00e8les pr\u00eats \u00e0 l'emploi</li> <li>Demande professionnelle: Le plus mentionn\u00e9 dans les offres de stage</li> </ol>"},{"location":"module3/frameworks/#demonstration-applications-reelles-en-entreprise","title":"D\u00e9monstration: Applications r\u00e9elles en entreprise","text":"<p>Voici quelques exemples concrets d\u00e9velopp\u00e9s par des entreprises locales employant des anciens \u00e9tudiants:</p> <ul> <li>PME de logistique: Application de reconnaissance de documents (bons de livraison, factures) permettant d'automatiser la saisie \u2192 \u00c9conomie de 15h/semaine</li> <li>Agence web: Syst\u00e8me de d\u00e9tection de contenu inappropri\u00e9 dans les commentaires de sites e-commerce</li> <li>Cabinet m\u00e9dical: Application de classification d'images pour le tri pr\u00e9liminaire de photos de l\u00e9sions cutan\u00e9es</li> </ul>"},{"location":"module3/frameworks/#atelier-pratique-prise-en-main-de-tensorflowkeras-30-min","title":"Atelier pratique : Prise en main de TensorFlow/Keras (30 min)","text":""},{"location":"module3/frameworks/#objectif","title":"Objectif","text":"<p>D\u00e9velopper une premi\u00e8re application de reconnaissance d'images simple en utilisant TensorFlow/Keras et en suivant les bonnes pratiques de l'industrie.</p>"},{"location":"module3/frameworks/#instructions-pas-a-pas","title":"Instructions pas \u00e0 pas","text":""},{"location":"module3/frameworks/#etape-1-configuration-de-lenvironnement-5-min","title":"\u00c9tape 1: Configuration de l'environnement (5 min)","text":"<ol> <li>Ouvrez Google Colab dans votre navigateur en allant sur colab.research.google.com</li> <li>Cr\u00e9ez un nouveau notebook en cliquant sur \"Nouveau notebook\"</li> <li>Renommez le notebook en \"Reconnaissance d'images TensorFlow\" en cliquant sur \"Untitled\" en haut</li> <li>Copiez-collez le code suivant dans la premi\u00e8re cellule et ex\u00e9cutez-la en cliquant sur le bouton Play ou en appuyant sur Shift+Enter:</li> </ol> <pre><code># V\u00e9rification de la version de TensorFlow\nimport tensorflow as tf\nprint(\"TensorFlow version:\", tf.__version__)\n\n# Importation des biblioth\u00e8ques essentielles\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.preprocessing import image\nimport numpy as np\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"module3/frameworks/#etape-2-utilisation-dun-modele-pre-entraine-10-min","title":"\u00c9tape 2: Utilisation d'un mod\u00e8le pr\u00e9-entra\u00een\u00e9 (10 min)","text":"<ol> <li>Cr\u00e9ez une nouvelle cellule en cliquant sur le bouton \"+ Code\" ou en appuyant sur Alt+Enter</li> <li>Copiez-collez le code suivant et ex\u00e9cutez-le:</li> </ol> <pre><code># Chargement d'un mod\u00e8le pr\u00e9-entra\u00een\u00e9 complet\nmodel = MobileNetV2(weights='imagenet')\n\n# Affichage du r\u00e9sum\u00e9 du mod\u00e8le pour comprendre son architecture\nprint(\"Structure du mod\u00e8le:\")\nmodel.summary()\n</code></pre> <ol> <li>Observez la structure du mod\u00e8le dans la sortie: notez le nombre de param\u00e8tres, les diff\u00e9rentes couches, etc.</li> </ol>"},{"location":"module3/frameworks/#etape-3-preparation-dune-image-de-test-5-min","title":"\u00c9tape 3: Pr\u00e9paration d'une image de test (5 min)","text":"<ol> <li>Cr\u00e9ez une nouvelle cellule et ex\u00e9cutez le code suivant:</li> </ol> <pre><code># T\u00e9l\u00e9chargement d'une image d'exemple\n!wget -q -O test_image.jpg https://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Pug_600.jpg/280px-Pug_600.jpg\n\n# Affichage de l'image t\u00e9l\u00e9charg\u00e9e\nplt.figure(figsize=(4, 4))\nplt.imshow(image.load_img('test_image.jpg'))\nplt.axis('off')\nplt.title(\"Image \u00e0 classifier\")\nplt.show()\n\n# Fonction pour pr\u00e9traiter l'image\ndef preprocess_image(img_path):\n    # Chargement et redimensionnement \u00e0 la taille attendue par le mod\u00e8le\n    img = image.load_img(img_path, target_size=(224, 224))\n\n    # Conversion en tableau numpy\n    img_array = image.img_to_array(img)\n\n    # Ajout de la dimension de batch (pour un seul \u00e9chantillon)\n    img_array = np.expand_dims(img_array, axis=0)\n\n    # Pr\u00e9traitement sp\u00e9cifique \u00e0 MobileNetV2\n    img_array = tf.keras.applications.mobilenet_v2.preprocess_input(img_array)\n\n    return img_array\n\n# Application du pr\u00e9traitement \u00e0 notre image\nprocessed_image = preprocess_image('test_image.jpg')\nprint(\"Forme de l'image pr\u00e9trait\u00e9e:\", processed_image.shape)\n</code></pre>"},{"location":"module3/frameworks/#etape-4-prediction-et-interpretation-des-resultats-10-min","title":"\u00c9tape 4: Pr\u00e9diction et interpr\u00e9tation des r\u00e9sultats (10 min)","text":"<ol> <li>Cr\u00e9ez une nouvelle cellule et ex\u00e9cutez le code suivant:</li> </ol> <pre><code># Utilisation du mod\u00e8le pour faire une pr\u00e9diction\npredictions = model.predict(processed_image)\n\n# D\u00e9codage des pr\u00e9dictions (conversion des indices en labels)\nfrom tensorflow.keras.applications.mobilenet_v2 import decode_predictions\ndecoded_predictions = decode_predictions(predictions, top=5)[0]\n\n# Affichage des r\u00e9sultats sous forme de graphique\nplt.figure(figsize=(10, 3))\nlabels = [pred[1].replace('_', ' ') for pred in decoded_predictions]\nscores = [pred[2] for pred in decoded_predictions]\n\nplt.barh(labels, scores)\nplt.xlabel('Probabilit\u00e9')\nplt.title('Top 5 des pr\u00e9dictions')\nplt.xlim(0, 1.0)\nplt.gca().invert_yaxis()  # Pour que le plus probable soit en haut\nplt.tight_layout()\nplt.show()\n\n# Affichage des r\u00e9sultats d\u00e9taill\u00e9s\nprint(\"Pr\u00e9dictions:\")\nfor i, (imagenet_id, label, score) in enumerate(decoded_predictions):\n    print(f\"{i+1}. {label} ({score:.2f})\")\n</code></pre> <ol> <li>Analysez les r\u00e9sultats:</li> <li>Les pr\u00e9dictions indiquent-elles correctement qu'il s'agit d'un chien de race carlin (pug)?</li> <li>Observez les probabilit\u00e9s associ\u00e9es \u00e0 chaque classe</li> <li>Quelles autres races de chiens sont d\u00e9tect\u00e9es?</li> </ol>"},{"location":"module3/frameworks/#mini-projet-application-interactive-de-reconnaissance-dimages-45-min","title":"Mini-projet : Application interactive de reconnaissance d'images (45 min)","text":""},{"location":"module3/frameworks/#objectif_1","title":"Objectif","text":"<p>Cr\u00e9er une application interactive simple qui permet de tester la reconnaissance d'images sur diff\u00e9rentes photos.</p>"},{"location":"module3/frameworks/#instructions-pas-a-pas_1","title":"Instructions pas \u00e0 pas","text":""},{"location":"module3/frameworks/#etape-1-creation-dune-interface-interactive-dans-colab-10-min","title":"\u00c9tape 1: Cr\u00e9ation d'une interface interactive dans Colab (10 min)","text":"<ol> <li>Cr\u00e9ez une nouvelle cellule et ex\u00e9cutez le code suivant:</li> </ol> <pre><code># Installation des widgets interactifs\n!pip install -q ipywidgets\nimport ipywidgets as widgets\nfrom IPython.display import display, HTML, clear_output\n</code></pre> <ol> <li>Dans une nouvelle cellule, cr\u00e9ez l'interface utilisateur:</li> </ol> <pre><code># Fonction pour classifier une image t\u00e9l\u00e9charg\u00e9e\ndef classify_uploaded_image(change):\n    # Effacer les sorties pr\u00e9c\u00e9dentes\n    clear_output(wait=True)\n\n    # Afficher l'interface\n    display(file_upload)\n    display(output)\n\n    # R\u00e9cup\u00e9rer le fichier t\u00e9l\u00e9charg\u00e9\n    uploaded_file = list(change['new'].values())[0]\n    content = uploaded_file['content']\n\n    # Sauvegarder l'image localement\n    with open('uploaded_image.jpg', 'wb') as f:\n        f.write(content)\n\n    # Pr\u00e9traiter l'image\n    img = image.load_img('uploaded_image.jpg', target_size=(224, 224))\n    img_array = image.img_to_array(img)\n    img_array = np.expand_dims(img_array, axis=0)\n    img_array = tf.keras.applications.mobilenet_v2.preprocess_input(img_array)\n\n    # Afficher l'image originale\n    plt.figure(figsize=(10, 6))\n    plt.subplot(1, 2, 1)\n    plt.imshow(image.load_img('uploaded_image.jpg'))\n    plt.title(\"Image t\u00e9l\u00e9charg\u00e9e\")\n    plt.axis('off')\n\n    # Faire la pr\u00e9diction\n    predictions = model.predict(img_array)\n    decoded_preds = decode_predictions(predictions, top=5)[0]\n\n    # Afficher les r\u00e9sultats sous forme de graphique\n    plt.subplot(1, 2, 2)\n    labels = [pred[1].replace('_', ' ') for pred in decoded_preds]\n    scores = [pred[2] for pred in decoded_preds]\n\n    plt.barh(labels, scores)\n    plt.xlabel('Probabilit\u00e9')\n    plt.title('Top 5 des pr\u00e9dictions')\n    plt.xlim(0, 1.0)\n    plt.gca().invert_yaxis()\n    plt.tight_layout()\n    plt.show()\n\n    # Afficher les r\u00e9sultats textuels\n    print(\"Pr\u00e9dictions:\")\n    for i, (imagenet_id, label, score) in enumerate(decoded_preds):\n        print(f\"{i+1}. {label.replace('_', ' ')} ({score:.2f})\")\n\n# Cr\u00e9er les widgets\nfile_upload = widgets.FileUpload(\n    accept='.jpg, .jpeg, .png',\n    multiple=False,\n    description='T\u00e9l\u00e9charger une image:'\n)\noutput = widgets.Output()\n\n# Lier la fonction au widget\nfile_upload.observe(classify_uploaded_image, names='value')\n\n# Afficher l'interface\ndisplay(HTML(\"&lt;h3&gt;Application de reconnaissance d'images&lt;/h3&gt;\"))\ndisplay(HTML(\"&lt;p&gt;T\u00e9l\u00e9chargez une image pour voir ce que le mod\u00e8le peut reconna\u00eetre:&lt;/p&gt;\"))\ndisplay(file_upload)\ndisplay(output)\n</code></pre>"},{"location":"module3/frameworks/#etape-2-demonstration-avec-des-exemples-varies-15-min","title":"\u00c9tape 2: D\u00e9monstration avec des exemples vari\u00e9s (15 min)","text":"<ol> <li>Cr\u00e9ez une nouvelle cellule pour t\u00e9l\u00e9charger diff\u00e9rentes images de test:</li> </ol> <pre><code># T\u00e9l\u00e9chargement d'images vari\u00e9es pour nos tests\n!wget -q -O cat.jpg https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/1200px-Cat03.jpg\n!wget -q -O car.jpg https://upload.wikimedia.org/wikipedia/commons/thumb/2/2e/2022_Tesla_Model_Y.jpg/1200px-2022_Tesla_Model_Y.jpg\n!wget -q -O laptop.jpg https://upload.wikimedia.org/wikipedia/commons/thumb/e/e0/15-inch_MacBook_Pro_with_Touch_Bar_July_2018.jpg/1200px-15-inch_MacBook_Pro_with_Touch_Bar_July_2018.jpg\n\n# Fonction pour classifier les images d'exemple\ndef classify_example_images():\n    example_images = ['cat.jpg', 'car.jpg', 'laptop.jpg']\n\n    for img_path in example_images:\n        # Pr\u00e9traiter l'image\n        img = image.load_img(img_path, target_size=(224, 224))\n        img_array = image.img_to_array(img)\n        img_array = np.expand_dims(img_array, axis=0)\n        img_array = tf.keras.applications.mobilenet_v2.preprocess_input(img_array)\n\n        # Afficher l'image originale\n        plt.figure(figsize=(10, 4))\n        plt.subplot(1, 2, 1)\n        plt.imshow(image.load_img(img_path))\n        plt.title(f\"Image: {img_path}\")\n        plt.axis('off')\n\n        # Faire la pr\u00e9diction\n        predictions = model.predict(img_array)\n        decoded_preds = decode_predictions(predictions, top=5)[0]\n\n        # Afficher les r\u00e9sultats sous forme de graphique\n        plt.subplot(1, 2, 2)\n        labels = [pred[1].replace('_', ' ') for pred in decoded_preds]\n        scores = [pred[2] for pred in decoded_preds]\n\n        plt.barh(labels, scores)\n        plt.xlabel('Probabilit\u00e9')\n        plt.title('Top 5 des pr\u00e9dictions')\n        plt.xlim(0, 1.0)\n        plt.gca().invert_yaxis()\n        plt.tight_layout()\n        plt.show()\n\n        # Afficher les r\u00e9sultats textuels\n        print(f\"Pr\u00e9dictions pour {img_path}:\")\n        for i, (imagenet_id, label, score) in enumerate(decoded_preds):\n            print(f\"{i+1}. {label.replace('_', ' ')} ({score:.2f})\")\n        print(\"\\n\" + \"-\"*50 + \"\\n\")\n\n# Ex\u00e9cuter la fonction\nclassify_example_images()\n</code></pre>"},{"location":"module3/frameworks/#etape-3-creation-dun-outil-danalyse-approfondie-20-min","title":"\u00c9tape 3: Cr\u00e9ation d'un outil d'analyse approfondie (20 min)","text":"<ol> <li>Cr\u00e9ez une nouvelle cellule pour impl\u00e9menter un outil qui montre les activations internes du r\u00e9seau:</li> </ol> <pre><code>def visualize_activations(img_path):\n    \"\"\"Visualise les activations interm\u00e9diaires du mod\u00e8le pour mieux comprendre la reconnaissance\"\"\"\n    # Charger et pr\u00e9traiter l'image\n    img = image.load_img(img_path, target_size=(224, 224))\n    img_array = image.img_to_array(img)\n    img_array = np.expand_dims(img_array, axis=0)\n    img_array = tf.keras.applications.mobilenet_v2.preprocess_input(img_array)\n\n    # Cr\u00e9er un mod\u00e8le pour extraire les activations interm\u00e9diaires\n    layer_outputs = [layer.output for layer in model.layers if 'block' in layer.name][:3]  # Seulement les 3 premi\u00e8res couches de bloc\n    activation_model = tf.keras.Model(inputs=model.input, outputs=layer_outputs)\n\n    # Obtenir les activations\n    activations = activation_model.predict(img_array)\n\n    # Afficher l'image originale\n    plt.figure(figsize=(15, 10))\n    plt.subplot(1, 4, 1)\n    plt.title(\"Image originale\")\n    plt.imshow(image.load_img(img_path))\n    plt.axis('off')\n\n    # Afficher quelques activations pour chaque couche\n    for i, layer_activation in enumerate(activations):\n        # Seulement 9 filtres par couche\n        n_features = min(9, layer_activation.shape[-1])\n        layer_name = f\"Couche {i+1}\"\n\n        plt.subplot(1, 4, i+2)\n        plt.title(layer_name)\n\n        # Cr\u00e9er une grille pour les visualisations\n        n_cols = 3\n        n_rows = n_features // n_cols + (1 if n_features % n_cols &gt; 0 else 0)\n        display_grid = np.zeros((n_rows * 64, n_cols * 64))\n\n        # Remplir la grille avec des images\n        for row in range(n_rows):\n            for col in range(n_cols):\n                channel_idx = row * n_cols + col\n                if channel_idx &lt; n_features:\n                    # Prendre une activit\u00e9 moyenne sur la dimension du batch\n                    channel_image = layer_activation[0, :, :, channel_idx]\n\n                    # Normaliser pour une meilleure visualisation\n                    channel_image -= channel_image.mean()\n                    if channel_image.std() &gt; 0:\n                        channel_image /= channel_image.std()\n                    channel_image *= 64\n                    channel_image += 128\n                    channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n\n                    # Ajouter \u00e0 la grille\n                    display_grid[row*64:(row+1)*64, col*64:(col+1)*64] = channel_image\n\n        plt.imshow(display_grid, aspect='auto', cmap='viridis')\n        plt.axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\n    # Faire et afficher la pr\u00e9diction\n    predictions = model.predict(img_array)\n    decoded_preds = decode_predictions(predictions, top=5)[0]\n\n    print(f\"Pr\u00e9dictions pour {img_path}:\")\n    for i, (imagenet_id, label, score) in enumerate(decoded_preds):\n        print(f\"{i+1}. {label.replace('_', ' ')} ({score:.2f})\")\n\n# Tester l'outil sur plusieurs images\nfor img_path in ['cat.jpg', 'car.jpg', 'laptop.jpg']:\n    print(\"-\"*50)\n    print(f\"Analyse de {img_path}\")\n    print(\"-\"*50)\n    visualize_activations(img_path)\n    print(\"\\n\")\n</code></pre>"},{"location":"module3/frameworks/#bonnes-pratiques-pour-les-projets-professionnels-15-min","title":"Bonnes pratiques pour les projets professionnels (15 min)","text":"<p>Pour conclure cette phase, passons en revue les bonnes pratiques essentielles pour d\u00e9velopper des applications de Deep Learning en contexte professionnel:</p>"},{"location":"module3/frameworks/#1-structure-du-code","title":"1. Structure du code","text":"<ul> <li>Modularit\u00e9: S\u00e9parez clairement les diff\u00e9rentes fonctionnalit\u00e9s (pr\u00e9traitement, mod\u00e8le, API)</li> <li>Documentation: Commentez votre code et cr\u00e9ez une documentation utilisateur</li> <li>Gestion d'erreurs: Pr\u00e9voyez des cas d'erreur et des messages adapt\u00e9s</li> <li>Logging: Ajoutez des logs pour faciliter le d\u00e9bogage</li> </ul>"},{"location":"module3/frameworks/#2-performances","title":"2. Performances","text":"<ul> <li>Batch processing: Traitez les donn\u00e9es par lots plut\u00f4t qu'individuellement</li> <li>Mise en cache: \u00c9vitez de recharger le mod\u00e8le \u00e0 chaque requ\u00eate</li> <li>Pr\u00e9calcul: Pr\u00e9calculez ce qui peut l'\u00eatre pour acc\u00e9l\u00e9rer les inf\u00e9rences</li> <li>Optimisation mat\u00e9rielle: Utilisez GPU/TPU quand disponible, CPU optimis\u00e9 sinon</li> </ul>"},{"location":"module3/frameworks/#3-securite","title":"3. S\u00e9curit\u00e9","text":"<ul> <li>Validation des entr\u00e9es: V\u00e9rifiez toujours les donn\u00e9es entrantes</li> <li>Limitation de taille: Fixez une taille maximale pour les fichiers</li> <li>Rate limiting: Limitez le nombre de requ\u00eates par utilisateur</li> <li>Sanitization: Nettoyez les chemins de fichiers et autres entr\u00e9es sensibles</li> </ul>"},{"location":"module3/frameworks/#4-deploiement","title":"4. D\u00e9ploiement","text":"<ul> <li>Conteneurisation: Utilisez Docker pour faciliter le d\u00e9ploiement</li> <li>CI/CD: Automatisez les tests et le d\u00e9ploiement</li> <li>Monitoring: Surveillez les performances et erreurs</li> <li>Versioning: Versionnez vos mod\u00e8les et API</li> </ul>"},{"location":"module3/frameworks/#conclusion-et-transition","title":"Conclusion et transition","text":"<p>Cette phase vous a permis de d\u00e9couvrir comment utiliser efficacement TensorFlow/Keras dans un contexte professionnel. Vous avez appris \u00e0:</p> <ul> <li>Utiliser un mod\u00e8le pr\u00e9-entra\u00een\u00e9 pour la reconnaissance d'images</li> <li>Pr\u00e9traiter des images pour l'inf\u00e9rence</li> <li>Cr\u00e9er une interface interactive pour tester votre mod\u00e8le</li> <li>Visualiser et comprendre les activations internes du r\u00e9seau</li> </ul> <p>Ces comp\u00e9tences sont directement applicables dans des projets professionnels. Dans la prochaine partie, nous allons nous concentrer sur l'am\u00e9lioration des performances de nos mod\u00e8les pour les rendre plus adapt\u00e9s \u00e0 des environnements de production.</p> <p>Retour au Module 3 Continuer vers l'Int\u00e9gration et optimisation</p>"},{"location":"module3/integration/","title":"\u2699\ufe0f Phase 2 : Am\u00e9lioration des performances et int\u00e9gration (1h30)","text":""},{"location":"module3/integration/#introduction-aux-techniques-doptimisation-30-min","title":"\ud83d\udd0d Introduction aux techniques d'optimisation (30 min)","text":"<p>** \ud83c\udfaf Objectif**: Comprendre les diff\u00e9rentes techniques d'optimisation des mod\u00e8les de Deep Learning pour les environnements \u00e0 ressources limit\u00e9es et les applications en production.</p>"},{"location":"module3/integration/#pourquoi-optimiser-les-modeles","title":"\ud83e\udd14 Pourquoi optimiser les mod\u00e8les ?","text":"<p>Dans un contexte d'entreprise, l'optimisation des mod\u00e8les est essentielle pour plusieurs raisons :</p> <ul> <li>\ud83d\udcb0 Co\u00fbts d'infrastructure : R\u00e9duire les besoins en ressources mat\u00e9rielles</li> <li>\u23f1\ufe0f Latence : Am\u00e9liorer le temps de r\u00e9ponse pour une meilleure exp\u00e9rience utilisateur</li> <li>\ud83d\udd0b \u00c9nergie : Diminuer la consommation \u00e9nerg\u00e9tique (crucial pour les appareils mobiles)</li> <li>\ud83c\udf10 Accessibilit\u00e9 : Permettre l'ex\u00e9cution sur des appareils \u00e0 ressources limit\u00e9es</li> </ul>"},{"location":"module3/integration/#panorama-des-techniques-doptimisation","title":"Panorama des techniques d'optimisation","text":""},{"location":"module3/integration/#1-quantification","title":"1. Quantification","text":"<p>La quantification consiste \u00e0 r\u00e9duire la pr\u00e9cision des poids du mod\u00e8le (par exemple, passer de float32 \u00e0 int8). Cette technique peut r\u00e9duire la taille du mod\u00e8le par 4 et acc\u00e9l\u00e9rer l'inf\u00e9rence, avec une perte de pr\u00e9cision souvent n\u00e9gligeable.</p> <p>Comment \u00e7a marche :</p> <ul> <li>Les poids du mod\u00e8le, initialement stock\u00e9s en nombres \u00e0 virgule flottante 32 bits (float32), sont convertis en entiers 8 bits (int8)</li> <li>Une table de correspondance est cr\u00e9\u00e9e pour convertir les valeurs lors de l'inf\u00e9rence</li> <li>Les op\u00e9rations math\u00e9matiques sont effectu\u00e9es sur des entiers plut\u00f4t que sur des flottants, ce qui est beaucoup plus rapide sur la plupart des processeurs</li> </ul> <p>Avantages :</p> <ul> <li>Mod\u00e8les 3-4 fois plus petits</li> <li>Inf\u00e9rence 2-4 fois plus rapide sur CPU</li> <li>Consommation d'\u00e9nergie r\u00e9duite</li> </ul> <p>Inconv\u00e9nients :</p> <ul> <li>L\u00e9g\u00e8re baisse de pr\u00e9cision possible (1-2%)</li> <li>Plus sensible aux valeurs extr\u00eames</li> </ul>"},{"location":"module3/integration/#2-elagage-pruning","title":"2. \u00c9lagage (Pruning)","text":"<p>L'\u00e9lagage consiste \u00e0 supprimer les connexions (poids) les moins importantes du r\u00e9seau. Cette technique peut r\u00e9duire la taille du mod\u00e8le et acc\u00e9l\u00e9rer l'inf\u00e9rence sans impact significatif sur les performances.</p> <p>Comment \u00e7a marche :</p> <ul> <li>Pendant ou apr\u00e8s l'entra\u00eenement, on identifie les poids qui contribuent le moins aux pr\u00e9dictions</li> <li>Ces poids sont mis \u00e0 z\u00e9ro ou compl\u00e8tement supprim\u00e9s de la structure du r\u00e9seau</li> <li>Le mod\u00e8le peut \u00eatre r\u00e9entra\u00een\u00e9 apr\u00e8s \u00e9lagage pour r\u00e9cup\u00e9rer une partie de la pr\u00e9cision perdue</li> <li>Deux approches principales : \u00e9lagage structur\u00e9 (\u00e9liminer des neurones entiers) ou non structur\u00e9 (\u00e9liminer des connexions individuelles)</li> </ul> <p>Avantages :</p> <ul> <li>Peut r\u00e9duire la taille du mod\u00e8le de 75-90%</li> <li>Am\u00e9liore les performances sur des appareils \u00e0 m\u00e9moire limit\u00e9e</li> <li>Maintient la pr\u00e9cision si fait correctement</li> </ul> <p>Inconv\u00e9nients :</p> <ul> <li>N\u00e9cessite souvent un r\u00e9entra\u00eenement apr\u00e8s \u00e9lagage</li> <li>L'acc\u00e9l\u00e9ration r\u00e9elle d\u00e9pend du mat\u00e9riel et des biblioth\u00e8ques</li> </ul>"},{"location":"module3/integration/#3-distillation-de-connaissances","title":"3. Distillation de connaissances","text":"<p>La distillation consiste \u00e0 entra\u00eener un mod\u00e8le plus petit (\u00e9l\u00e8ve) \u00e0 imiter un mod\u00e8le plus grand et plus performant (enseignant).</p> <p>Comment \u00e7a marche :</p> <ul> <li>Un grand mod\u00e8le pr\u00e9-entra\u00een\u00e9 (l'enseignant) g\u00e9n\u00e8re des pr\u00e9dictions sur un ensemble de donn\u00e9es</li> <li>Un mod\u00e8le plus petit (l'\u00e9l\u00e8ve) est entra\u00een\u00e9 \u00e0 reproduire ces pr\u00e9dictions</li> <li>L'\u00e9l\u00e8ve apprend non seulement les bonnes r\u00e9ponses finales, mais aussi les \"probabilit\u00e9s douces\" du mod\u00e8le enseignant</li> <li>La fonction de perte combine g\u00e9n\u00e9ralement l'erreur de classification traditionnelle et l'erreur entre les distributions de probabilit\u00e9 de l'enseignant et de l'\u00e9l\u00e8ve</li> </ul> <p>Avantages :</p> <ul> <li>Mod\u00e8les plus petits avec des performances proches du grand mod\u00e8le</li> <li>Flexibilit\u00e9 dans le choix de l'architecture de l'\u00e9l\u00e8ve</li> <li>Transfert des \"incertitudes\" du mod\u00e8le enseignant qui contiennent une information pr\u00e9cieuse</li> </ul> <p>Inconv\u00e9nients :</p> <ul> <li>N\u00e9cessite deux phases : entra\u00eenement de l'enseignant puis distillation vers l'\u00e9l\u00e8ve</li> <li>Le choix de la fonction de perte de distillation est d\u00e9licat</li> </ul>"},{"location":"module3/integration/#4-architectures-efficientes","title":"4. Architectures efficientes","text":"<p>Utiliser des architectures sp\u00e9cialement con\u00e7ues pour l'efficience comme MobileNet, EfficientNet ou SqueezeNet.</p> <p>Comment \u00e7a marche : - Les architectures efficientes utilisent des blocs de construction optimis\u00e9s :   - Convolutions s\u00e9parables en profondeur (MobileNet) : s\u00e9parent une convolution standard en deux op\u00e9rations plus l\u00e9g\u00e8res   - Scaling compos\u00e9 (EfficientNet) : \u00e9quilibre optimal entre largeur, profondeur et r\u00e9solution du r\u00e9seau   - Strat\u00e9gie Fire (SqueezeNet) : remplace les gros filtres par des couches squeeze (1x1) suivies de couches expand (1x1 et 3x3)</p> <p>Avantages :</p> <ul> <li>Optimis\u00e9es pour des dispositifs sp\u00e9cifiques (mobile, embarqu\u00e9)</li> <li>Bon \u00e9quilibre performance/taille</li> <li>Souvent disponibles comme mod\u00e8les pr\u00e9-entra\u00een\u00e9s</li> </ul> <p>Inconv\u00e9nients :</p> <ul> <li>Performance l\u00e9g\u00e8rement inf\u00e9rieure aux grandes architectures</li> <li>Peut n\u00e9cessiter plus d'\u00e9poques d'entra\u00eenement</li> </ul>"},{"location":"module3/integration/#tp-integration-de-modeles-pre-entraines-dans-des-applications-45-min","title":"TP : Int\u00e9gration de mod\u00e8les pr\u00e9-entra\u00een\u00e9s dans des applications (45 min)","text":""},{"location":"module3/integration/#mise-en-contexte-stage-en-entreprise-pour-bts-sio","title":"Mise en contexte : Stage en entreprise pour BTS SIO","text":"<p>Sc\u00e9nario professionnel : Vous \u00eates stagiaire en d\u00e9veloppement dans une boutique de commerce \u00e9lectronique sp\u00e9cialis\u00e9e dans les v\u00eatements et accessoires. L'entreprise souhaite am\u00e9liorer l'exp\u00e9rience client avec une fonction de recherche visuelle. Le responsable technique vous demande de d\u00e9velopper un prototype permettant aux clients de prendre une photo d'un v\u00eatement pour trouver des articles similaires dans le catalogue.</p> <p>Ce projet r\u00e9pond \u00e0 plusieurs comp\u00e9tences du r\u00e9f\u00e9rentiel BTS SIO : - B1.3 - D\u00e9velopper la pr\u00e9sence en ligne de l'organisation - B2.2 - Concevoir une solution applicative - B2.3 - D\u00e9velopper des composants logiciels - B3.1 - Tester et d\u00e9ployer une solution applicative</p>"},{"location":"module3/integration/#objectif-du-tp","title":"Objectif du TP","text":"<p>Explorer et comprendre une application qui int\u00e8gre un mod\u00e8le de deep learning optimis\u00e9 pour la classification de v\u00eatements.</p>"},{"location":"module3/integration/#telechargement-et-exploration-du-projet","title":"T\u00e9l\u00e9chargement et exploration du projet","text":"<ol> <li>T\u00e9l\u00e9chargez le projet API de recherche visuelle (ZIP)</li> <li>Extrayez le contenu et examinez l'arborescence du projet</li> </ol>"},{"location":"module3/integration/#architecture-de-lapplication","title":"Architecture de l'application","text":"<p>L'application est structur\u00e9e selon une architecture en couches, avec une s\u00e9paration claire des responsabilit\u00e9s :</p> <pre><code>api-vetements-ia/\n\u251c\u2500\u2500 app.py                  # Point d'entr\u00e9e de l'application\n\u251c\u2500\u2500 config.py               # Configuration centralis\u00e9e\n\u251c\u2500\u2500 models/                 # Couche mod\u00e8le et logique m\u00e9tier\n\u251c\u2500\u2500 utils/                  # Utilitaires et fonctions d'aide\n\u251c\u2500\u2500 static/                 # Ressources statiques (CSS, JS, images)\n\u2514\u2500\u2500 templates/              # Templates HTML pour l'interface\n</code></pre>"},{"location":"module3/integration/#principaux-mecanismes-a-comprendre","title":"Principaux m\u00e9canismes \u00e0 comprendre","text":"<ol> <li> <p>Chargement optimis\u00e9 du mod\u00e8le</p> </li> <li> <p>Dans <code>models/classifier.py</code>, le mod\u00e8le est charg\u00e9 une seule fois au d\u00e9marrage de l'application</p> </li> <li>Un pattern singleton est utilis\u00e9 pour \u00e9viter les rechargements multiples</li> <li> <p>Le m\u00e9canisme de \"lazy loading\" permet de ne charger le mod\u00e8le que lorsqu'il est n\u00e9cessaire</p> </li> <li> <p>Pipeline de pr\u00e9traitement des images</p> </li> <li> <p>Dans <code>utils/image_utils.py</code>, on trouve les fonctions de pr\u00e9traitement des images</p> </li> <li>Le redimensionnement, la normalisation et la standardisation sont appliqu\u00e9s avant l'inf\u00e9rence</li> <li> <p>La d\u00e9tection et gestion de diff\u00e9rents formats d'entr\u00e9e (fichier, base64) simplifie l'int\u00e9gration</p> </li> <li> <p>Optimisations de performance</p> </li> <li> <p>Dans <code>utils/model_utils.py</code>, plusieurs techniques d'optimisation sont appliqu\u00e9es :</p> <ul> <li>Quantification des poids pour r\u00e9duire la taille du mod\u00e8le</li> <li>Fusion des op\u00e9rations de batch normalization avec les couches convolutives</li> <li>Optimisations sp\u00e9cifiques \u00e0 TensorFlow pour l'inf\u00e9rence</li> </ul> </li> <li> <p>Architecture API REST</p> </li> <li> <p>L'application expose une API REST pour permettre l'int\u00e9gration avec diff\u00e9rents clients</p> </li> <li>Le endpoint principal <code>/api/predict</code> accepte des images en entr\u00e9e et retourne les pr\u00e9dictions</li> <li> <p>Le endpoint de sant\u00e9 <code>/api/health</code> permet de v\u00e9rifier que l'API est op\u00e9rationnelle</p> </li> <li> <p>Interface utilisateur progressive</p> </li> <li> <p>L'interface web utilise JavaScript pour offrir une exp\u00e9rience fluide sans rechargement</p> </li> <li>La cam\u00e9ra peut \u00eatre utilis\u00e9e sur les appareils mobiles pour capturer directement des images</li> <li>Des indicateurs visuels (spinner, barres de progression) informent l'utilisateur sur l'\u00e9tat du traitement</li> </ol>"},{"location":"module3/integration/#points-cles-a-explorer-dans-le-code","title":"Points cl\u00e9s \u00e0 explorer dans le code","text":""},{"location":"module3/integration/#1-chargement-et-optimisation-du-modele-modelsclassifierpy","title":"1. Chargement et optimisation du mod\u00e8le (<code>models/classifier.py</code>)","text":"<p>Le chargement du mod\u00e8le est une op\u00e9ration co\u00fbteuse qui ne devrait \u00eatre effectu\u00e9e qu'une seule fois. Examinez comment :</p> <ul> <li>La classe <code>ClothingClassifier</code> impl\u00e9mente un pattern singleton pour partager une instance du mod\u00e8le</li> <li>Le syst\u00e8me g\u00e8re un mod\u00e8le de repli en cas d'\u00e9chec du chargement du mod\u00e8le principal</li> <li>La m\u00e9thode <code>optimize_model_for_inference</code> am\u00e9liore les performances d'inf\u00e9rence</li> </ul>"},{"location":"module3/integration/#2-pretraitement-des-images-utilsimage_utilspy","title":"2. Pr\u00e9traitement des images (<code>utils/image_utils.py</code>)","text":"<p>Le pr\u00e9traitement correct des images est crucial pour obtenir de bonnes pr\u00e9dictions. Analysez comment :</p> <ul> <li>Les images sont normalis\u00e9es et redimensionn\u00e9es aux dimensions attendues par le mod\u00e8le</li> <li>Diff\u00e9rents formats d'entr\u00e9e (fichiers, URLs, base64) sont g\u00e9r\u00e9s de mani\u00e8re transparente</li> <li>Le recadrage centr\u00e9 permet d'am\u00e9liorer la qualit\u00e9 des pr\u00e9dictions</li> </ul>"},{"location":"module3/integration/#3-api-rest-flask-apppy","title":"3. API REST Flask (<code>app.py</code>)","text":"<p>L'API REST est le point d'entr\u00e9e principal pour l'int\u00e9gration. Observez comment :</p> <ul> <li>Les requ\u00eates avec diff\u00e9rents formats de donn\u00e9es sont trait\u00e9es</li> <li>Les erreurs sont g\u00e9r\u00e9es et communiqu\u00e9es au client de mani\u00e8re claire</li> <li>Les informations de performance (temps de traitement) sont mesur\u00e9es et incluses dans la r\u00e9ponse</li> </ul>"},{"location":"module3/integration/#4-interface-progressive-staticjsappjs-et-templatesindexhtml","title":"4. Interface progressive (<code>static/js/app.js</code> et <code>templates/index.html</code>)","text":"<p>L'interface utilisateur est con\u00e7ue pour \u00eatre r\u00e9active et informative. Examinez comment :</p> <ul> <li>La capture d'image via la cam\u00e9ra est g\u00e9r\u00e9e</li> <li>L'interface affiche clairement la progression et les r\u00e9sultats</li> <li>Les exemples pr\u00e9d\u00e9finis permettent de tester rapidement le syst\u00e8me</li> </ul>"},{"location":"module3/integration/#exercices-pratiques","title":"Exercices pratiques","text":"<ol> <li> <p>Exploration du code</p> </li> <li> <p>Parcourez les diff\u00e9rents fichiers du projet pour comprendre leur r\u00f4le</p> </li> <li>Identifiez o\u00f9 les techniques d'optimisation vues pr\u00e9c\u00e9demment sont appliqu\u00e9es</li> <li> <p>Rep\u00e9rez les m\u00e9canismes de gestion d'erreurs et de fallback</p> </li> <li> <p>Comprendre le flux de donn\u00e9es</p> </li> <li> <p>Tracez le parcours d'une image depuis son upload jusqu'\u00e0 l'affichage des pr\u00e9dictions</p> </li> <li>Identifiez les transformations appliqu\u00e9es \u00e0 l'image</li> <li> <p>Rep\u00e9rez comment les pr\u00e9dictions du mod\u00e8le sont converties en r\u00e9sultats exploitables</p> </li> <li> <p>Optimisations potentielles</p> </li> <li> <p>R\u00e9fl\u00e9chissez \u00e0 d'autres optimisations qui pourraient \u00eatre appliqu\u00e9es</p> </li> <li>Comment am\u00e9liorer encore le temps de r\u00e9ponse de l'API ?</li> <li>Quelles fonctionnalit\u00e9s suppl\u00e9mentaires pourraient enrichir cette application ?</li> </ol>"},{"location":"module3/integration/#bonnes-pratiques-pour-lintegration-de-modeles-dans-des-applications-web-15-min","title":"Bonnes pratiques pour l'int\u00e9gration de mod\u00e8les dans des applications web (15 min)","text":"<p>\u00c0 travers ce projet, nous avons explor\u00e9 plusieurs approches pour optimiser et int\u00e9grer des mod\u00e8les de Deep Learning. R\u00e9sumons les bonnes pratiques essentielles \u00e0 retenir:</p>"},{"location":"module3/integration/#1-choix-du-modele","title":"1. Choix du mod\u00e8le","text":"<ul> <li>Privil\u00e9gier les architectures efficientes: MobileNet, EfficientNet, SqueezeNet</li> <li>\u00c9valuer le compromis taille/pr\u00e9cision: Un petit mod\u00e8le moins pr\u00e9cis est souvent pr\u00e9f\u00e9rable \u00e0 un mod\u00e8le lourd inutilisable</li> <li>Adapter la complexit\u00e9 au cas d'usage: La classification d'images simples ne n\u00e9cessite pas un ResNet152</li> </ul>"},{"location":"module3/integration/#2-techniques-doptimisation","title":"2. Techniques d'optimisation","text":"<ul> <li>Quantification: Toujours essayer la quantification post-entra\u00eenement</li> <li>\u00c9lagage: Pour les mod\u00e8les plus grands, envisager l'\u00e9lagage pendant l'entra\u00eenement</li> <li>Distillation: Pour des cas plus avanc\u00e9s, envisager la distillation de connaissances</li> <li>Combinaison des techniques: Utiliser plusieurs techniques peut donner les meilleurs r\u00e9sultats</li> </ul>"},{"location":"module3/integration/#3-integration-cote-serveur","title":"3. Int\u00e9gration c\u00f4t\u00e9 serveur","text":"<ul> <li>Mise en cache: \u00c9viter de recharger le mod\u00e8le pour chaque requ\u00eate</li> <li>Traitement par lot: Regrouper les requ\u00eates quand c'est possible</li> <li>Gestion asynchrone: Utiliser des files d'attente pour les requ\u00eates intensives</li> <li>Surveillance des performances: Mettre en place des m\u00e9triques (temps d'inf\u00e9rence, utilisation m\u00e9moire)</li> </ul>"},{"location":"module3/integration/#4-integration-cote-client","title":"4. Int\u00e9gration c\u00f4t\u00e9 client","text":"<ul> <li>Pr\u00e9traitement efficace: Redimensionner les images c\u00f4t\u00e9 client quand c'est possible</li> <li>Feedback utilisateur: Toujours indiquer que le traitement est en cours</li> <li>D\u00e9gradation gracieuse: Pr\u00e9voir un comportement de repli en cas d'\u00e9chec de l'IA</li> <li>Conservation de contexte: Limiter les allers-retours avec le serveur</li> </ul>"},{"location":"module3/integration/#5-documentation-et-maintenance","title":"5. Documentation et maintenance","text":"<ul> <li>Versionnement des mod\u00e8les: Suivre les versions des mod\u00e8les d\u00e9ploy\u00e9s</li> <li>Tests A/B: Comparer les performances des diff\u00e9rentes optimisations</li> <li>Journalisation des erreurs: Capturer les cas o\u00f9 le mod\u00e8le \u00e9choue</li> <li>Mise \u00e0 jour progressive: Planifier des am\u00e9liorations incr\u00e9mentales</li> </ul>"},{"location":"module3/integration/#conclusion-et-transition","title":"Conclusion et transition","text":"<p>Dans cette phase, nous avons explor\u00e9 des techniques d'optimisation essentielles pour rendre les mod\u00e8les de Deep Learning utilisables dans des applications r\u00e9elles. Nous avons vu comment r\u00e9duire la taille des mod\u00e8les, acc\u00e9l\u00e9rer leur inf\u00e9rence et les int\u00e9grer dans des applications web.</p> <p>Nous avons \u00e9galement examin\u00e9 un projet concret qui met en \u0153uvre ces concepts dans un contexte professionnel de stage BTS SIO. En comprenant comment structurer une application qui int\u00e8gre un mod\u00e8le de deep learning optimis\u00e9, vous \u00eates maintenant mieux pr\u00e9par\u00e9s pour d\u00e9velopper votre propre chatbot p\u00e9dagogique.</p> <p>Dans la prochaine phase, nous allons nous concentrer sur la pr\u00e9paration sp\u00e9cifique du projet de chatbot, en explorant l'API Mistral AI et en d\u00e9finissant le cahier des charges complet.</p> <p>Retour au Module 3 Continuer vers la pr\u00e9paration du projet</p>"},{"location":"module3/preparation-projet/","title":"\ud83d\udccb Phase 3 : Pr\u00e9paration au projet final (60 min)","text":""},{"location":"module3/preparation-projet/#presentation-du-cahier-des-charges-du-chatbot-pedagogique-15-min","title":"\ud83d\udcdd Pr\u00e9sentation du cahier des charges du chatbot p\u00e9dagogique (15 min)","text":"<p>\ud83c\udfaf Objectif: Comprendre les sp\u00e9cifications d\u00e9taill\u00e9es du projet final et les crit\u00e8res d'\u00e9valuation.</p>"},{"location":"module3/preparation-projet/#vision-du-projet","title":"\ud83d\udd0d Vision du projet","text":"<p>Le projet final consiste \u00e0 d\u00e9velopper un chatbot p\u00e9dagogique capable d'expliquer les concepts du Deep Learning, de r\u00e9pondre aux questions techniques et d'accompagner les apprenants dans leur d\u00e9couverte de ce domaine.</p> <p>\ud83c\udfaf Objectif : Concevoir un chatbot interactif qui aide les \u00e9tudiants de BTS SIO \u00e0 comprendre les concepts du Deep Learning \u00e0 travers des explications personnalis\u00e9es, des exemples concrets et des exercices adapt\u00e9s.</p>"},{"location":"module3/preparation-projet/#architecture-technique","title":"Architecture technique","text":"<p>Le chatbot s'appuiera sur une architecture moderne compos\u00e9e de trois \u00e9l\u00e9ments principaux :</p> <pre><code>flowchart LR\n    A[Interface Web] &lt;--&gt; B[Backend Python]\n    B &lt;--&gt; C[API Mistral AI]\n    D[Base de connaissances] &lt;--&gt; B</code></pre>"},{"location":"module3/preparation-projet/#1-interface-conversationnelle","title":"1. Interface conversationnelle","text":"<ul> <li>Interface web simple et intuitive</li> <li>Affichage des messages en format discussion</li> <li>Indicateur de chargement pendant le traitement</li> <li>Historique de conversation</li> </ul>"},{"location":"module3/preparation-projet/#2-backend-flaskfastapi","title":"2. Backend Flask/FastAPI","text":"<ul> <li>Gestion des requ\u00eates et des sessions</li> <li>Enrichissement des prompts avec la base de connaissances</li> <li>Communication avec l'API Mistral</li> <li>Logique de traitement des r\u00e9ponses</li> </ul>"},{"location":"module3/preparation-projet/#3-integration-api-mistral-ai","title":"3. Int\u00e9gration API Mistral AI","text":"<ul> <li>Configuration et param\u00e8trage des requ\u00eates</li> <li>Gestion du contexte de conversation</li> <li>Optimisation des prompts</li> <li>Gestion des erreurs et limitations</li> </ul>"},{"location":"module3/preparation-projet/#4-base-de-connaissances","title":"4. Base de connaissances","text":"<ul> <li>Structure JSON organis\u00e9e par concepts</li> <li>Exercices et quiz par th\u00e9matique</li> </ul>"},{"location":"module3/preparation-projet/#fonctionnalites-cles","title":"Fonctionnalit\u00e9s cl\u00e9s","text":"<p>Le chatbot p\u00e9dagogique offrira les fonctionnalit\u00e9s suivantes :</p> <ol> <li> <p>Explication des concepts</p> <ul> <li>D\u00e9finition adapt\u00e9e au niveau de l'utilisateur</li> <li>Exemples concrets pour illustrer chaque notion</li> <li>Analogies et comparaisons pour faciliter la compr\u00e9hension</li> </ul> </li> <li> <p>R\u00e9ponse aux questions</p> <ul> <li>Compr\u00e9hension des questions techniques</li> <li>R\u00e9ponses pr\u00e9cises bas\u00e9es sur la base de connaissances</li> <li>Capacit\u00e9 \u00e0 demander des clarifications si n\u00e9cessaire</li> </ul> </li> <li> <p>Progression adaptative</p> <ul> <li>D\u00e9tection du niveau de l'utilisateur</li> <li>Suggestions de concepts \u00e0 explorer ensuite</li> <li>Augmentation progressive de la complexit\u00e9</li> </ul> </li> <li> <p>Exercices interactifs</p> <ul> <li>G\u00e9n\u00e9ration de quiz sur les concepts vus</li> <li>Probl\u00e8mes simples \u00e0 r\u00e9soudre</li> <li>Feedback sur les r\u00e9ponses</li> </ul> </li> </ol>"},{"location":"module3/preparation-projet/#criteres-devaluation","title":"Crit\u00e8res d'\u00e9valuation","text":"<p>Votre chatbot p\u00e9dagogique sera \u00e9valu\u00e9 selon les crit\u00e8res suivants :</p> Crit\u00e8re Pond\u00e9ration Description Fonctionnalit\u00e9 30% Interface utilisable, r\u00e9ponses coh\u00e9rentes, absence de bugs Qualit\u00e9 p\u00e9dagogique 25% Pertinence des explications, adaptation au niveau, exemples appropri\u00e9s Int\u00e9gration technique 20% Utilisation efficace de l'API, gestion du contexte, optimisation Base de connaissances 15% Structure, couverture des concepts, pr\u00e9cision technique Documentation 10% Guide utilisateur, documentation technique, commentaires code"},{"location":"module3/preparation-projet/#livrables-attendus","title":"Livrables attendus","text":"<ol> <li>Code source complet du chatbot p\u00e9dagogique</li> <li>Base de connaissances structur\u00e9e sur le Deep Learning</li> <li>Documentation technique expliquant l'architecture et les choix d'impl\u00e9mentation</li> <li>Guide utilisateur pour la prise en main</li> <li>Pr\u00e9sentation de 5 minutes du projet finalis\u00e9</li> </ol>"},{"location":"module3/preparation-projet/#etude-de-cas-dentreprises-utilisant-des-chatbots-10-min","title":"\u00c9tude de cas d'entreprises utilisant des chatbots (10 min)","text":"<p>Avant de commencer le d\u00e9veloppement, examinons quelques exemples concrets d'entreprises qui ont mis en place des chatbots similaires \u00e0 celui que vous allez d\u00e9velopper.</p>"},{"location":"module3/preparation-projet/#cas-1-chatbot-pedagogique-pour-une-ecole-de-programmation","title":"Cas 1: Chatbot p\u00e9dagogique pour une \u00e9cole de programmation","text":"<p>Entreprise: CodeSchool (30 formateurs, 500+ \u00e9tudiants)</p> <p>Probl\u00e9matique:  Les formateurs recevaient de nombreuses questions basiques identiques, ce qui limitait leur disponibilit\u00e9 pour des probl\u00e8mes plus complexes.</p> <p>Solution:  D\u00e9veloppement d'un chatbot assistant bas\u00e9 sur une API de LLM, avec une base de connaissances construite \u00e0 partir du mat\u00e9riel de cours.</p> <p>Architecture:</p> <ul> <li>Frontend: Interface web int\u00e9gr\u00e9e \u00e0 la plateforme d'apprentissage</li> <li>Backend: API Flask avec mise en cache Redis</li> <li>LLM: OpenAI API avec fine-tuning sp\u00e9cifique aux cours</li> <li>Base de connaissances: Structur\u00e9e en JSON par modules de cours</li> </ul> <p>R\u00e9sultats:</p> <ul> <li>R\u00e9duction de 40% des questions basiques aux formateurs</li> <li>Satisfaction des \u00e9tudiants \u00e0 85% concernant les r\u00e9ponses du chatbot</li> <li>ROI positif apr\u00e8s 4 mois d'utilisation</li> <li>Cr\u00e9ation de 15 nouveaux modules de cours gr\u00e2ce au temps lib\u00e9r\u00e9</li> </ul> <p>Le\u00e7ons apprises:</p> <ul> <li>Importance d'un syst\u00e8me de feedback imm\u00e9diat sur les r\u00e9ponses</li> <li>N\u00e9cessit\u00e9 de maintenir la base de connaissances \u00e0 jour</li> <li>Valeur des r\u00e9ponses comportant des exemples de code fonctionnels</li> </ul>"},{"location":"module3/preparation-projet/#cas-2-assistant-virtuel-pour-la-formation-interne","title":"Cas 2: Assistant virtuel pour la formation interne","text":"<p>Entreprise: TechConsult (cabinet de conseil IT, 120 employ\u00e9s)</p> <p>Probl\u00e9matique:  Difficult\u00e9 \u00e0 former rapidement les nouveaux consultants sur les technologies sp\u00e9cifiques utilis\u00e9es par l'entreprise.</p> <p>Solution: Chatbot de formation accessible 24/7, int\u00e9gr\u00e9 \u00e0 l'intranet, avec connaissance des processus et technologies internes.</p> <p>Architecture:</p> <ul> <li>Interface: Application web responsive</li> <li>Backend: NodeJS avec FastAPI</li> <li>LLM: Combinaison d'API locale et Mistral AI</li> <li>Base de connaissances: Documents techniques convertis en embeddings vectoriels</li> </ul> <p>R\u00e9sultats:</p> <ul> <li>R\u00e9duction du temps d'onboarding de 3 semaines \u00e0 10 jours</li> <li>Augmentation de 25% du taux de r\u00e9ussite aux certifications internes</li> <li>\u00c9conomie estim\u00e9e de 180 heures de formation par an</li> <li>Adoption \u00e0 92% parmi les nouveaux employ\u00e9s</li> </ul> <p>Le\u00e7ons apprises:</p> <ul> <li>L'importance d'utiliser le vocabulaire sp\u00e9cifique de l'entreprise</li> <li>La valeur d'un historique de conversation persistant</li> <li>L'utilit\u00e9 des prompts techniques bien formul\u00e9s</li> </ul> <p>Ces \u00e9tudes de cas montrent que les chatbots p\u00e9dagogiques peuvent apporter une valeur significative lorsqu'ils sont bien con\u00e7us et adapt\u00e9s \u00e0 leur contexte d'utilisation. Votre projet s'inspirera de ces bonnes pratiques tout en se focalisant sur l'enseignement du Deep Learning.</p>"},{"location":"module3/preparation-projet/#exploration-guidee-de-lapi-mistral-ai-30-min","title":"Exploration guid\u00e9e de l'API Mistral AI (30 min)","text":"<p>Maintenant, explorons l'API Mistral AI que vous utiliserez pour d\u00e9velopper votre chatbot p\u00e9dagogique.</p>"},{"location":"module3/preparation-projet/#preparation-avant-la-seance","title":"Pr\u00e9paration avant la s\u00e9ance","text":"<p>Pour optimiser le temps de d\u00e9veloppement lors de la s\u00e9ance du Module 4, veuillez effectuer les \u00e9tapes suivantes avant la s\u00e9ance :</p>"},{"location":"module3/preparation-projet/#creation-dun-compte-et-cle-api","title":"Cr\u00e9ation d'un compte et cl\u00e9 API","text":"<ol> <li>Rendez-vous sur console.mistral.ai</li> <li>Cr\u00e9ez un compte (gratuit)</li> <li>Une fois connect\u00e9, cliquez sur \"API Keys\" dans le menu</li> <li>Cliquez sur \"Create API Key\", donnez-lui un nom (ex: \"Projet Chatbot BTS\")</li> <li>Important: Copiez et sauvegardez la cl\u00e9 g\u00e9n\u00e9r\u00e9e, elle ne sera plus affich\u00e9e ensuite</li> </ol>"},{"location":"module3/preparation-projet/#tester-avec-lapi","title":"Tester avec l'API","text":"<p>Commen\u00e7ons par un exemple simple pour tester l'API:</p> <pre><code>import os\nfrom mistralai.client import MistralClient\nfrom mistralai.models.chat_completion import ChatMessage\n\n# Configuration de l'API\napi_key = \"votre_cl\u00e9_api_ici\"  # Remplacez par votre cl\u00e9\nclient = MistralClient(api_key=api_key)\n\n# Messages\nmessages = [\n    ChatMessage(role=\"system\", content=\"Tu es un assistant p\u00e9dagogique sp\u00e9cialis\u00e9 dans l'explication du Deep Learning pour des \u00e9tudiants de BTS SIO.\"),\n    ChatMessage(role=\"user\", content=\"Peux-tu m'expliquer simplement ce qu'est un r\u00e9seau de neurones convolutif?\")\n]\n\n# Appel \u00e0 l'API\nchat_response = client.chat(\n    model=\"mistral-tiny\",  # Mod\u00e8le le plus l\u00e9ger\n    messages=messages,\n)\n\n# Affichage de la r\u00e9ponse\nprint(chat_response.choices[0].message.content)\n</code></pre>"},{"location":"module3/preparation-projet/#structure-de-lapi-mistral","title":"Structure de l'API Mistral","text":"<p>L'API Mistral AI fonctionne avec une structure simple :</p> <ol> <li>Messages : Liste de messages repr\u00e9sentant une conversation, chacun avec un r\u00f4le (system, user, assistant)</li> <li>Mod\u00e8le : Choix du mod\u00e8le Mistral \u00e0 utiliser (mistral-tiny, mistral-small, mistral-medium...)</li> <li>Param\u00e8tres : Configuration du comportement (temp\u00e9rature, nombre max de tokens, etc.)</li> </ol>"},{"location":"module3/preparation-projet/#gestion-du-contexte-conversationnel","title":"Gestion du contexte conversationnel","text":"<p>Pour maintenir un contexte de conversation, il suffit d'ajouter les messages pr\u00e9c\u00e9dents \u00e0 chaque requ\u00eate :</p> <pre><code># Fonction pour g\u00e9rer une conversation\ndef chat_with_context(messages, user_input):\n    # Ajouter le message de l'utilisateur\n    messages.append(ChatMessage(role=\"user\", content=user_input))\n\n    # Appel \u00e0 l'API\n    response = client.chat(\n        model=\"mistral-tiny\",\n        messages=messages,\n    )\n\n    # R\u00e9cup\u00e9rer la r\u00e9ponse\n    assistant_message = response.choices[0].message.content\n\n    # Ajouter la r\u00e9ponse au contexte\n    messages.append(ChatMessage(role=\"assistant\", content=assistant_message))\n\n    return assistant_message, messages\n\n# Initialiser la conversation\nconversation = [\n    ChatMessage(role=\"system\", content=\"Tu es un assistant p\u00e9dagogique sp\u00e9cialis\u00e9 dans l'explication du Deep Learning pour des \u00e9tudiants de BTS SIO.\")\n]\n\n# Premier \u00e9change\nresponse, conversation = chat_with_context(conversation, \"Qu'est-ce qu'un r\u00e9seau de neurones?\")\nprint(\"Assistant:\", response)\n\n# Deuxi\u00e8me \u00e9change (avec le contexte pr\u00e9c\u00e9dent)\nresponse, conversation = chat_with_context(conversation, \"Comment fonctionne l'apprentissage?\")\nprint(\"Assistant:\", response)\n</code></pre>"},{"location":"module3/preparation-projet/#optimisation-des-prompts","title":"Optimisation des prompts","text":"<p>La qualit\u00e9 des r\u00e9ponses d\u00e9pend beaucoup de la fa\u00e7on dont vous formulez vos instructions (prompts). Voici quelques conseils pour les optimiser :</p>"},{"location":"module3/preparation-projet/#1-instructions-systeme-claires-et-detaillees","title":"1. Instructions syst\u00e8me claires et d\u00e9taill\u00e9es","text":"<pre><code>system_prompt = \"\"\"\nTu es un assistant p\u00e9dagogique sp\u00e9cialis\u00e9 dans le Deep Learning pour des \u00e9tudiants de BTS SIO. \nQuand tu r\u00e9ponds:\n1. Utilise un langage simple et accessible\n2. Fournis toujours un exemple concret\n3. Structure tes explications en plusieurs points\n4. Si tu n'es pas s\u00fbr d'une information, indique-le clairement\n5. Adapte le niveau technique au profil de l'\u00e9tudiant (d\u00e9butant, interm\u00e9diaire, avanc\u00e9)\n\"\"\"\n</code></pre>"},{"location":"module3/preparation-projet/#2-enrichissement-avec-la-base-de-connaissances","title":"2. Enrichissement avec la base de connaissances","text":"<pre><code>def enrich_prompt_with_knowledge(user_query, knowledge_base, user_level=\"d\u00e9butant\"):\n    # Rechercher des informations pertinentes dans la base de connaissances\n    relevant_info = search_knowledge_base(user_query, knowledge_base)\n\n    # Enrichir le prompt avec ces informations\n    enriched_prompt = f\"\"\"\nQuestion de l'utilisateur: {user_query}\n\nInformations pertinentes (niveau: {user_level}):\n{relevant_info}\n\nR\u00e9ponds de mani\u00e8re p\u00e9dagogique en utilisant ces informations et en adaptant ton explication au niveau {user_level}.\n\"\"\"\n    return enriched_prompt\n</code></pre>"},{"location":"module3/preparation-projet/#3-parametrage-adapte","title":"3. Param\u00e9trage adapt\u00e9","text":"<pre><code># Pour des explications techniques (plus pr\u00e9cises, moins cr\u00e9atives)\ntechnical_params = {\n    \"temperature\": 0.3,  # Faible temp\u00e9rature pour des r\u00e9ponses plus d\u00e9terministes\n    \"max_tokens\": 500    # Limite de longueur raisonnable\n}\n\n# Pour des exemples et analogies (plus cr\u00e9atifs)\ncreative_params = {\n    \"temperature\": 0.7,  # Temp\u00e9rature plus \u00e9lev\u00e9e pour plus de cr\u00e9ativit\u00e9\n    \"max_tokens\": 300    # Limite de longueur adapt\u00e9e\n}\n\n# Fonction de choix de param\u00e8tres selon le contexte\ndef get_params_for_query(query):\n    if \"explique\" in query.lower() or \"d\u00e9finition\" in query.lower():\n        return technical_params\n    elif \"exemple\" in query.lower() or \"analogie\" in query.lower():\n        return creative_params\n    else:\n        return {\"temperature\": 0.5, \"max_tokens\": 400}  # Param\u00e8tres par d\u00e9faut\n</code></pre>"},{"location":"module3/preparation-projet/#gestion-des-erreurs-et-limites","title":"Gestion des erreurs et limites","text":"<p>Il est important de g\u00e9rer correctement les erreurs potentielles lors de l'utilisation de l'API :</p> <pre><code>def safe_api_call(messages, max_retries=3):\n    retries = 0\n    while retries &lt; max_retries:\n        try:\n            response = client.chat(\n                model=\"mistral-tiny\",\n                messages=messages,\n                timeout=10  # Timeout en secondes\n            )\n            return response\n        except Exception as e:\n            retries += 1\n            print(f\"Erreur API (tentative {retries}/{max_retries}): {e}\")\n            if retries &gt;= max_retries:\n                # R\u00e9ponse de secours si l'API \u00e9choue\n                return {\n                    \"choices\": [{\n                        \"message\": {\n                            \"content\": \"D\u00e9sol\u00e9, je rencontre des difficult\u00e9s techniques. Veuillez r\u00e9essayer dans quelques instants.\"\n                        }\n                    }]\n                }\n            # Attente avant nouvelle tentative\n            time.sleep(2)\n</code></pre>"},{"location":"module3/preparation-projet/#preparation-au-developpement","title":"Pr\u00e9paration au d\u00e9veloppement","text":"<p>Pour pr\u00e9parer efficacement votre projet de chatbot p\u00e9dagogique, voici les premi\u00e8res \u00e9tapes \u00e0 suivre :</p> <ol> <li> <p>Structure de votre projet <pre><code>chatbot-pedagogique/\n\u251c\u2500\u2500 app.py                   # Application principale Flask/FastAPI\n\u251c\u2500\u2500 config.py                # Configuration (cl\u00e9s API, param\u00e8tres)\n\u251c\u2500\u2500 templates/               # Templates HTML\n\u2502   \u2514\u2500\u2500 index.html           # Interface web\n\u251c\u2500\u2500 static/                  # Fichiers statiques (CSS, JS)\n\u251c\u2500\u2500 services/                # Services m\u00e9tier\n\u2502   \u251c\u2500\u2500 mistral_service.py   # Int\u00e9gration API Mistral\n\u2502   \u2514\u2500\u2500 knowledge_service.py # Gestion base de connaissances\n\u2514\u2500\u2500 knowledge_base/          # Base de connaissances\n    \u2514\u2500\u2500 concepts.json        # Structure des concepts DL\n</code></pre></p> </li> <li> <p>Technologies recommand\u00e9es</p> <ul> <li>Backend: Python avec Flask ou FastAPI</li> <li>Frontend: HTML/CSS/JavaScript (ou framework simple comme Vue.js)</li> <li>API: Mistral AI</li> <li>Base de connaissances: JSON structur\u00e9 ou base NoSQL</li> </ul> </li> <li> <p>Planification</p> <ul> <li>S\u00e9ance 4, Phase 1 (2h30): D\u00e9veloppement du chatbot</li> <li>S\u00e9ance 4, Phase 2 (1h): Finalisation et tests</li> <li>S\u00e9ance 4, Phase 3 (30min): Pr\u00e9sentation des projets</li> </ul> </li> </ol>"},{"location":"module3/preparation-projet/#conclusion","title":"Conclusion","text":"<p>Cette phase vous a permis de comprendre le cahier des charges d\u00e9taill\u00e9 de votre projet de chatbot p\u00e9dagogique, d'explorer les possibilit\u00e9s de l'API Mistral AI, et de vous pr\u00e9parer au d\u00e9veloppement.</p> <p>Lors de la prochaine s\u00e9ance, vous passerez \u00e0 l'impl\u00e9mentation concr\u00e8te de votre chatbot. D'ici l\u00e0, nous vous recommandons de: - Vous familiariser davantage avec l'API Mistral AI - R\u00e9fl\u00e9chir \u00e0 la structure de votre base de connaissances - Explorer des exemples de chatbots \u00e9ducatifs existants</p> <p>Retour au Module 3 Continuer vers le Module 4</p>"},{"location":"module3/qcm-evaluation-module3/","title":"\ud83d\udcdd Auto-\u00e9valuation du Module 3:  D\u00e9veloppement d'applications pratiques","text":"<p>Ce QCM vous permettra d'\u00e9valuer votre compr\u00e9hension des frameworks, de l'optimisation et de l'int\u00e9gration des mod\u00e8les de Deep Learning dans des applications concr\u00e8tes.</p>"},{"location":"module3/qcm-evaluation-module3/#instructions","title":"Instructions","text":"<ul> <li>Cochez la ou les r\u00e9ponses correctes pour chaque question</li> <li>Certaines questions peuvent avoir plusieurs r\u00e9ponses correctes</li> <li>\u00c0 la fin du questionnaire, r\u00e9pondez aux questions \u00e0 r\u00e9ponse courte et compl\u00e9tez l'exercice pratique</li> <li>Calculez votre score gr\u00e2ce au corrig\u00e9 fourni</li> <li>Dur\u00e9e recommand\u00e9e : 20 minutes</li> </ul>"},{"location":"module3/qcm-evaluation-module3/#partie-a-qcm-frameworks-et-optimisation-15-points","title":"Partie A: QCM : Frameworks et optimisation (15 points)","text":"<p>Pour chaque question, cochez la ou les r\u00e9ponses correctes.</p>"},{"location":"module3/qcm-evaluation-module3/#1-parmi-ces-frameworks-de-deep-learning-lequel-est-le-plus-adapte-pour-le-deploiement-en-production-sur-des-appareils-mobiles","title":"1. Parmi ces frameworks de Deep Learning, lequel est le plus adapt\u00e9 pour le d\u00e9ploiement en production sur des appareils mobiles ?","text":"<ul> <li> a) PyTorch</li> <li> b) TensorFlow/Keras</li> <li> c) Scikit-learn</li> <li> d) Theano</li> </ul>"},{"location":"module3/qcm-evaluation-module3/#2-quels-sont-les-avantages-des-modeles-pre-entraines-plusieurs-reponses-possibles","title":"2. Quels sont les avantages des mod\u00e8les pr\u00e9-entra\u00een\u00e9s ? (plusieurs r\u00e9ponses possibles)","text":"<ul> <li> a) R\u00e9duction du temps de d\u00e9veloppement</li> <li> b) Besoin de moins de donn\u00e9es d'entra\u00eenement</li> <li> c) Poids plus petits que les mod\u00e8les entra\u00een\u00e9s from scratch</li> <li> d) Meilleures performances sur des datasets limit\u00e9s</li> </ul>"},{"location":"module3/qcm-evaluation-module3/#3-la-quantification-dun-modele-consiste-a","title":"3. La quantification d'un mod\u00e8le consiste \u00e0 :","text":"<ul> <li> a) R\u00e9duire le nombre de couches du mod\u00e8le</li> <li> b) R\u00e9duire la pr\u00e9cision des poids (ex: float32 \u2192 int8)</li> <li> c) Supprimer les poids proches de z\u00e9ro</li> <li> d) Combiner plusieurs mod\u00e8les ensemble</li> </ul>"},{"location":"module3/qcm-evaluation-module3/#4-quelle-technique-doptimisation-consiste-a-supprimer-les-connexions-les-moins-importantes-dans-un-reseau","title":"4. Quelle technique d'optimisation consiste \u00e0 supprimer les connexions les moins importantes dans un r\u00e9seau ?","text":"<ul> <li> a) Quantification</li> <li> b) \u00c9lagage (pruning)</li> <li> c) Distillation</li> <li> d) Factorisation matricielle</li> </ul>"},{"location":"module3/qcm-evaluation-module3/#5-quel-format-est-generalement-utilise-pour-deployer-des-modeles-sur-des-appareils-mobiles","title":"5. Quel format est g\u00e9n\u00e9ralement utilis\u00e9 pour d\u00e9ployer des mod\u00e8les sur des appareils mobiles ?","text":"<ul> <li> a) HDF5</li> <li> b) SavedModel</li> <li> c) TensorFlow Lite</li> <li> d) ONNX</li> </ul>"},{"location":"module3/qcm-evaluation-module3/#6-dans-une-api-de-deep-learning-quelle-technique-est-recommandee-pour-ameliorer-les-performances","title":"6. Dans une API de Deep Learning, quelle technique est recommand\u00e9e pour am\u00e9liorer les performances ?","text":"<ul> <li> a) Recharger le mod\u00e8le \u00e0 chaque requ\u00eate</li> <li> b) Convertir les images en CSV avant traitement</li> <li> c) Mettre en cache le mod\u00e8le en m\u00e9moire</li> <li> d) D\u00e9sactiver la gestion d'erreurs</li> </ul>"},{"location":"module3/qcm-evaluation-module3/#7-quelle-architecture-de-modele-est-specifiquement-concue-pour-etre-legere-et-efficace","title":"7. Quelle architecture de mod\u00e8le est sp\u00e9cifiquement con\u00e7ue pour \u00eatre l\u00e9g\u00e8re et efficace ?","text":"<ul> <li> a) VGG16</li> <li> b) MobileNetV2</li> <li> c) ResNet152</li> <li> d) InceptionV4</li> </ul>"},{"location":"module3/qcm-evaluation-module3/#8-la-distillation-de-connaissances-consiste-a","title":"8. La distillation de connaissances consiste \u00e0 :","text":"<ul> <li> a) Extraire les poids d'un mod\u00e8le pour les analyser</li> <li> b) Entra\u00eener un plus petit mod\u00e8le (\u00e9l\u00e8ve) \u00e0 imiter un plus grand mod\u00e8le (enseignant)</li> <li> c) D\u00e9composer un gros mod\u00e8le en plusieurs petits</li> <li> d) Fusionner plusieurs mod\u00e8les sp\u00e9cialis\u00e9s</li> </ul>"},{"location":"module3/qcm-evaluation-module3/#9-lors-de-lintegration-dun-modele-dans-une-application-web-quelle-affirmation-est-correcte","title":"9. Lors de l'int\u00e9gration d'un mod\u00e8le dans une application web, quelle affirmation est correcte ?","text":"<ul> <li> a) Le mod\u00e8le doit toujours \u00eatre ex\u00e9cut\u00e9 c\u00f4t\u00e9 client (JavaScript)</li> <li> b) Les pr\u00e9dictions doivent \u00eatre trait\u00e9es de mani\u00e8re synchrone</li> <li> c) Il est recommand\u00e9 de pr\u00e9traiter les images c\u00f4t\u00e9 client avant envoi</li> <li> d) L'API du mod\u00e8le ne n\u00e9cessite pas de documentation si elle est utilis\u00e9e en interne</li> </ul>"},{"location":"module3/qcm-evaluation-module3/#10-parmi-ces-parametres-lequel-peut-etre-ajuste-pour-rendre-les-reponses-dun-modele-de-langage-plus-deterministes-moins-creatives","title":"10. Parmi ces param\u00e8tres, lequel peut \u00eatre ajust\u00e9 pour rendre les r\u00e9ponses d'un mod\u00e8le de langage plus d\u00e9terministes (moins cr\u00e9atives) ?","text":"<ul> <li> a) Augmenter la temp\u00e9rature</li> <li> b) Diminuer la temp\u00e9rature</li> <li> c) Augmenter le nombre maximum de tokens</li> <li> d) Activer le mode streaming</li> </ul>"},{"location":"module3/qcm-evaluation-module3/#partie-b-questions-a-reponse-courte-10-points","title":"Partie B: Questions \u00e0 r\u00e9ponse courte (10 points)","text":"<p>R\u00e9pondez bri\u00e8vement aux questions suivantes (2-3 phrases par question).</p>"},{"location":"module3/qcm-evaluation-module3/#11-expliquez-pourquoi-loptimisation-des-modeles-de-deep-learning-est-importante-dans-un-contexte-professionnel","title":"11. Expliquez pourquoi l'optimisation des mod\u00e8les de Deep Learning est importante dans un contexte professionnel.","text":"<p>...................................................................</p> <p>...................................................................</p>"},{"location":"module3/qcm-evaluation-module3/#12-decrivez-deux-differences-principales-entre-tensorflow-et-pytorch","title":"12. D\u00e9crivez deux diff\u00e9rences principales entre TensorFlow et PyTorch.","text":"<p>...................................................................</p> <p>...................................................................</p>"},{"location":"module3/qcm-evaluation-module3/#13-quels-sont-les-avantages-et-inconvenients-de-la-quantification-post-entrainement","title":"13. Quels sont les avantages et inconv\u00e9nients de la quantification post-entra\u00eenement ?","text":"<p>...................................................................</p> <p>...................................................................</p>"},{"location":"module3/qcm-evaluation-module3/#14-comment-lapi-mistral-ai-peut-elle-etre-utilisee-pour-creer-un-chatbot-pedagogique","title":"14. Comment l'API Mistral AI peut-elle \u00eatre utilis\u00e9e pour cr\u00e9er un chatbot p\u00e9dagogique ?","text":"<p>...................................................................</p> <p>...................................................................</p>"},{"location":"module3/qcm-evaluation-module3/#15-quelles-sont-les-deux-bonnes-pratiques-essentielles-pour-securiser-une-api-de-reconnaissance-dimages","title":"15. Quelles sont les deux bonnes pratiques essentielles pour s\u00e9curiser une API de reconnaissance d'images ?","text":"<p>...................................................................</p> <p>...................................................................</p>"},{"location":"module3/qcm-evaluation-module3/#partie-c-exercice-pratique-optimisation-dun-modele-15-points","title":"Partie C: Exercice pratique : Optimisation d'un mod\u00e8le (15 points)","text":"<p>Compl\u00e9tez le code suivant pour optimiser un mod\u00e8le MobileNetV2 avec la quantification TensorFlow Lite.</p> <pre><code>import tensorflow as tf\nfrom tensorflow.keras.applications import MobileNetV2\nimport numpy as np\n\n# Chargement du mod\u00e8le pr\u00e9-entra\u00een\u00e9\nbase_model = MobileNetV2(weights='imagenet', include_top=True)\n\n# 1. Convertir le mod\u00e8le en TensorFlow Lite\nconverter = tf.lite.TFLiteConverter.from_keras_model(base_model)\ntflite_model = ................................. # \u00c0 compl\u00e9ter\n\n# 2. Appliquer la quantification post-entra\u00eenement\nconverter = tf.lite.TFLiteConverter.from_keras_model(base_model)\nconverter.optimizations = ........................... # \u00c0 compl\u00e9ter\nquantized_model = converter.convert()\n\n# 3. Comparer les tailles des mod\u00e8les\noriginal_size = .................. # \u00c0 compl\u00e9ter : calculer la taille du mod\u00e8le original\ntflite_size = len(tflite_model) / (1024 * 1024)  # Taille en Mo\nquantized_size = len(quantized_model) / (1024 * 1024)  # Taille en Mo\n\nprint(f\"Taille du mod\u00e8le original: {original_size:.2f} Mo\")\nprint(f\"Taille du mod\u00e8le TFLite: {tflite_size:.2f} Mo\")\nprint(f\"Taille du mod\u00e8le quantifi\u00e9: {quantized_size:.2f} Mo\")\nprint(f\"R\u00e9duction de taille: {(1 - quantized_size/original_size) * 100:.2f}%\")\n\n# 4. Fonction pour pr\u00e9traiter une image pour l'inf\u00e9rence\ndef preprocess_image(image_path):\n    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))\n    img_array = .................. # \u00c0 compl\u00e9ter : convertir l'image en tableau\n    img_array = np.expand_dims(img_array, axis=0)\n    return tf.keras.applications.mobilenet_v2.preprocess_input(img_array)\n\n# 5. Cr\u00e9ation d'un interpr\u00e9teur TFLite\ninterpreter = tf.lite.Interpreter(model_content=quantized_model)\n....................... # \u00c0 compl\u00e9ter : allouer les tenseurs\n\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# 6. Fonction d'inf\u00e9rence avec le mod\u00e8le quantifi\u00e9\ndef predict_with_tflite(image_path):\n    # Pr\u00e9traitement de l'image\n    input_data = preprocess_image(image_path)\n\n    # D\u00e9finir les donn\u00e9es d'entr\u00e9e\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Ex\u00e9cuter l'inf\u00e9rence\n    ...................... # \u00c0 compl\u00e9ter : invoquer l'interpr\u00e9teur\n\n    # Obtenir les r\u00e9sultats\n    output_data = interpreter.get_tensor(output_details[0]['index'])\n\n    # Traiter les r\u00e9sultats\n    results = tf.keras.applications.mobilenet_v2.decode_predictions(output_data)\n    return results[0]\n</code></pre>"},{"location":"module3/qcm-evaluation-module3/#partie-d-exercice-de-reflexion-cas-pratique-dintegration-10-points","title":"Partie D: Exercice de r\u00e9flexion : Cas pratique d'int\u00e9gration (10 points)","text":"<p>Vous \u00eates d\u00e9veloppeur dans une petite entreprise qui propose des solutions de reconnaissance d'objets pour le commerce de d\u00e9tail. On vous demande de cr\u00e9er une API qui permettra d'identifier les produits \u00e0 partir de photos prises par les employ\u00e9s sur leurs smartphones.</p> <ol> <li>Quelle architecture de mod\u00e8le choisiriez-vous et pourquoi ? (2 points)</li> </ol> <p>...................................................................</p> <p>...................................................................</p> <ol> <li>Quelles techniques d'optimisation mettriez-vous en place ? (2 points)</li> </ol> <p>...................................................................</p> <p>...................................................................</p> <ol> <li>Comment structureriez-vous votre API REST ? D\u00e9crivez les endpoints et leurs param\u00e8tres. (3 points)</li> </ol> <p>...................................................................</p> <p>...................................................................</p> <p>...................................................................</p> <ol> <li>Quelles mesures de s\u00e9curit\u00e9 impl\u00e9menteriez-vous ? (3 points)</li> </ol> <p>...................................................................</p> <p>...................................................................</p> <p>...................................................................</p>"},{"location":"module3/qcm-evaluation-module3/#corrige","title":"Corrig\u00e9","text":""},{"location":"module3/qcm-evaluation-module3/#qcm","title":"QCM","text":"<ol> <li>b) TensorFlow/Keras</li> <li>a) b) d)</li> <li>b) R\u00e9duire la pr\u00e9cision des poids (ex: float32 \u2192 int8)</li> <li>b) \u00c9lagage (pruning)</li> <li>c) TensorFlow Lite</li> <li>c) Mettre en cache le mod\u00e8le en m\u00e9moire</li> <li>b) MobileNetV2</li> <li>b) Entra\u00eener un plus petit mod\u00e8le (\u00e9l\u00e8ve) \u00e0 imiter un plus grand mod\u00e8le (enseignant)</li> <li>c) Il est recommand\u00e9 de pr\u00e9traiter les images c\u00f4t\u00e9 client avant envoi</li> <li>b) Diminuer la temp\u00e9rature</li> </ol>"},{"location":"module3/qcm-evaluation-module3/#questions-a-reponse-courte-elements-attendus","title":"Questions \u00e0 r\u00e9ponse courte (\u00e9l\u00e9ments attendus)","text":"<ol> <li> <p>L'optimisation permet de r\u00e9duire les co\u00fbts d'infrastructure, diminuer la latence pour une meilleure exp\u00e9rience utilisateur, \u00e9conomiser l'\u00e9nergie (crucial pour les appareils mobiles) et rendre les mod\u00e8les accessibles sur des appareils \u00e0 ressources limit\u00e9es.</p> </li> <li> <p>TensorFlow utilise des graphes statiques (plus efficaces en production) tandis que PyTorch utilise des graphes dynamiques (plus flexibles pour la recherche). TensorFlow a un \u00e9cosyst\u00e8me plus complet pour le d\u00e9ploiement (TFLite, TF Serving) alors que PyTorch est g\u00e9n\u00e9ralement consid\u00e9r\u00e9 comme plus intuitif pour le d\u00e9veloppement.</p> </li> <li> <p>Avantages : R\u00e9duction significative de la taille du mod\u00e8le (jusqu'\u00e0 4x), acc\u00e9l\u00e9ration de l'inf\u00e9rence, pas besoin de r\u00e9entra\u00eenement. Inconv\u00e9nients : Perte potentielle de pr\u00e9cision, surtout pour les t\u00e2ches complexes, incompatibilit\u00e9 avec certaines op\u00e9rations avanc\u00e9es.</p> </li> <li> <p>L'API Mistral AI peut \u00eatre utilis\u00e9e pour cr\u00e9er un chatbot p\u00e9dagogique en envoyant des requ\u00eates avec un prompt syst\u00e8me adapt\u00e9 \u00e0 l'enseignement, en enrichissant les prompts avec une base de connaissances sp\u00e9cifique au domaine enseign\u00e9, et en maintenant un contexte conversationnel pour assurer la coh\u00e9rence des \u00e9changes.</p> </li> <li> <p>Validation des entr\u00e9es (v\u00e9rification du format et de la taille des images), limitation du taux de requ\u00eates (rate limiting), authentification par cl\u00e9 API, sanitization des chemins de fichiers, restriction des types MIME accept\u00e9s.</p> </li> </ol>"},{"location":"module3/qcm-evaluation-module3/#exercice-pratique-elements-de-correction","title":"Exercice pratique (\u00e9l\u00e9ments de correction)","text":"<pre><code># 1. Convertir le mod\u00e8le en TensorFlow Lite\ntflite_model = converter.convert()\n\n# 2. Appliquer la quantification post-entra\u00eenement\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# 3. Comparer les tailles des mod\u00e8les\noriginal_size = sum(np.prod(w.shape) * w.dtype.size for w in base_model.weights) / (1024 * 1024)\n\n# 4. Fonction pour pr\u00e9traiter une image pour l'inf\u00e9rence\nimg_array = tf.keras.preprocessing.image.img_to_array(img)\n\n# 5. Cr\u00e9ation d'un interpr\u00e9teur TFLite\ninterpreter.allocate_tensors()\n\n# 6. Fonction d'inf\u00e9rence avec le mod\u00e8le quantifi\u00e9\ninterpreter.invoke()\n</code></pre>"},{"location":"module3/qcm-evaluation-module3/#exercice-de-reflexion","title":"Exercice de r\u00e9flexion","text":"<p>Les r\u00e9ponses peuvent varier, mais devraient inclure des points comme:</p> <ol> <li> <p>Architecture: MobileNetV2 ou EfficientNet serait un bon choix car ils offrent un bon \u00e9quilibre entre pr\u00e9cision et performance, sont optimis\u00e9s pour les appareils mobiles, et peuvent \u00eatre facilement affin\u00e9s pour des t\u00e2ches sp\u00e9cifiques.</p> </li> <li> <p>Techniques d'optimisation: Quantification post-entra\u00eenement pour r\u00e9duire la taille du mod\u00e8le, transfer learning sur un petit dataset de produits sp\u00e9cifiques, et \u00e9ventuellement pruning pour r\u00e9duire davantage la taille.</p> </li> <li> <p>Structure API REST:</p> </li> <li>POST /predict - Pour envoyer une image et recevoir des pr\u00e9dictions</li> <li>GET /categories - Pour r\u00e9cup\u00e9rer la liste des cat\u00e9gories de produits</li> <li> <p>POST /feedback - Pour recueillir les retours sur les pr\u00e9dictions incorrectes    Param\u00e8tres pour /predict: image (fichier ou base64), top_k (nombre de pr\u00e9dictions), confidence_threshold.</p> </li> <li> <p>S\u00e9curit\u00e9:</p> </li> <li>Authentification par cl\u00e9 API</li> <li>Rate limiting pour pr\u00e9venir les abus</li> <li>Validation des entr\u00e9es (taille et format d'image)</li> <li>HTTPS pour le chiffrement des donn\u00e9es</li> <li>Logging s\u00e9curis\u00e9 pour l'audit</li> </ol>"},{"location":"module3/qcm-evaluation-module3/#bareme-et-auto-evaluation","title":"Bar\u00e8me et auto-\u00e9valuation","text":""},{"location":"module3/qcm-evaluation-module3/#calcul-de-votre-score","title":"Calcul de votre score","text":"<p>Partie A : 1 point par r\u00e9ponse correcte = 10 points max Partie B : 2 points par r\u00e9ponse correcte = 10 points max Partie C : 15 points pour l'exercice compl\u00e9t\u00e9 correctement Partie D : 10 points pour les r\u00e9ponses pertinentes</p>"},{"location":"module3/qcm-evaluation-module3/#total-des-points-possibles-45","title":"Total des points possibles : 45","text":"<p>Interpr\u00e9tation</p> <p>35-45 points : Excellente ma\u00eetrise des concepts d'int\u00e9gration et d'optimisation des mod\u00e8les 24-35 points : Bonne compr\u00e9hension, certains aspects \u00e0 approfondir 16-23 points : Compr\u00e9hension de base, n\u00e9cessite une r\u00e9vision approfondie 0-15 points : R\u00e9vision compl\u00e8te recommand\u00e9e avant de poursuivre</p>"},{"location":"module3/qcm-evaluation-module3/#questions-pour-approfondir","title":"Questions pour approfondir","text":"<p>Si vous avez obtenu un bon score, vous pouvez explorer ces questions pour aller plus loin :</p> <ol> <li>Comment impl\u00e9menteriez-vous un syst\u00e8me de mise \u00e0 jour progressive des mod\u00e8les en production ?</li> <li>Quelles strat\u00e9gies pourriez-vous utiliser pour g\u00e9rer les biais potentiels dans un mod\u00e8le de vision par ordinateur ?</li> <li>Comment adapter l'architecture d'une API de Deep Learning pour g\u00e9rer des millions de requ\u00eates par jour ?</li> <li>Quelles techniques permettraient d'optimiser les prompts pour un mod\u00e8le de langage au-del\u00e0 des exemples vus dans ce module ?</li> </ol> <p>Retour au Module 3 Continuer vers le Module 4</p>"},{"location":"module3/api-vetements-ia/app/","title":"App","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nApplication principale pour l'API de recherche par image de v\u00eatements\n\"\"\"\n</pre> \"\"\" Application principale pour l'API de recherche par image de v\u00eatements \"\"\" In\u00a0[\u00a0]: Copied! <pre>from flask import Flask, request, jsonify, render_template, url_for\nimport os\nimport numpy as np\nimport time\nfrom PIL import Image\nimport io\nfrom config import Config\nfrom models.classifier import ClothingClassifier\nfrom utils.image_utils import preprocess_image\n</pre> from flask import Flask, request, jsonify, render_template, url_for import os import numpy as np import time from PIL import Image import io from config import Config from models.classifier import ClothingClassifier from utils.image_utils import preprocess_image In\u00a0[\u00a0]: Copied! <pre># Initialisation de l'application Flask\napp = Flask(__name__)\napp.config.from_object(Config)\n</pre> # Initialisation de l'application Flask app = Flask(__name__) app.config.from_object(Config) In\u00a0[\u00a0]: Copied! <pre># Dictionnaire des cat\u00e9gories de v\u00eatements\nCATEGORIES = {\n    0: \"T-shirt/top\",\n    1: \"Pantalon\",\n    2: \"Pull-over\",\n    3: \"Robe\",\n    4: \"Manteau\",\n    5: \"Sandale\",\n    6: \"Chemise\",\n    7: \"Sneaker\",\n    8: \"Sac\",\n    9: \"Bottine\"\n}\n</pre> # Dictionnaire des cat\u00e9gories de v\u00eatements CATEGORIES = {     0: \"T-shirt/top\",     1: \"Pantalon\",     2: \"Pull-over\",     3: \"Robe\",     4: \"Manteau\",     5: \"Sandale\",     6: \"Chemise\",     7: \"Sneaker\",     8: \"Sac\",     9: \"Bottine\" } In\u00a0[\u00a0]: Copied! <pre># Initialisation du mod\u00e8le (singleton pour \u00e9viter de le recharger \u00e0 chaque requ\u00eate)\nclassifier = ClothingClassifier()\n</pre> # Initialisation du mod\u00e8le (singleton pour \u00e9viter de le recharger \u00e0 chaque requ\u00eate) classifier = ClothingClassifier() In\u00a0[\u00a0]: Copied! <pre>@app.route('/')\ndef index():\n    \"\"\"Page d'accueil avec interface de test\"\"\"\n    return render_template('index.html')\n</pre> @app.route('/') def index():     \"\"\"Page d'accueil avec interface de test\"\"\"     return render_template('index.html') In\u00a0[\u00a0]: Copied! <pre>@app.route('/api/predict', methods=['POST'])\ndef predict():\n    \"\"\"\n    API endpoint pour classifier une image de v\u00eatement\n    \n    Accepte:\n    - Fichier image via formulaire multipart\n    - Image encod\u00e9e en base64 via JSON\n    \n    Retourne:\n    - JSON avec pr\u00e9dictions et scores\n    \"\"\"\n    # Variables pour le timing\n    start_time = time.time()\n    \n    # V\u00e9rification de la pr\u00e9sence d'une image\n    if 'image' in request.files:\n        file = request.files['image']\n        if file.filename == '':\n            return jsonify({'error': 'Aucun fichier s\u00e9lectionn\u00e9'}), 400\n            \n        # Lecture de l'image\n        image = Image.open(file.stream)\n    \n    elif request.json and 'image' in request.json:\n        # D\u00e9codage de l'image base64\n        try:\n            image_data = request.json['image'].split(',')[1]\n            image = Image.open(io.BytesIO(base64.b64decode(image_data)))\n        except Exception as e:\n            return jsonify({'error': f'Erreur de d\u00e9codage: {str(e)}'}), 400\n    \n    else:\n        return jsonify({'error': 'Aucune image fournie'}), 400\n    \n    # Pr\u00e9traitement de l'image\n    try:\n        processed_image = preprocess_image(image)\n    except Exception as e:\n        return jsonify({'error': f'Erreur de pr\u00e9traitement: {str(e)}'}), 400\n    \n    # Pr\u00e9diction\n    try:\n        predictions = classifier.predict(processed_image)\n        \n        # Formatage des r\u00e9sultats\n        results = []\n        for i in np.argsort(predictions[0])[-3:][::-1]:  # Top 3 des pr\u00e9dictions\n            results.append({\n                'category': CATEGORIES[i],\n                'category_id': int(i),\n                'confidence': float(predictions[0][i])\n            })\n        \n        # Temps d'ex\u00e9cution\n        processing_time = time.time() - start_time\n        \n        return jsonify({\n            'results': results,\n            'processing_time_ms': round(processing_time * 1000, 2)\n        })\n        \n    except Exception as e:\n        return jsonify({'error': f'Erreur de pr\u00e9diction: {str(e)}'}), 500\n</pre> @app.route('/api/predict', methods=['POST']) def predict():     \"\"\"     API endpoint pour classifier une image de v\u00eatement          Accepte:     - Fichier image via formulaire multipart     - Image encod\u00e9e en base64 via JSON          Retourne:     - JSON avec pr\u00e9dictions et scores     \"\"\"     # Variables pour le timing     start_time = time.time()          # V\u00e9rification de la pr\u00e9sence d'une image     if 'image' in request.files:         file = request.files['image']         if file.filename == '':             return jsonify({'error': 'Aucun fichier s\u00e9lectionn\u00e9'}), 400                      # Lecture de l'image         image = Image.open(file.stream)          elif request.json and 'image' in request.json:         # D\u00e9codage de l'image base64         try:             image_data = request.json['image'].split(',')[1]             image = Image.open(io.BytesIO(base64.b64decode(image_data)))         except Exception as e:             return jsonify({'error': f'Erreur de d\u00e9codage: {str(e)}'}), 400          else:         return jsonify({'error': 'Aucune image fournie'}), 400          # Pr\u00e9traitement de l'image     try:         processed_image = preprocess_image(image)     except Exception as e:         return jsonify({'error': f'Erreur de pr\u00e9traitement: {str(e)}'}), 400          # Pr\u00e9diction     try:         predictions = classifier.predict(processed_image)                  # Formatage des r\u00e9sultats         results = []         for i in np.argsort(predictions[0])[-3:][::-1]:  # Top 3 des pr\u00e9dictions             results.append({                 'category': CATEGORIES[i],                 'category_id': int(i),                 'confidence': float(predictions[0][i])             })                  # Temps d'ex\u00e9cution         processing_time = time.time() - start_time                  return jsonify({             'results': results,             'processing_time_ms': round(processing_time * 1000, 2)         })              except Exception as e:         return jsonify({'error': f'Erreur de pr\u00e9diction: {str(e)}'}), 500 In\u00a0[\u00a0]: Copied! <pre>@app.route('/api/health', methods=['GET'])\ndef health_check():\n    \"\"\"Endpoint de v\u00e9rification de l'\u00e9tat de l'API\"\"\"\n    return jsonify({\n        'status': 'healthy',\n        'model_loaded': classifier.is_loaded(),\n        'version': '1.0.0'\n    })\n</pre> @app.route('/api/health', methods=['GET']) def health_check():     \"\"\"Endpoint de v\u00e9rification de l'\u00e9tat de l'API\"\"\"     return jsonify({         'status': 'healthy',         'model_loaded': classifier.is_loaded(),         'version': '1.0.0'     }) In\u00a0[\u00a0]: Copied! <pre>if __name__ == '__main__':\n    app.run(debug=app.config['DEBUG'], port=app.config['PORT'])\n</pre> if __name__ == '__main__':     app.run(debug=app.config['DEBUG'], port=app.config['PORT'])"},{"location":"module3/api-vetements-ia/config/","title":"Config","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nConfiguration de l'application\n\"\"\"\n</pre> \"\"\" Configuration de l'application \"\"\" In\u00a0[\u00a0]: Copied! <pre>import os\n</pre> import os In\u00a0[\u00a0]: Copied! <pre>class Config:\n    \"\"\"Configuration de base pour l'application Flask\"\"\"\n    # Flask\n    DEBUG = os.environ.get('DEBUG', 'True') == 'True'\n    PORT = int(os.environ.get('PORT', 5000))\n    \n    # S\u00e9curit\u00e9\n    SECRET_KEY = os.environ.get('SECRET_KEY', 'cl\u00e9_secr\u00e8te_\u00e0_changer_en_production')\n    \n    # Upload\n    MAX_CONTENT_LENGTH = 5 * 1024 * 1024  # 5 MB max\n    UPLOAD_FOLDER = os.path.join(os.path.dirname(__file__), 'uploads')\n    ALLOWED_EXTENSIONS = {'jpg', 'jpeg', 'png', 'gif'}\n    \n    # Mod\u00e8le\n    MODEL_PATH = os.environ.get(\n        'MODEL_PATH', \n        os.path.join(os.path.dirname(__file__), 'models', 'pretrained_model', 'mobilenet_clothing_model.h5')\n    )\n    DEFAULT_MODEL = 'mobilenetv2'\n    MODEL_INPUT_SIZE = (224, 224)\n    \n    # Performance\n    CACHE_TIMEOUT = 3600  # 1 heure\n    BATCH_SIZE = 4        # Traitement par lots\n    \n    # M\u00e9tier\n    CATEGORIES = {\n        0: \"T-shirt/top\",\n        1: \"Pantalon\",\n        2: \"Pull-over\",\n        3: \"Robe\",\n        4: \"Manteau\",\n        5: \"Sandale\",\n        6: \"Chemise\",\n        7: \"Sneaker\",\n        8: \"Sac\",\n        9: \"Bottine\"\n    }\n</pre> class Config:     \"\"\"Configuration de base pour l'application Flask\"\"\"     # Flask     DEBUG = os.environ.get('DEBUG', 'True') == 'True'     PORT = int(os.environ.get('PORT', 5000))          # S\u00e9curit\u00e9     SECRET_KEY = os.environ.get('SECRET_KEY', 'cl\u00e9_secr\u00e8te_\u00e0_changer_en_production')          # Upload     MAX_CONTENT_LENGTH = 5 * 1024 * 1024  # 5 MB max     UPLOAD_FOLDER = os.path.join(os.path.dirname(__file__), 'uploads')     ALLOWED_EXTENSIONS = {'jpg', 'jpeg', 'png', 'gif'}          # Mod\u00e8le     MODEL_PATH = os.environ.get(         'MODEL_PATH',          os.path.join(os.path.dirname(__file__), 'models', 'pretrained_model', 'mobilenet_clothing_model.h5')     )     DEFAULT_MODEL = 'mobilenetv2'     MODEL_INPUT_SIZE = (224, 224)          # Performance     CACHE_TIMEOUT = 3600  # 1 heure     BATCH_SIZE = 4        # Traitement par lots          # M\u00e9tier     CATEGORIES = {         0: \"T-shirt/top\",         1: \"Pantalon\",         2: \"Pull-over\",         3: \"Robe\",         4: \"Manteau\",         5: \"Sandale\",         6: \"Chemise\",         7: \"Sneaker\",         8: \"Sac\",         9: \"Bottine\"     }"},{"location":"module3/api-vetements-ia/models/__init__/","title":"init","text":""},{"location":"module3/api-vetements-ia/models/classifier/","title":"Classifier","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nClasse pour la classification d'images de v\u00eatements avec un mod\u00e8le pr\u00e9-entra\u00een\u00e9\n\"\"\"\n</pre> \"\"\" Classe pour la classification d'images de v\u00eatements avec un mod\u00e8le pr\u00e9-entra\u00een\u00e9 \"\"\" In\u00a0[\u00a0]: Copied! <pre>import tensorflow as tf\nimport numpy as np\nimport os\nfrom tensorflow.keras.models import load_model\nfrom utils.model_utils import optimize_model_for_inference\n</pre> import tensorflow as tf import numpy as np import os from tensorflow.keras.models import load_model from utils.model_utils import optimize_model_for_inference In\u00a0[\u00a0]: Copied! <pre>class ClothingClassifier:\n    \"\"\"Classificateur de v\u00eatements bas\u00e9 sur MobileNetV2\"\"\"\n    \n    def __init__(self, model_path=None):\n        \"\"\"\n        Initialise le classificateur\n        \n        Args:\n            model_path: Chemin vers le mod\u00e8le pr\u00e9-entra\u00een\u00e9 (optionnel)\n        \"\"\"\n        self.model = None\n        self.model_path = model_path or os.path.join(\n            os.path.dirname(__file__), \n            'pretrained_model', \n            'mobilenet_clothing_model.h5'\n        )\n        self.input_shape = (224, 224, 3)\n        self.load_model()\n    \n    def load_model(self):\n        \"\"\"Charge le mod\u00e8le pr\u00e9-entra\u00een\u00e9\"\"\"\n        try:\n            # Chargement du mod\u00e8le\n            print(f\"Chargement du mod\u00e8le depuis {self.model_path}...\")\n            self.model = load_model(self.model_path)\n            \n            # Optimisation du mod\u00e8le pour l'inf\u00e9rence\n            self.model = optimize_model_for_inference(self.model)\n            \n            # Pr\u00e9paration du mod\u00e8le avec une pr\u00e9diction sur des donn\u00e9es factices\n            dummy_input = np.zeros((1, *self.input_shape))\n            _ = self.model.predict(dummy_input)\n            \n            print(\"Mod\u00e8le charg\u00e9 avec succ\u00e8s!\")\n            return True\n            \n        except Exception as e:\n            print(f\"Erreur lors du chargement du mod\u00e8le: {e}\")\n            \n            # Chargement d'un mod\u00e8le par d\u00e9faut si le mod\u00e8le personnalis\u00e9 \u00e9choue\n            try:\n                print(\"Tentative de chargement du mod\u00e8le MobileNetV2 pr\u00e9-entra\u00een\u00e9...\")\n                base_model = tf.keras.applications.MobileNetV2(\n                    input_shape=self.input_shape,\n                    include_top=True,\n                    weights='imagenet'\n                )\n                self.model = base_model\n                print(\"Mod\u00e8le MobileNetV2 charg\u00e9 comme solution de repli.\")\n                return True\n            except Exception as e2:\n                print(f\"\u00c9chec du chargement du mod\u00e8le de repli: {e2}\")\n                return False\n    \n    def predict(self, image_array):\n        \"\"\"\n        Pr\u00e9diction sur une image pr\u00e9trait\u00e9e\n        \n        Args:\n            image_array: Image pr\u00e9trait\u00e9e au format numpy array\n            \n        Returns:\n            np.array: Pr\u00e9dictions pour chaque classe\n        \"\"\"\n        if self.model is None:\n            raise ValueError(\"Le mod\u00e8le n'est pas charg\u00e9\")\n            \n        # V\u00e9rification de la forme de l'entr\u00e9e\n        if len(image_array.shape) != 4:\n            image_array = np.expand_dims(image_array, axis=0)\n            \n        # Pr\u00e9diction\n        predictions = self.model.predict(image_array)\n        return predictions\n    \n    def is_loaded(self):\n        \"\"\"V\u00e9rifie si le mod\u00e8le est charg\u00e9\"\"\"\n        return self.model is not None\n</pre> class ClothingClassifier:     \"\"\"Classificateur de v\u00eatements bas\u00e9 sur MobileNetV2\"\"\"          def __init__(self, model_path=None):         \"\"\"         Initialise le classificateur                  Args:             model_path: Chemin vers le mod\u00e8le pr\u00e9-entra\u00een\u00e9 (optionnel)         \"\"\"         self.model = None         self.model_path = model_path or os.path.join(             os.path.dirname(__file__),              'pretrained_model',              'mobilenet_clothing_model.h5'         )         self.input_shape = (224, 224, 3)         self.load_model()          def load_model(self):         \"\"\"Charge le mod\u00e8le pr\u00e9-entra\u00een\u00e9\"\"\"         try:             # Chargement du mod\u00e8le             print(f\"Chargement du mod\u00e8le depuis {self.model_path}...\")             self.model = load_model(self.model_path)                          # Optimisation du mod\u00e8le pour l'inf\u00e9rence             self.model = optimize_model_for_inference(self.model)                          # Pr\u00e9paration du mod\u00e8le avec une pr\u00e9diction sur des donn\u00e9es factices             dummy_input = np.zeros((1, *self.input_shape))             _ = self.model.predict(dummy_input)                          print(\"Mod\u00e8le charg\u00e9 avec succ\u00e8s!\")             return True                      except Exception as e:             print(f\"Erreur lors du chargement du mod\u00e8le: {e}\")                          # Chargement d'un mod\u00e8le par d\u00e9faut si le mod\u00e8le personnalis\u00e9 \u00e9choue             try:                 print(\"Tentative de chargement du mod\u00e8le MobileNetV2 pr\u00e9-entra\u00een\u00e9...\")                 base_model = tf.keras.applications.MobileNetV2(                     input_shape=self.input_shape,                     include_top=True,                     weights='imagenet'                 )                 self.model = base_model                 print(\"Mod\u00e8le MobileNetV2 charg\u00e9 comme solution de repli.\")                 return True             except Exception as e2:                 print(f\"\u00c9chec du chargement du mod\u00e8le de repli: {e2}\")                 return False          def predict(self, image_array):         \"\"\"         Pr\u00e9diction sur une image pr\u00e9trait\u00e9e                  Args:             image_array: Image pr\u00e9trait\u00e9e au format numpy array                      Returns:             np.array: Pr\u00e9dictions pour chaque classe         \"\"\"         if self.model is None:             raise ValueError(\"Le mod\u00e8le n'est pas charg\u00e9\")                      # V\u00e9rification de la forme de l'entr\u00e9e         if len(image_array.shape) != 4:             image_array = np.expand_dims(image_array, axis=0)                      # Pr\u00e9diction         predictions = self.model.predict(image_array)         return predictions          def is_loaded(self):         \"\"\"V\u00e9rifie si le mod\u00e8le est charg\u00e9\"\"\"         return self.model is not None"},{"location":"module3/api-vetements-ia/models/pretrained_model/","title":"Index","text":""},{"location":"module3/api-vetements-ia/models/pretrained_model/#4-contenu-mis-a-jour-du-fichier-modelspretrained_modelreadmemd","title":"4. Contenu mis \u00e0 jour du fichier <code>models/pretrained_model/README.md</code>","text":""},{"location":"module3/api-vetements-ia/models/pretrained_model/#modele-pre-entraine-pour-la-classification-de-vetements","title":"Mod\u00e8le pr\u00e9-entra\u00een\u00e9 pour la classification de v\u00eatements","text":""},{"location":"module3/api-vetements-ia/models/pretrained_model/#a-propos-du-modele","title":"\u00c0 propos du mod\u00e8le","text":"<p>Ce dossier contient le mod\u00e8le pr\u00e9-entra\u00een\u00e9 <code>mobilenet_clothing_model.h5</code> utilis\u00e9 par l'application pour classifier les images de v\u00eatements.</p>"},{"location":"module3/api-vetements-ia/models/pretrained_model/#caracteristiques-du-modele","title":"Caract\u00e9ristiques du mod\u00e8le","text":"<ul> <li>Architecture : MobileNetV2 adapt\u00e9 avec transfer learning</li> <li>Jeu de donn\u00e9es : Fashion MNIST (10 cat\u00e9gories de v\u00eatements)</li> <li>Dimensions d'entr\u00e9e : Images 224\u00d7224 pixels RGB</li> <li>Format : H5 (format Keras)</li> <li>Taille : ~14 Mo</li> <li>Pr\u00e9cision : ~92% sur le jeu de test</li> </ul>"},{"location":"module3/api-vetements-ia/models/pretrained_model/#options-disponibles","title":"Options disponibles","text":""},{"location":"module3/api-vetements-ia/models/pretrained_model/#option-1-utiliser-le-modele-inclus","title":"Option 1 : Utiliser le mod\u00e8le inclus","text":"<p>Le mod\u00e8le <code>mobilenet_clothing_model.h5</code> est d\u00e9j\u00e0 inclus dans cette archive et pr\u00eat \u00e0 l'emploi. C'est l'option recommand\u00e9e pour ce TP.</p>"},{"location":"module3/api-vetements-ia/models/pretrained_model/#modele-pre-entraine-pour-la-classification-de-vetements_1","title":"Mod\u00e8le pr\u00e9-entra\u00een\u00e9 pour la classification de v\u00eatements","text":""},{"location":"module3/api-vetements-ia/models/pretrained_model/#a-propos-du-modele_1","title":"\u00c0 propos du mod\u00e8le","text":"<p>Ce dossier doit contenir le mod\u00e8le pr\u00e9-entra\u00een\u00e9 <code>mobilenet_clothing_model.h5</code> utilis\u00e9 par l'application pour classifier les images de v\u00eatements.</p>"},{"location":"module3/api-vetements-ia/models/pretrained_model/#caracteristiques-du-modele_1","title":"Caract\u00e9ristiques du mod\u00e8le","text":"<ul> <li>Architecture : MobileNetV2 adapt\u00e9 avec transfer learning</li> <li>Jeu de donn\u00e9es : Fashion MNIST (10 cat\u00e9gories de v\u00eatements)</li> <li>Dimensions d'entr\u00e9e : Images 224\u00d7224 pixels RGB</li> <li>Format : H5 (format Keras)</li> <li>Taille : ~14 Mo</li> <li>Pr\u00e9cision : ~92% sur le jeu de test</li> </ul>"},{"location":"module3/api-vetements-ia/models/pretrained_model/#obtention-du-modele","title":"Obtention du mod\u00e8le","text":""},{"location":"module3/api-vetements-ia/models/pretrained_model/#option-2-creation-avec-google-colab-recommandee","title":"Option 2 : Cr\u00e9ation avec Google Colab (Recommand\u00e9e)","text":"<p>Cette option ne n\u00e9cessite aucune installation sur votre machine :</p> <ol> <li>Acc\u00e9dez au notebook Google Colab pr\u00e9par\u00e9 pour ce projet : Cr\u00e9ation du mod\u00e8le Fashion MNIST</li> <li> <p>Alternativement, ouvrez Google Colab, cr\u00e9ez un nouveau notebook et copiez-collez le code du fichier <code>tools/create_model.py</code></p> </li> <li> <p>Ex\u00e9cutez le notebook en cliquant sur \"Ex\u00e9cuter tout\" dans le menu \"Ex\u00e9cution\"</p> </li> <li> <p>L'ex\u00e9cution prendra environ 2-3 minutes avec l'acc\u00e9l\u00e9ration GPU de Colab</p> </li> <li> <p>\u00c0 la fin de l'ex\u00e9cution, le mod\u00e8le sera cr\u00e9\u00e9. T\u00e9l\u00e9chargez-le en ajoutant cette cellule \u00e0 la fin du notebook et en l'ex\u00e9cutant :    <pre><code># Cellule pour t\u00e9l\u00e9charger le mod\u00e8le depuis Colab\nfrom google.colab import files\nfiles.download('mobilenet_clothing_model.h5')\n</code></pre></p> </li> <li>Placez le fichier t\u00e9l\u00e9charg\u00e9 dans ce dossier <code>models/pretrained_model/</code></li> </ol>"},{"location":"module3/api-vetements-ia/models/pretrained_model/#option-2-recreer-le-modele","title":"Option 2 : Recr\u00e9er le mod\u00e8le","text":"<p>Si vous souhaitez comprendre comment le mod\u00e8le a \u00e9t\u00e9 cr\u00e9\u00e9 ou exp\u00e9rimenter avec diff\u00e9rents param\u00e8tres : 1. Consultez le script <code>tools/create_model.py</code> 2. Ex\u00e9cutez ce script pour cr\u00e9er votre propre version du mod\u00e8le 3. Le script g\u00e9n\u00e9rera un nouveau fichier <code>mobilenet_clothing_model.h5</code></p>"},{"location":"module3/api-vetements-ia/models/pretrained_model/#option-3-solution-de-repli-automatique","title":"Option 3 : Solution de repli automatique","text":"<p>L'application est con\u00e7ue pour utiliser automatiquement MobileNetV2 standard comme solution de repli si le mod\u00e8le sp\u00e9cifique n'est pas trouv\u00e9. Cette fonctionnalit\u00e9 garantit que l'application reste op\u00e9rationnelle m\u00eame sans le mod\u00e8le personnalis\u00e9.</p>"},{"location":"module3/api-vetements-ia/models/pretrained_model/#optimisations-appliquees","title":"Optimisations appliqu\u00e9es","text":"<ul> <li>Transfer learning : Adaptation d'un mod\u00e8le pr\u00e9-entra\u00een\u00e9 sur ImageNet</li> <li>Quantification : R\u00e9duction de la pr\u00e9cision num\u00e9rique (version TFLite disponible)</li> <li>Fine-tuning : Ajustement sur les donn\u00e9es Fashion MNIST</li> </ul>"},{"location":"module3/api-vetements-ia/tools/","title":"Outils pour le mod\u00e8le de classification de v\u00eatements","text":"<p>Ce dossier contient des scripts utilitaires pour la cr\u00e9ation et la gestion du mod\u00e8le de deep learning utilis\u00e9 dans ce projet.</p>"},{"location":"module3/api-vetements-ia/tools/#script-create_modelpy","title":"Script <code>create_model.py</code>","text":""},{"location":"module3/api-vetements-ia/tools/#objectif","title":"Objectif","text":"<p>Ce script permet de cr\u00e9er et d'entra\u00eener un mod\u00e8le de classification de v\u00eatements bas\u00e9 sur MobileNetV2, en utilisant le jeu de donn\u00e9es Fashion MNIST.</p>"},{"location":"module3/api-vetements-ia/tools/#fonctionnement","title":"Fonctionnement","text":"<ol> <li>T\u00e9l\u00e9chargement automatique du jeu de donn\u00e9es Fashion MNIST</li> <li>Pr\u00e9paration des donn\u00e9es (redimensionnement, conversion en RGB)</li> <li>Chargement de MobileNetV2 pr\u00e9-entra\u00een\u00e9 sur ImageNet</li> <li>Adaptation du mod\u00e8le pour la classification de v\u00eatements (transfer learning)</li> <li>Entra\u00eenement rapide du mod\u00e8le</li> <li>\u00c9valuation et sauvegarde du mod\u00e8le</li> </ol>"},{"location":"module3/api-vetements-ia/tools/#execution","title":"Ex\u00e9cution","text":"<pre><code># Depuis le dossier tools/\npython create_model.py\n</code></pre>"},{"location":"module3/api-vetements-ia/tools/#prerequis","title":"Pr\u00e9requis","text":"<ul> <li>TensorFlow 2.x</li> <li>Connexion internet : pour t\u00e9l\u00e9charger MobileNetV2 pr\u00e9-entra\u00een\u00e9</li> <li>RAM : minimum 4 Go</li> <li>Temps d'ex\u00e9cution estim\u00e9 : 5-10 minutes (CPU), 2-3 minutes (GPU)</li> </ul>"},{"location":"module3/api-vetements-ia/tools/#note-importante","title":"Note importante","text":"<ul> <li>Ce script est fourni \u00e0 des fins p\u00e9dagogiques pour comprendre comment le mod\u00e8le a \u00e9t\u00e9 cr\u00e9\u00e9. </li> <li>Le mod\u00e8le final est d\u00e9j\u00e0 inclus dans l'archive du projet dans le dossier <code>models/pretrained_model/</code>, donc vous n'avez pas besoin d'ex\u00e9cuter ce script pour utiliser l'application.</li> </ul>"},{"location":"module3/api-vetements-ia/tools/create_model/","title":"Create model","text":"In\u00a0[\u00a0]: Copied! <pre># Installation/mise \u00e0 jour des biblioth\u00e8ques si n\u00e9cessaire\n!pip install -q tensorflow\n</pre> # Installation/mise \u00e0 jour des biblioth\u00e8ques si n\u00e9cessaire !pip install -q tensorflow In\u00a0[\u00a0]: Copied! <pre>import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\n</pre> import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D import numpy as np import matplotlib.pyplot as plt import time In\u00a0[\u00a0]: Copied! <pre>print(f\"TensorFlow version: {tf.__version__}\")\n</pre> print(f\"TensorFlow version: {tf.__version__}\") In\u00a0[\u00a0]: Copied! <pre># V\u00e9rification si GPU est disponible\nif tf.config.list_physical_devices('GPU'):\n    print(\"GPU d\u00e9tect\u00e9 et activ\u00e9 \u2705\")\nelse:\n    print(\"\u26a0\ufe0f Pas de GPU d\u00e9tect\u00e9, l'ex\u00e9cution sera plus lente\")\n</pre> # V\u00e9rification si GPU est disponible if tf.config.list_physical_devices('GPU'):     print(\"GPU d\u00e9tect\u00e9 et activ\u00e9 \u2705\") else:     print(\"\u26a0\ufe0f Pas de GPU d\u00e9tect\u00e9, l'ex\u00e9cution sera plus lente\") In\u00a0[\u00a0]: Copied! <pre>## 1. T\u00e9l\u00e9chargement et exploration du jeu de donn\u00e9es\nfashion_mnist = tf.keras.datasets.fashion_mnist\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n</pre> ## 1. T\u00e9l\u00e9chargement et exploration du jeu de donn\u00e9es fashion_mnist = tf.keras.datasets.fashion_mnist (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data() In\u00a0[\u00a0]: Copied! <pre># Noms des classes\nclass_names = ['T-shirt/top', 'Pantalon', 'Pull-over', 'Robe', 'Manteau',\n               'Sandale', 'Chemise', 'Sneaker', 'Sac', 'Bottine']\n</pre> # Noms des classes class_names = ['T-shirt/top', 'Pantalon', 'Pull-over', 'Robe', 'Manteau',                'Sandale', 'Chemise', 'Sneaker', 'Sac', 'Bottine'] In\u00a0[\u00a0]: Copied! <pre>print(f\"Nombre d'exemples d'entra\u00eenement: {len(train_images)}\")\nprint(f\"Nombre d'exemples de test: {len(test_images)}\")\nprint(f\"Taille des images: {train_images[0].shape}\")\n</pre> print(f\"Nombre d'exemples d'entra\u00eenement: {len(train_images)}\") print(f\"Nombre d'exemples de test: {len(test_images)}\") print(f\"Taille des images: {train_images[0].shape}\") In\u00a0[\u00a0]: Copied! <pre># Visualisation de quelques exemples\nplt.figure(figsize=(10, 10))\nfor i in range(9):\n    plt.subplot(3, 3, i + 1)\n    plt.imshow(train_images[i], cmap='gray')\n    plt.title(class_names[train_labels[i]])\n    plt.axis('off')\nplt.tight_layout()\nplt.show()\n</pre> # Visualisation de quelques exemples plt.figure(figsize=(10, 10)) for i in range(9):     plt.subplot(3, 3, i + 1)     plt.imshow(train_images[i], cmap='gray')     plt.title(class_names[train_labels[i]])     plt.axis('off') plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre>## 2. Pr\u00e9paration des donn\u00e9es\n# Redimensionnement pour le mod\u00e8le (28x28x1)\ntrain_images = train_images / 255.0  # Normalisation\ntest_images = test_images / 255.0\n</pre> ## 2. Pr\u00e9paration des donn\u00e9es # Redimensionnement pour le mod\u00e8le (28x28x1) train_images = train_images / 255.0  # Normalisation test_images = test_images / 255.0 In\u00a0[\u00a0]: Copied! <pre>print(f\"Forme des donn\u00e9es d'entra\u00eenement: {train_images.shape}\")\nprint(f\"Forme des donn\u00e9es de test: {test_images.shape}\")\n</pre> print(f\"Forme des donn\u00e9es d'entra\u00eenement: {train_images.shape}\") print(f\"Forme des donn\u00e9es de test: {test_images.shape}\") In\u00a0[\u00a0]: Copied! <pre># Utilisation d'un sous-ensemble tr\u00e8s petit pour l'entra\u00eenement\nSUBSET_SIZE = 500  # Utiliser seulement 500 exemples pour l'entra\u00eenement rapide\ntrain_subset = train_images[:SUBSET_SIZE]\nlabels_subset = train_labels[:SUBSET_SIZE]\n</pre> # Utilisation d'un sous-ensemble tr\u00e8s petit pour l'entra\u00eenement SUBSET_SIZE = 500  # Utiliser seulement 500 exemples pour l'entra\u00eenement rapide train_subset = train_images[:SUBSET_SIZE] labels_subset = train_labels[:SUBSET_SIZE] In\u00a0[\u00a0]: Copied! <pre>## 3. Cr\u00e9ation du mod\u00e8le (plus l\u00e9ger)\nmodel = Sequential([\n    Flatten(input_shape=(28, 28)),\n    Dense(128, activation='relu'),\n    Dense(10, activation='softmax')\n])\n</pre> ## 3. Cr\u00e9ation du mod\u00e8le (plus l\u00e9ger) model = Sequential([     Flatten(input_shape=(28, 28)),     Dense(128, activation='relu'),     Dense(10, activation='softmax') ]) In\u00a0[\u00a0]: Copied! <pre># R\u00e9sum\u00e9 du mod\u00e8le\nprint(\"Structure du mod\u00e8le:\")\nmodel.summary()\n</pre> # R\u00e9sum\u00e9 du mod\u00e8le print(\"Structure du mod\u00e8le:\") model.summary() In\u00a0[\u00a0]: Copied! <pre>## 4. Compilation et entra\u00eenement\nmodel.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n</pre> ## 4. Compilation et entra\u00eenement model.compile(     optimizer='adam',     loss='sparse_categorical_crossentropy',     metrics=['accuracy'] ) In\u00a0[\u00a0]: Copied! <pre>print(\"D\u00e9but de l'entra\u00eenement...\")\nstart_time = time.time()\n</pre> print(\"D\u00e9but de l'entra\u00eenement...\") start_time = time.time() In\u00a0[\u00a0]: Copied! <pre># Entra\u00eenement sur 5 \u00e9poques\nhistory = model.fit(\n    train_subset, labels_subset,\n    epochs=5,\n    batch_size=32,\n    validation_split=0.2,\n    verbose=1\n)\n</pre> # Entra\u00eenement sur 5 \u00e9poques history = model.fit(     train_subset, labels_subset,     epochs=5,     batch_size=32,     validation_split=0.2,     verbose=1 ) In\u00a0[\u00a0]: Copied! <pre>print(f\"Entra\u00eenement termin\u00e9 en {time.time() - start_time:.2f} secondes\")\n</pre> print(f\"Entra\u00eenement termin\u00e9 en {time.time() - start_time:.2f} secondes\") In\u00a0[\u00a0]: Copied! <pre>## 5. \u00c9valuation\ntest_loss, test_acc = model.evaluate(test_images, test_labels, verbose=0)\nprint(f\"Pr\u00e9cision sur les donn\u00e9es de test: {test_acc*100:.2f}%\")\n</pre> ## 5. \u00c9valuation test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=0) print(f\"Pr\u00e9cision sur les donn\u00e9es de test: {test_acc*100:.2f}%\") In\u00a0[\u00a0]: Copied! <pre># Visualisation des r\u00e9sultats d'entra\u00eenement\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Pr\u00e9cision (entra\u00eenement)')\nplt.plot(history.history['val_accuracy'], label='Pr\u00e9cision (validation)')\nplt.xlabel('\u00c9poque')\nplt.ylabel('Pr\u00e9cision')\nplt.legend()\n</pre> # Visualisation des r\u00e9sultats d'entra\u00eenement plt.figure(figsize=(12, 4)) plt.subplot(1, 2, 1) plt.plot(history.history['accuracy'], label='Pr\u00e9cision (entra\u00eenement)') plt.plot(history.history['val_accuracy'], label='Pr\u00e9cision (validation)') plt.xlabel('\u00c9poque') plt.ylabel('Pr\u00e9cision') plt.legend() In\u00a0[\u00a0]: Copied! <pre>plt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Perte (entra\u00eenement)')\nplt.plot(history.history['val_loss'], label='Perte (validation)')\nplt.xlabel('\u00c9poque')\nplt.ylabel('Perte')\nplt.legend()\nplt.tight_layout()\nplt.show()\n</pre> plt.subplot(1, 2, 2) plt.plot(history.history['loss'], label='Perte (entra\u00eenement)') plt.plot(history.history['val_loss'], label='Perte (validation)') plt.xlabel('\u00c9poque') plt.ylabel('Perte') plt.legend() plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre>## 6. Exemples de pr\u00e9dictions\n# Faisons quelques pr\u00e9dictions pour v\u00e9rifier\npredictions = model.predict(test_images[:9])\n</pre> ## 6. Exemples de pr\u00e9dictions # Faisons quelques pr\u00e9dictions pour v\u00e9rifier predictions = model.predict(test_images[:9]) In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(12, 12))\nfor i in range(9):\n    plt.subplot(3, 3, i+1)\n    plt.imshow(test_images[i], cmap='gray')\n    predicted_label = np.argmax(predictions[i])\n    true_label = test_labels[i]\n\n    if predicted_label == true_label:\n        color = 'green'\n    else:\n        color = 'red'\n\n    plt.title(f\"Pr\u00e9dit: {class_names[predicted_label]}\\nR\u00e9el: {class_names[true_label]}\",\n              color=color)\n    plt.axis('off')\nplt.tight_layout()\nplt.show()\n</pre> plt.figure(figsize=(12, 12)) for i in range(9):     plt.subplot(3, 3, i+1)     plt.imshow(test_images[i], cmap='gray')     predicted_label = np.argmax(predictions[i])     true_label = test_labels[i]      if predicted_label == true_label:         color = 'green'     else:         color = 'red'      plt.title(f\"Pr\u00e9dit: {class_names[predicted_label]}\\nR\u00e9el: {class_names[true_label]}\",               color=color)     plt.axis('off') plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre>## 7. Sauvegarde du mod\u00e8le\nmodel.save('simple_clothing_model.h5')\nprint(\"Mod\u00e8le sauvegard\u00e9 avec succ\u00e8s sous le nom 'simple_clothing_model.h5'\")\n</pre> ## 7. Sauvegarde du mod\u00e8le model.save('simple_clothing_model.h5') print(\"Mod\u00e8le sauvegard\u00e9 avec succ\u00e8s sous le nom 'simple_clothing_model.h5'\") In\u00a0[\u00a0]: Copied! <pre># Pour t\u00e9l\u00e9charger le mod\u00e8le depuis Google Colab\nfrom google.colab import files\nfiles.download('simple_clothing_model.h5')\n</pre> # Pour t\u00e9l\u00e9charger le mod\u00e8le depuis Google Colab from google.colab import files files.download('simple_clothing_model.h5') In\u00a0[\u00a0]: Copied! <pre>print(\"\\n\u2705 Processus termin\u00e9 avec succ\u00e8s!\")\nprint(\"Vous pouvez maintenant utiliser ce mod\u00e8le dans l'application de classification de v\u00eatements.\")\n</pre> print(\"\\n\u2705 Processus termin\u00e9 avec succ\u00e8s!\") print(\"Vous pouvez maintenant utiliser ce mod\u00e8le dans l'application de classification de v\u00eatements.\")"},{"location":"module3/api-vetements-ia/utils/__init__/","title":"init","text":""},{"location":"module3/api-vetements-ia/utils/image_utils/","title":"Image utils","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nUtilitaires pour le pr\u00e9traitement des images\n\"\"\"\n</pre> \"\"\" Utilitaires pour le pr\u00e9traitement des images \"\"\" In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nfrom PIL import Image, ImageOps\nimport io\nimport base64\n</pre> import numpy as np from PIL import Image, ImageOps import io import base64 In\u00a0[\u00a0]: Copied! <pre>def preprocess_image(image, target_size=(224, 224)):\n    \"\"\"\n    Pr\u00e9traite une image pour l'inf\u00e9rence\n    \n    Args:\n        image: Image PIL ou chemin vers une image\n        target_size: Taille cible pour le redimensionnement\n        \n    Returns:\n        np.array: Image pr\u00e9trait\u00e9e\n    \"\"\"\n    # Convertir en PIL Image si ce n'est pas d\u00e9j\u00e0 le cas\n    if not isinstance(image, Image.Image):\n        if isinstance(image, str):\n            image = Image.open(image)\n        elif isinstance(image, bytes) or isinstance(image, io.BytesIO):\n            image = Image.open(io.BytesIO(image))\n        else:\n            raise ValueError(\"Format d'image non pris en charge\")\n    \n    # Assurer que l'image est en RGB\n    if image.mode != 'RGB':\n        image = image.convert('RGB')\n    \n    # Redimensionner\n    image = image.resize(target_size, Image.LANCZOS)\n    \n    # Convertir en tableau numpy\n    img_array = np.array(image) / 255.0  # Normalisation\n    \n    return np.expand_dims(img_array, axis=0)\n</pre> def preprocess_image(image, target_size=(224, 224)):     \"\"\"     Pr\u00e9traite une image pour l'inf\u00e9rence          Args:         image: Image PIL ou chemin vers une image         target_size: Taille cible pour le redimensionnement              Returns:         np.array: Image pr\u00e9trait\u00e9e     \"\"\"     # Convertir en PIL Image si ce n'est pas d\u00e9j\u00e0 le cas     if not isinstance(image, Image.Image):         if isinstance(image, str):             image = Image.open(image)         elif isinstance(image, bytes) or isinstance(image, io.BytesIO):             image = Image.open(io.BytesIO(image))         else:             raise ValueError(\"Format d'image non pris en charge\")          # Assurer que l'image est en RGB     if image.mode != 'RGB':         image = image.convert('RGB')          # Redimensionner     image = image.resize(target_size, Image.LANCZOS)          # Convertir en tableau numpy     img_array = np.array(image) / 255.0  # Normalisation          return np.expand_dims(img_array, axis=0) In\u00a0[\u00a0]: Copied! <pre>def decode_base64_image(base64_string):\n    \"\"\"\n    D\u00e9code une image encod\u00e9e en base64\n    \n    Args:\n        base64_string: Image encod\u00e9e en base64\n        \n    Returns:\n        PIL.Image: Image d\u00e9cod\u00e9e\n    \"\"\"\n    if ',' in base64_string:\n        base64_string = base64_string.split(',')[1]\n        \n    image_data = base64.b64decode(base64_string)\n    return Image.open(io.BytesIO(image_data))\n</pre> def decode_base64_image(base64_string):     \"\"\"     D\u00e9code une image encod\u00e9e en base64          Args:         base64_string: Image encod\u00e9e en base64              Returns:         PIL.Image: Image d\u00e9cod\u00e9e     \"\"\"     if ',' in base64_string:         base64_string = base64_string.split(',')[1]              image_data = base64.b64decode(base64_string)     return Image.open(io.BytesIO(image_data)) In\u00a0[\u00a0]: Copied! <pre>def center_crop_image(image):\n    \"\"\"\n    Recadre une image au centre pour obtenir un carr\u00e9\n    \n    Args:\n        image: Image PIL\n        \n    Returns:\n        PIL.Image: Image recadr\u00e9e\n    \"\"\"\n    width, height = image.size\n    \n    # D\u00e9terminer la dimension la plus petite\n    min_dim = min(width, height)\n    \n    # Calculer les coordonn\u00e9es de d\u00e9coupe\n    left = (width - min_dim) // 2\n    top = (height - min_dim) // 2\n    right = left + min_dim\n    bottom = top + min_dim\n    \n    # D\u00e9couper l'image\n    return image.crop((left, top, right, bottom))\n</pre> def center_crop_image(image):     \"\"\"     Recadre une image au centre pour obtenir un carr\u00e9          Args:         image: Image PIL              Returns:         PIL.Image: Image recadr\u00e9e     \"\"\"     width, height = image.size          # D\u00e9terminer la dimension la plus petite     min_dim = min(width, height)          # Calculer les coordonn\u00e9es de d\u00e9coupe     left = (width - min_dim) // 2     top = (height - min_dim) // 2     right = left + min_dim     bottom = top + min_dim          # D\u00e9couper l'image     return image.crop((left, top, right, bottom))"},{"location":"module3/api-vetements-ia/utils/model_utils/","title":"Model utils","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nUtilitaires pour l'optimisation des mod\u00e8les de deep learning\n\"\"\"\n</pre> \"\"\" Utilitaires pour l'optimisation des mod\u00e8les de deep learning \"\"\" In\u00a0[\u00a0]: Copied! <pre>import tensorflow as tf\nimport numpy as np\nimport os\n</pre> import tensorflow as tf import numpy as np import os In\u00a0[\u00a0]: Copied! <pre>def optimize_model_for_inference(model):\n    \"\"\"\n    Optimise un mod\u00e8le Keras pour l'inf\u00e9rence\n    \n    Args:\n        model: Mod\u00e8le Keras \u00e0 optimiser\n        \n    Returns:\n        Mod\u00e8le optimis\u00e9\n    \"\"\"\n    # On applique plusieurs optimisations courantes\n    \n    # 1. Fusionner les op\u00e9rations BatchNorm avec les couches Conv pr\u00e9c\u00e9dentes\n    tf.keras.backend.clear_session()\n    model_config = model.get_config()\n    weights = model.get_weights()\n    \n    # Cr\u00e9ation du mod\u00e8le avec l'optimisation pour l'inf\u00e9rence\n    optimized_model = tf.keras.models.Model.from_config(model_config)\n    optimized_model.set_weights(weights)\n    \n    return optimized_model\n</pre> def optimize_model_for_inference(model):     \"\"\"     Optimise un mod\u00e8le Keras pour l'inf\u00e9rence          Args:         model: Mod\u00e8le Keras \u00e0 optimiser              Returns:         Mod\u00e8le optimis\u00e9     \"\"\"     # On applique plusieurs optimisations courantes          # 1. Fusionner les op\u00e9rations BatchNorm avec les couches Conv pr\u00e9c\u00e9dentes     tf.keras.backend.clear_session()     model_config = model.get_config()     weights = model.get_weights()          # Cr\u00e9ation du mod\u00e8le avec l'optimisation pour l'inf\u00e9rence     optimized_model = tf.keras.models.Model.from_config(model_config)     optimized_model.set_weights(weights)          return optimized_model In\u00a0[\u00a0]: Copied! <pre>def quantize_model(model, model_path, quantize_type='default'):\n    \"\"\"\n    Quantifie un mod\u00e8le Keras et le sauvegarde au format TFLite\n    \n    Args:\n        model: Mod\u00e8le Keras \u00e0 quantifier\n        model_path: Chemin o\u00f9 sauvegarder le mod\u00e8le quantifi\u00e9\n        quantize_type: Type de quantification ('default', 'float16', 'int8')\n        \n    Returns:\n        Chemin vers le mod\u00e8le TFLite quantifi\u00e9\n    \"\"\"\n    # Cr\u00e9ation du convertisseur TFLite\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n    \n    # Configuration de la quantification\n    if quantize_type == 'default':\n        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    elif quantize_type == 'float16':\n        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n        converter.target_spec.supported_types = [tf.float16]\n    elif quantize_type == 'int8':\n        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n        converter.inference_input_type = tf.uint8\n        converter.inference_output_type = tf.uint8\n        \n        # Pour la quantification int8 compl\u00e8te, il faudrait ajouter un dataset repr\u00e9sentatif\n        # et configurer le representative_dataset_gen\n    \n    # Conversion du mod\u00e8le\n    tflite_model = converter.convert()\n    \n    # Sauvegarde du mod\u00e8le\n    output_path = f\"{model_path}.tflite\"\n    with open(output_path, 'wb') as f:\n        f.write(tflite_model)\n    \n    print(f\"Mod\u00e8le quantifi\u00e9 sauvegard\u00e9 \u00e0 {output_path}\")\n    print(f\"Taille originale: {os.path.getsize(model_path)} octets\")\n    print(f\"Taille apr\u00e8s quantification: {os.path.getsize(output_path)} octets\")\n    \n    return output_path\n</pre> def quantize_model(model, model_path, quantize_type='default'):     \"\"\"     Quantifie un mod\u00e8le Keras et le sauvegarde au format TFLite          Args:         model: Mod\u00e8le Keras \u00e0 quantifier         model_path: Chemin o\u00f9 sauvegarder le mod\u00e8le quantifi\u00e9         quantize_type: Type de quantification ('default', 'float16', 'int8')              Returns:         Chemin vers le mod\u00e8le TFLite quantifi\u00e9     \"\"\"     # Cr\u00e9ation du convertisseur TFLite     converter = tf.lite.TFLiteConverter.from_keras_model(model)          # Configuration de la quantification     if quantize_type == 'default':         converter.optimizations = [tf.lite.Optimize.DEFAULT]     elif quantize_type == 'float16':         converter.optimizations = [tf.lite.Optimize.DEFAULT]         converter.target_spec.supported_types = [tf.float16]     elif quantize_type == 'int8':         converter.optimizations = [tf.lite.Optimize.DEFAULT]         converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]         converter.inference_input_type = tf.uint8         converter.inference_output_type = tf.uint8                  # Pour la quantification int8 compl\u00e8te, il faudrait ajouter un dataset repr\u00e9sentatif         # et configurer le representative_dataset_gen          # Conversion du mod\u00e8le     tflite_model = converter.convert()          # Sauvegarde du mod\u00e8le     output_path = f\"{model_path}.tflite\"     with open(output_path, 'wb') as f:         f.write(tflite_model)          print(f\"Mod\u00e8le quantifi\u00e9 sauvegard\u00e9 \u00e0 {output_path}\")     print(f\"Taille originale: {os.path.getsize(model_path)} octets\")     print(f\"Taille apr\u00e8s quantification: {os.path.getsize(output_path)} octets\")          return output_path In\u00a0[\u00a0]: Copied! <pre>def prune_model(model, sparsity=0.5):\n    \"\"\"\n    \u00c9lague un mod\u00e8le pour r\u00e9duire sa taille (d\u00e9mo conceptuelle)\n    \n    Note: L'\u00e9lagage r\u00e9el n\u00e9cessiterait TensorFlow Model Optimization Toolkit\n    \n    Args:\n        model: Mod\u00e8le Keras \u00e0 \u00e9laguer\n        sparsity: Niveau de parcimonie cible (% de poids \u00e0 mettre \u00e0 z\u00e9ro)\n        \n    Returns:\n        Une version conceptuellement \"\u00e9lagu\u00e9e\" du mod\u00e8le\n    \"\"\"\n    # Ceci est une d\u00e9monstration conceptuelle\n    # Dans un cas r\u00e9el, nous utiliserions:\n    # import tensorflow_model_optimization as tfmot\n    # pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(...)\n    # pruned_model = tfmot.sparsity.keras.prune_low_magnitude(model, pruning_schedule)\n    \n    print(f\"[D\u00c9MO] \u00c9lagage du mod\u00e8le avec une parcimonie cible de {sparsity*100}%\")\n    print(\"Note: Pour un vrai \u00e9lagage, utilisez TensorFlow Model Optimization\")\n    \n    return model\n</pre> def prune_model(model, sparsity=0.5):     \"\"\"     \u00c9lague un mod\u00e8le pour r\u00e9duire sa taille (d\u00e9mo conceptuelle)          Note: L'\u00e9lagage r\u00e9el n\u00e9cessiterait TensorFlow Model Optimization Toolkit          Args:         model: Mod\u00e8le Keras \u00e0 \u00e9laguer         sparsity: Niveau de parcimonie cible (% de poids \u00e0 mettre \u00e0 z\u00e9ro)              Returns:         Une version conceptuellement \"\u00e9lagu\u00e9e\" du mod\u00e8le     \"\"\"     # Ceci est une d\u00e9monstration conceptuelle     # Dans un cas r\u00e9el, nous utiliserions:     # import tensorflow_model_optimization as tfmot     # pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(...)     # pruned_model = tfmot.sparsity.keras.prune_low_magnitude(model, pruning_schedule)          print(f\"[D\u00c9MO] \u00c9lagage du mod\u00e8le avec une parcimonie cible de {sparsity*100}%\")     print(\"Note: Pour un vrai \u00e9lagage, utilisez TensorFlow Model Optimization\")          return model"},{"location":"module4/","title":"\ud83e\udde0 Module 4 : Projet int\u00e9grateur - Chatbot p\u00e9dagogique","text":""},{"location":"module4/#objectifs-du-module","title":"\ud83c\udfaf Objectifs du module","text":"<p>Cette derni\u00e8re s\u00e9ance vous permettra de :</p> <ul> <li>Appliquer l'ensemble des connaissances acquises dans un projet concret et complet</li> <li>D\u00e9velopper un chatbot p\u00e9dagogique fonctionnel expliquant le Deep Learning</li> <li>Int\u00e9grer l'API Mistral AI dans une solution compl\u00e8te</li> <li>Pr\u00e9senter et d\u00e9fendre votre solution technique</li> </ul>"},{"location":"module4/#vision-du-projet","title":"\ud83d\udd0d Vision du projet","text":"<p>Le projet consiste \u00e0 d\u00e9velopper un assistant virtuel conversationnel capable d'expliquer les concepts du Deep Learning, de r\u00e9pondre aux questions techniques et d'accompagner les apprenants dans leur d\u00e9couverte de ce domaine.</p> <p>\ud83c\udfaf Objectif : Concevoir un chatbot interactif qui aide les \u00e9tudiants de BTS SIO \u00e0 comprendre les concepts du Deep Learning \u00e0 travers des explications personnalis\u00e9es, des exemples concrets et des exercices adapt\u00e9s.</p>"},{"location":"module4/#architecture-technique","title":"Architecture technique","text":"<p>Le chatbot s'appuiera sur une architecture moderne compos\u00e9e de trois \u00e9l\u00e9ments principaux :</p> <pre><code>flowchart LR\n    A[Interface Web] &lt;--&gt; B[Backend Python]\n    B &lt;--&gt; C[API Mistral AI]\n    D[Base de connaissances] &lt;--&gt; B</code></pre>"},{"location":"module4/#1-interface-conversationnelle","title":"1. Interface conversationnelle","text":"<ul> <li>Interface web simple et intuitive</li> <li>Affichage des messages en format discussion</li> <li>Indicateur de chargement pendant le traitement</li> <li>Historique de conversation</li> </ul>"},{"location":"module4/#2-backend-flaskfastapi","title":"2. Backend Flask/FastAPI","text":"<ul> <li>Gestion des requ\u00eates et des sessions</li> <li>Enrichissement des prompts avec la base de connaissances</li> <li>Communication avec l'API Mistral</li> <li>Logique de traitement des r\u00e9ponses</li> </ul>"},{"location":"module4/#3-integration-api-mistral-ai","title":"3. Int\u00e9gration API Mistral AI","text":"<ul> <li>Configuration et param\u00e8trage des requ\u00eates</li> <li>Gestion du contexte de conversation</li> <li>Optimisation des prompts</li> <li>Gestion des erreurs et limitations</li> </ul>"},{"location":"module4/#4-base-de-connaissances","title":"4. Base de connaissances","text":"<ul> <li>Structure JSON organis\u00e9e par concepts</li> <li>Exercices et quiz par th\u00e9matique</li> </ul>"},{"location":"module4/#fonctionnalites-cles","title":"Fonctionnalit\u00e9s cl\u00e9s","text":"<p>Le chatbot p\u00e9dagogique offrira les fonctionnalit\u00e9s suivantes :</p> <ol> <li> <p>Explication des concepts</p> <ul> <li>D\u00e9finition adapt\u00e9e au niveau de l'utilisateur</li> <li>Exemples concrets pour illustrer chaque notion</li> <li>Analogies et comparaisons pour faciliter la compr\u00e9hension</li> </ul> </li> <li> <p>R\u00e9ponse aux questions</p> <ul> <li>Compr\u00e9hension des questions techniques</li> <li>R\u00e9ponses pr\u00e9cises bas\u00e9es sur la base de connaissances</li> <li>Capacit\u00e9 \u00e0 demander des clarifications si n\u00e9cessaire</li> </ul> </li> <li> <p>Progression adaptative</p> <ul> <li>D\u00e9tection du niveau de l'utilisateur</li> <li>Suggestions de concepts \u00e0 explorer ensuite</li> <li>Augmentation progressive de la complexit\u00e9</li> </ul> </li> <li> <p>Exercices interactifs</p> <ul> <li>G\u00e9n\u00e9ration de quiz sur les concepts vus</li> <li>Probl\u00e8mes simples \u00e0 r\u00e9soudre</li> <li>Feedback sur les r\u00e9ponses</li> </ul> </li> </ol>"},{"location":"module4/#approche-pedagogique","title":"Approche p\u00e9dagogique","text":"<p>Cette s\u00e9ance est enti\u00e8rement bas\u00e9e sur la r\u00e9alisation d'un projet concret en \u00e9quipe. Vous devrez mobiliser toutes les comp\u00e9tences d\u00e9velopp\u00e9es lors des s\u00e9ances pr\u00e9c\u00e9dentes pour cr\u00e9er une application compl\u00e8te et fonctionnelle. L'accent est mis sur l'autonomie, la collaboration et la mise en pratique professionnelle.</p>"},{"location":"module4/#structure-de-la-seance-4h","title":"Structure de la s\u00e9ance (4h)","text":"<pre><code>gantt\n    title Planning de la s\u00e9ance\n    dateFormat  HH:mm\n    axisFormat %H:%M\n\n    section Phase 1\n    D\u00e9veloppement du chatbot           :2h30, 00:00, 02:30\n\n    section Phase 2\n    Finalisation et tests              :1h, 02:30, 03:30\n\n    section Phase 3\n    Pr\u00e9sentation des projets           :30min, 03:30, 04:00  </code></pre>"},{"location":"module4/#trois-phases-de-realisation","title":"Trois phases de r\u00e9alisation","text":""},{"location":"module4/#phase-1-developpement-du-chatbot-2h30","title":"Phase 1 : D\u00e9veloppement du chatbot (2h30)","text":"<p>Impl\u00e9mentez les fonctionnalit\u00e9s principales de votre chatbot p\u00e9dagogique :</p> <ul> <li>Mise en place de l'interface conversationnelle</li> <li>Int\u00e9gration avanc\u00e9e avec l'API Mistral AI</li> <li>Structuration et enrichissement de la base de connaissances</li> <li>D\u00e9veloppement des fonctionnalit\u00e9s d'aide \u00e0 l'apprentissage</li> </ul>"},{"location":"module4/#phase-2-finalisation-et-tests-1h","title":"Phase 2 : Finalisation et tests (1h)","text":"<p>Peaufinez votre solution et assurez-vous de sa qualit\u00e9 :</p> <ul> <li>Tests fonctionnels et sc\u00e9narios d'utilisation</li> <li>Optimisation des performances</li> <li>Documentation technique et guide utilisateur</li> <li>Pr\u00e9paration de la d\u00e9monstration</li> </ul>"},{"location":"module4/#phase-3-presentation-des-projets-30min","title":"Phase 3 : Pr\u00e9sentation des projets (30min)","text":"<p>Pr\u00e9sentez votre solution \u00e0 la classe :</p> <ul> <li>D\u00e9monstration en direct du chatbot</li> <li>Explication des choix techniques</li> <li>Retour sur les d\u00e9fis rencontr\u00e9s et les solutions adopt\u00e9es</li> <li>Questions-r\u00e9ponses</li> </ul>"},{"location":"module4/#defis-techniques","title":"D\u00e9fis techniques","text":"<p>Les principaux d\u00e9fis \u00e0 relever seront :</p> <ol> <li>Prompt engineering efficace</li> <li>Formuler des instructions claires pour l'API Mistral</li> <li>Maintenir la coh\u00e9rence p\u00e9dagogique dans les r\u00e9ponses</li> <li> <p>\u00c9viter les hallucinations du mod\u00e8le</p> </li> <li> <p>Int\u00e9gration technique</p> </li> <li>Communication fluide entre frontend et backend</li> <li>Gestion asynchrone des requ\u00eates API</li> <li> <p>Optimisation des temps de r\u00e9ponse</p> </li> <li> <p>Qualit\u00e9 p\u00e9dagogique</p> </li> <li>Structure coh\u00e9rente de la base de connaissances</li> <li>Adaptation au niveau de l'utilisateur</li> <li>Progression logique entre les concepts</li> </ol>"},{"location":"module4/#ressources-necessaires","title":"Ressources n\u00e9cessaires","text":"<p>Pour cette s\u00e9ance, vous aurez besoin de :</p> <ul> <li>Votre document de conception pr\u00e9par\u00e9 lors de la s\u00e9ance 3</li> <li>Compte et cl\u00e9 API Mistral AI</li> <li>Environnement de d\u00e9veloppement (Google Colab ou local)</li> <li>Templates fournis pour la documentation</li> </ul> <p>Ressources fournies : - Documentation compl\u00e8te de l'API Mistral - Structure JSON pour la base de connaissances - Templates de code pour l'interface et le backend - Exemples de prompts efficaces</p>"},{"location":"module4/#livrables-attendus","title":"Livrables attendus","text":"<p>\u00c0 l'issue de cette s\u00e9ance, vous devrez remettre :</p> <ol> <li>Code source complet du chatbot p\u00e9dagogique</li> <li>Base de connaissances structur\u00e9e sur le Deep Learning</li> <li>Documentation technique expliquant l'architecture et les choix d'impl\u00e9mentation</li> <li>Guide utilisateur pour la prise en main</li> <li>Pr\u00e9sentation avec support \u00e0 fournir</li> </ol> <p>Ces livrables constituent l'aboutissement de votre parcours et seront \u00e9valu\u00e9s selon les crit\u00e8res d\u00e9taill\u00e9s dans la grille d'\u00e9valuation.</p>"},{"location":"module4/#pret-a-relever-le-defi","title":"Pr\u00eat \u00e0 relever le d\u00e9fi ?","text":"<p>C'est l'heure de mettre en pratique tout ce que vous avez appris pour cr\u00e9er un outil r\u00e9ellement utile. Bonne chance !</p> <p>Commencer la Phase 1 \u00c9valuer vos connaissances</p>"},{"location":"module4/partie1-developpement/","title":"\ud83d\udcbb Phase 1 : D\u00e9veloppement du chatbot (2h30)","text":""},{"location":"module4/partie1-developpement/#objectif","title":"\ud83c\udfaf Objectif","text":"<p>D\u00e9velopper un chatbot p\u00e9dagogique fonctionnel en une s\u00e9ance de 4 heures en utilisant l'API Mistral AI et les connaissances acquises lors des modules pr\u00e9c\u00e9dents.</p>"},{"location":"module4/partie1-developpement/#organisation-de-la-seance","title":"\ud83d\udcca Organisation de la s\u00e9ance","text":"<p>Pour maximiser l'efficacit\u00e9 sur cette courte dur\u00e9e, la s\u00e9ance est organis\u00e9e en 4 phases distinctes avec des objectifs clairs pour chacune.</p>"},{"location":"module4/partie1-developpement/#phase-1-cadrage-et-conception-rapide-30-min","title":"\ud83d\udcdd Phase 1: Cadrage et conception rapide (30 min)","text":"<p>\ud83c\udfaf Objectif: D\u00e9finir clairement le projet et effectuer une conception minimaliste.</p> <ol> <li> <p>\ud83d\udce2Pr\u00e9sentation du projet et des objectifs (10 min)</p> <ul> <li>Rappel des concepts cl\u00e9s vus dans les modules pr\u00e9c\u00e9dents</li> <li>D\u00e9monstration d'un exemple de chatbot fonctionnel</li> <li>Clarification des livrables attendus</li> </ul> </li> <li> <p>Choix de la variante (5 min)</p> <ul> <li>Option SISR: Chatbot d'aide au diagnostic r\u00e9seau/syst\u00e8me</li> <li>Option SLAM: Chatbot avec authentification simple</li> </ul> </li> <li> <p>Mini-atelier de wireframing (15 min)</p> <ul> <li>Sketch rapide sur papier de l'interface</li> <li>Identification des composants essentiels</li> <li>D\u00e9finition du flux de conversation principal</li> </ul> </li> </ol> <p>Utilisation du kit de d\u00e9marrage:   - R\u00e9cup\u00e9rez le kit de d\u00e9marrage correspondant \u00e0 votre option (SISR/SLAM)   - Examinez rapidement la structure des fichiers fournis   - Identifiez les parties \u00e0 modifier/compl\u00e9ter</p>"},{"location":"module4/partie1-developpement/#phase-2-developpement-en-parallele-2h","title":"Phase 2: D\u00e9veloppement en parall\u00e8le (2h)","text":"<p>Pour optimiser le temps, divisez votre \u00e9quipe en deux groupes travaillant en parall\u00e8le:</p>"},{"location":"module4/partie1-developpement/#groupe-a-interface-et-integration-api-1h-30min","title":"Groupe A: Interface et int\u00e9gration API (1h + 30min)","text":"<p>Objectif: Cr\u00e9er l'interface conversationnelle et int\u00e9grer l'API Mistral.</p> <ol> <li> <p>Pr\u00e9paration de l'interface (30 min)</p> <ul> <li>Partez du template HTML/CSS/JS fourni</li> <li>Personnalisez l'apparence selon votre wireframe</li> <li>Assurez-vous que la zone de messages et le champ de saisie fonctionnent</li> </ul> </li> <li> <p>Int\u00e9gration de l'API Mistral (30 min)</p> <ul> <li>Utilisez le code de base fourni dans <code>api-integration-template.py</code></li> <li>Configurez votre cl\u00e9 API (d\u00e9j\u00e0 cr\u00e9\u00e9e avant la s\u00e9ance)</li> <li>Testez une requ\u00eate simple pour v\u00e9rifier la connexion</li> </ul> </li> <li> <p>Connexion frontend/backend (30 min)</p> <ul> <li>Impl\u00e9mentez la communication entre l'interface et le backend</li> <li>Assurez-vous que les messages s'affichent correctement</li> <li>Ajoutez l'indicateur de chargement pendant les requ\u00eates</li> </ul> </li> </ol>"},{"location":"module4/partie1-developpement/#groupe-b-base-de-connaissances-et-fonctionnalites-pedagogiques-1h-30min","title":"Groupe B: Base de connaissances et fonctionnalit\u00e9s p\u00e9dagogiques (1h + 30min)","text":"<p>Objectif: Cr\u00e9er une base de connaissances minimale et impl\u00e9menter les fonctionnalit\u00e9s p\u00e9dagogiques essentielles.</p> <ol> <li> <p>Structuration de la base de connaissances (30 min)</p> <ul> <li>Utilisez le mod\u00e8le JSON fourni dans le kit</li> <li>Compl\u00e9tez 2 concepts cl\u00e9s:</li> <li>1 concept g\u00e9n\u00e9ral sur le Deep Learning</li> <li>1 concept sp\u00e9cifique \u00e0 votre option (SISR/SLAM)</li> </ul> </li> <li> <p>Enrichissement des prompts (30 min)</p> <ul> <li>Impl\u00e9mentez la fonction d'enrichissement des prompts</li> <li>Testez avec des requ\u00eates pour v\u00e9rifier l'int\u00e9gration de la base de connaissances</li> <li>Ajustez le prompt syst\u00e8me pour am\u00e9liorer les r\u00e9ponses</li> </ul> </li> <li> <p>Fonctionnalit\u00e9 p\u00e9dagogique simple (30 min)</p> <ul> <li>SISR: Impl\u00e9mentez un arbre de d\u00e9cision simple pour le diagnostic</li> <li>SLAM: Impl\u00e9mentez l'authentification basique et la persistance locale</li> </ul> </li> </ol>"},{"location":"module4/partie1-developpement/#phase-3-integration-et-tests-45-min","title":"Phase 3: Int\u00e9gration et tests (45 min)","text":"<p>Objectif: Assembler les composants et tester l'ensemble.</p> <ol> <li> <p>Int\u00e9gration des composants (20 min)</p> <ul> <li>Fusionnez le travail des deux groupes</li> <li>R\u00e9solvez les conflits \u00e9ventuels</li> <li>Assurez-vous que tous les \u00e9l\u00e9ments fonctionnent ensemble</li> </ul> </li> <li> <p>Tests fonctionnels (15 min)</p> <ul> <li>Testez avec les sc\u00e9narios pr\u00e9d\u00e9finis fournis</li> <li>Identifiez et notez les probl\u00e8mes rencontr\u00e9s</li> <li>Priorisez les corrections selon la criticit\u00e9</li> </ul> </li> <li> <p>Corrections rapides (10 min)</p> <ul> <li>Corrigez les probl\u00e8mes critiques</li> <li>Si n\u00e9cessaire, simplifiez certaines fonctionnalit\u00e9s pour assurer un produit minimal fonctionnel</li> <li>Documentez les probl\u00e8mes connus que vous n'avez pas eu le temps de r\u00e9soudre</li> </ul> </li> </ol>"},{"location":"module4/partie1-developpement/#phase-4-finalisation-et-presentation-45-min","title":"Phase 4: Finalisation et pr\u00e9sentation (45 min)","text":"<p>Objectif: Finaliser le projet et pr\u00e9parer la pr\u00e9sentation.</p> <ol> <li> <p>Documentation minimaliste (15 min)</p> <ul> <li>Compl\u00e9tez le README avec les informations essentielles</li> <li>Documentez les fonctionnalit\u00e9s impl\u00e9ment\u00e9es</li> <li>Ajoutez des commentaires dans le code pour les sections complexes</li> </ul> </li> <li> <p>Pr\u00e9paration de la d\u00e9monstration (15 min)</p> <ul> <li>D\u00e9finissez un sc\u00e9nario de d\u00e9monstration court mais percutant</li> <li>R\u00e9partissez les r\u00f4les pour la pr\u00e9sentation</li> <li>Pr\u00e9parez-vous \u00e0 expliquer vos choix techniques</li> </ul> </li> <li> <p>Pr\u00e9sentations crois\u00e9es (15 min)</p> <ul> <li>Chaque \u00e9quipe pr\u00e9sente son chatbot (2-3 minutes par \u00e9quipe)</li> <li>Feedback constructif des autres \u00e9quipes</li> <li>Auto-\u00e9valuation avec la grille fournie</li> </ul> </li> </ol>"},{"location":"module4/partie1-developpement/#variantes-du-projet-adaptees-aux-profils-sisrslam","title":"Variantes du projet adapt\u00e9es aux profils SISR/SLAM","text":""},{"location":"module4/partie1-developpement/#option-sisr-chatbot-daide-au-diagnostic-reseausysteme","title":"Option SISR: Chatbot d'aide au diagnostic r\u00e9seau/syst\u00e8me","text":"<p>Base de connaissances sp\u00e9cifique:   - Focus sur un probl\u00e8me r\u00e9seau courant (connexion WiFi)   - Structure d'arbre de d\u00e9cision pour le d\u00e9pannage   - Sc\u00e9nario typique: \"Je n'arrive pas \u00e0 me connecter au r\u00e9seau WiFi\"</p> <p>Fonctionnalit\u00e9s minimales:   - Interface avec zone de chat et boutons d'assistance rapide   - Capacit\u00e9 \u00e0 poser des questions de diagnostic   - Suggestion de solutions bas\u00e9es sur les r\u00e9ponses utilisateur</p> <p>Kit de d\u00e9marrage SISR:   - Template d'interface avec boutons d'assistance rapide   - Structure JSON pour probl\u00e8mes r\u00e9seau   - Exemples de prompts orient\u00e9s diagnostic   - Script de base pour l'arbre de d\u00e9cision</p>"},{"location":"module4/partie1-developpement/#option-slam-chatbot-integre-a-une-application-web-simple","title":"Option SLAM: Chatbot int\u00e9gr\u00e9 \u00e0 une application web simple","text":"<p>Base de connaissances sp\u00e9cifique:   - Focus sur un langage/framework de programmation    - Explications de concepts de d\u00e9veloppement   - Sc\u00e9nario typique: \"Comment impl\u00e9menter l'authentification en PHP?\"</p> <p>Fonctionnalit\u00e9s minimales:   - Interface avec syst\u00e8me de login basique   - Sauvegarde locale des conversations (localStorage)   - Historique des questions par utilisateur</p> <p>Kit de d\u00e9marrage SLAM:   - Template d'interface avec zone de login   - Structure JSON pour concepts de programmation   - Exemples de prompts orient\u00e9s d\u00e9veloppement   - Script de base pour l'authentification simple</p>"},{"location":"module4/partie1-developpement/#conseils-pour-optimiser-le-temps","title":"Conseils pour optimiser le temps","text":"<ol> <li>Timeboxing strict: Respectez scrupuleusement les temps allou\u00e9s \u00e0 chaque phase</li> <li>Minimum Viable Product: Concentrez-vous sur les fonctionnalit\u00e9s essentielles</li> <li>Utilisation du kit: Ne r\u00e9inventez pas la roue, partez des templates fournis</li> <li>Communication claire: Coordonnez-vous efficacement entre les groupes A et B</li> <li>Plan B: Si une fonctionnalit\u00e9 bloque trop longtemps, passez \u00e0 une solution plus simple</li> </ol>"},{"location":"module4/partie1-developpement/#livrables-attendus","title":"Livrables attendus","text":"<p>\u00c0 la fin de la s\u00e9ance de 4 heures, vous devrez remettre :</p> <ol> <li>Code source du chatbot (fichiers HTML, CSS, JS et Python)</li> <li>Base de connaissances JSON avec au moins 2 concepts complets</li> <li>Documentation minimale expliquant les fonctionnalit\u00e9s impl\u00e9ment\u00e9es</li> <li>Grille d'auto-\u00e9valuation compl\u00e9t\u00e9e</li> </ol>"},{"location":"module4/partie1-developpement/#fonctionnalites-a-implementer","title":"Fonctionnalit\u00e9s \u00e0 impl\u00e9menter","text":""},{"location":"module4/partie1-developpement/#1-interface-conversationnelle-30-min","title":"1. Interface conversationnelle (30 min)","text":"<p>L'interface du chatbot doit \u00eatre simple mais fonctionnelle. Elle comprendra :</p> <pre><code>  - Une zone d'affichage des messages\n  - Un champ de saisie pour les questions\n</code></pre> <ul> <li>Un bouton d'envoi<ul> <li>Une indication de chargement pendant le traitement</li> <li>Un syst\u00e8me d'historique de conversation</li> </ul> </li> </ul> <p>Mod\u00e8le de code pour l'interface web <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"fr\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;Chatbot p\u00e9dagogique - Deep Learning&lt;/title&gt;\n    &lt;link rel=\"stylesheet\" href=\"styles.css\"&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;div class=\"chat-container\"&gt;\n        &lt;div class=\"chat-header\"&gt;\n            &lt;h1&gt;Chatbot Deep Learning&lt;/h1&gt;\n        &lt;/div&gt;\n        &lt;div class=\"chat-messages\" id=\"chat-messages\"&gt;\n            &lt;!-- Les messages s'afficheront ici --&gt;\n            &lt;div class=\"message bot\"&gt;\n                &lt;div class=\"message-content\"&gt;\n                    Bonjour ! Je suis un chatbot sp\u00e9cialis\u00e9 dans le Deep Learning. \n                    Comment puis-je vous aider aujourd'hui ?\n                &lt;/div&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n        &lt;div class=\"chat-input\"&gt;\n            &lt;input type=\"text\" id=\"user-input\" placeholder=\"Posez votre question ici...\"&gt;\n            &lt;button id=\"send-button\"&gt;Envoyer&lt;/button&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n    &lt;script src=\"script.js\"&gt;&lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>Points cl\u00e9s \u00e0 respecter :  - Design responsive s'adaptant aux diff\u00e9rentes tailles d'\u00e9cran  - Indication claire des messages utilisateur vs assistant  - Gestion des erreurs (r\u00e9seau, API, etc.)</p>"},{"location":"module4/partie1-developpement/#2-integration-avancee-avec-lapi-mistral-ai-45-min","title":"2. Int\u00e9gration avanc\u00e9e avec l'API Mistral AI (45 min)","text":"<p>L'objectif est d'exploiter efficacement l'API Mistral AI pour g\u00e9n\u00e9rer des r\u00e9ponses pertinentes et contextualis\u00e9es.</p> <p>Structure d'int\u00e9gration recommand\u00e9e :</p> <pre><code>import os\nfrom dotenv import load_dotenv\nfrom mistralai.client import MistralClient\nfrom mistralai.models.chat_completion import ChatMessage\n\n# Chargement des variables d'environnement\nload_dotenv()\napi_key = os.getenv(\"MISTRAL_API_KEY\")\n\n# Initialisation du client\nclient = MistralClient(api_key=api_key)\n\n# Syst\u00e8me de gestion de contexte\nclass ConversationManager:\n    def __init__(self, system_prompt):\n        self.history = [\n            ChatMessage(role=\"system\", content=system_prompt)\n        ]\n\n    def add_user_message(self, message):\n        self.history.append(ChatMessage(role=\"user\", content=message))\n\n    def add_assistant_message(self, message):\n        self.history.append(ChatMessage(role=\"assistant\", content=message))\n\n    def get_response(self, model=\"mistral-medium\", temperature=0.7):\n        response = client.chat(\n            model=model,\n            messages=self.history,\n            temperature=temperature\n        )\n        content = response.choices[0].message.content\n        self.add_assistant_message(content)\n        return content\n\n    def get_history(self):\n        # Exclure le message syst\u00e8me pour l'affichage\n        return self.history[1:]\n\n# Exemple d'utilisation\nsystem_prompt = \"\"\"\nTu es un assistant p\u00e9dagogique sp\u00e9cialis\u00e9 dans le Deep Learning, con\u00e7u pour aider \nles \u00e9tudiants de BTS SIO. Tu expliques les concepts de mani\u00e8re claire et progressive, \nen adaptant ton niveau de technicit\u00e9 au niveau de l'\u00e9tudiant. Utilise des analogies \net des exemples concrets quand c'est pertinent.\n\"\"\"\n\nconversation = ConversationManager(system_prompt)\n</code></pre> <p>Aspects avanc\u00e9s \u00e0 impl\u00e9menter :</p> <ul> <li>Prompt engineering : Am\u00e9lioration des instructions syst\u00e8me pour obtenir des r\u00e9ponses optimales</li> <li>Gestion de contexte : Pr\u00e9servation de l'historique pour maintenir la coh\u00e9rence des conversations</li> <li>Param\u00e8tres ajustables : Contr\u00f4le de la temp\u00e9rature pour moduler la cr\u00e9ativit\u00e9 des r\u00e9ponses</li> <li>Contr\u00f4le de longueur : Limiter la longueur des r\u00e9ponses pour des explications concises</li> </ul>"},{"location":"module4/partie1-developpement/#3-structuration-de-la-base-de-connaissances-45-min","title":"3. Structuration de la base de connaissances (45 min)","text":"<p>La base de connaissances est le c\u0153ur de votre chatbot. Elle doit \u00eatre structur\u00e9e de mani\u00e8re logique et couvrir les concepts essentiels du Deep Learning, correspondant au programme que vous avez suivi.</p> <p>Structure recommand\u00e9e (format JSON) :</p> <pre><code>{\n  \"topics\": [\n    {\n      \"id\": \"intro_dl\",\n      \"title\": \"Introduction au Deep Learning\",\n      \"subtopics\": [\n        {\n          \"id\": \"diff_ml_dl\",\n          \"title\": \"Diff\u00e9rence entre Machine Learning et Deep Learning\",\n          \"content\": \"Le Machine Learning classique n\u00e9cessite une extraction manuelle des caract\u00e9ristiques (feature engineering) tandis que le Deep Learning les extrait automatiquement gr\u00e2ce \u00e0 ses multiples couches...\",\n          \"examples\": [\n            \"Dans la reconnaissance d'images, le ML classique n\u00e9cessite d'extraire manuellement des caract\u00e9ristiques comme les contours, les textures, alors que le DL apprend directement ces caract\u00e9ristiques.\",\n            \"Pour la classification de texte, le ML classique utilise des approches comme TF-IDF ou Bag-of-Words, alors que le DL utilise des embeddings et des architectures comme LSTM.\"\n          ],\n          \"related\": [\"neural_networks\", \"applications_dl\"]\n        },\n        // Autres sous-topics...\n      ]\n    },\n    // Autres topics principaux...\n  ]\n}\n</code></pre> <p>\u00c9l\u00e9ments \u00e0 inclure :      - Concepts fondamentaux du Deep Learning      - Types de r\u00e9seaux (CNN, RNN, etc.)      - Techniques d'entra\u00eenement et d'optimisation      - Applications pratiques      - Exemples de code simplifi\u00e9s      - Analogies pour faciliter la compr\u00e9hension</p> <p>La base de connaissances peut \u00eatre utilis\u00e9e pour enrichir les prompts envoy\u00e9s \u00e0 l'API ou pour offrir des r\u00e9ponses pr\u00e9d\u00e9finies \u00e0 certaines questions.</p>"},{"location":"module4/partie1-developpement/#4-fonctionnalites-daide-a-lapprentissage-30-min","title":"4. Fonctionnalit\u00e9s d'aide \u00e0 l'apprentissage (30 min)","text":"<p>Pour rendre votre chatbot v\u00e9ritablement p\u00e9dagogique, impl\u00e9mentez au moins deux des fonctionnalit\u00e9s suivantes :</p> <ol> <li> <p>G\u00e9n\u00e9ration de quiz : Cr\u00e9er des QCM pour tester les connaissances de l'utilisateur    <pre><code>def generate_quiz(topic):\n    # Exemple de structure\n    questions = {\n        \"intro_dl\": [\n            {\n                \"question\": \"Quelle est la principale diff\u00e9rence entre Machine Learning classique et Deep Learning ?\",\n                \"options\": [\n                    \"Le Deep Learning est toujours plus rapide\",\n                    \"Le Deep Learning extrait automatiquement les caract\u00e9ristiques pertinentes\",\n                    \"Le Deep Learning utilise exclusivement des GPUs\",\n                    \"Le Deep Learning n\u00e9cessite moins de donn\u00e9es\"\n                ],\n                \"correct\": 1,\n                \"explanation\": \"Le Deep Learning se distingue par sa capacit\u00e9 \u00e0 extraire automatiquement des caract\u00e9ristiques pertinentes gr\u00e2ce \u00e0 ses multiples couches, \u00e9liminant le besoin d'extraction manuelle (feature engineering).\"\n            },\n            # Autres questions...\n        ]\n    }\n    return questions.get(topic, [])\n</code></pre></p> </li> <li> <p>Syst\u00e8me de progression : Suivre le niveau de l'utilisateur et adapter le contenu    <pre><code>class LearnerProfile:\n    def __init__(self, user_id):\n        self.user_id = user_id\n        self.topics_seen = set()\n        self.quiz_scores = {}\n        self.current_level = \"beginner\"  # beginner, intermediate, advanced\n\n    def update_after_interaction(self, topic, subtopic):\n        self.topics_seen.add(f\"{topic}:{subtopic}\")\n        # Mise \u00e0 jour du niveau selon le nombre de topics vus\n        if len(self.topics_seen) &gt; 10:\n            self.current_level = \"intermediate\"\n        if len(self.topics_seen) &gt; 20:\n            self.current_level = \"advanced\"\n\n    def record_quiz_result(self, topic, score):\n        self.quiz_scores[topic] = score\n</code></pre></p> </li> <li> <p>Visualisations adaptatives : G\u00e9n\u00e9rer des sch\u00e9mas explicatifs selon le niveau    <pre><code>def get_visualization(concept, level):\n    visualizations = {\n        \"neural_network\": {\n            \"beginner\": \"simple_nn.svg\",\n            \"intermediate\": \"detailed_nn.svg\",\n            \"advanced\": \"complex_nn.svg\"\n        },\n        # Autres concepts...\n    }\n    return visualizations.get(concept, {}).get(level, \"default.svg\")\n</code></pre></p> </li> <li> <p>Suivi des mots-cl\u00e9s : Assistant remontant les d\u00e9finitions des termes techniques    <pre><code>def extract_technical_terms(message):\n    technical_terms = [\n        \"neurone\", \"couche\", \"poids\", \"biais\", \"fonction d'activation\",\n        \"r\u00e9tropropagation\", \"descente de gradient\", \"CNN\", \"RNN\", \"LSTM\"\n    ]\n    found_terms = []\n    for term in technical_terms:\n        if term.lower() in message.lower():\n            found_terms.append(term)\n    return found_terms\n</code></pre></p> </li> </ol>"},{"location":"module4/partie1-developpement/#travail-dequipe-et-repartition-des-taches","title":"Travail d'\u00e9quipe et r\u00e9partition des t\u00e2ches","text":"<p>Si vous travaillez en bin\u00f4me, r\u00e9partissez-vous les t\u00e2ches efficacement :</p> <p>Suggestion de r\u00e9partition :        - Membre 1 : Interface + Int\u00e9gration API        - Membre 2 : Base de connaissances + Fonctionnalit\u00e9s p\u00e9dagogiques</p> <p>Ou alternativement :        - Membre 1 : Backend (API, logique, base de connaissances)        - Membre 2 : Frontend (interface, interactions, exp\u00e9rience utilisateur)</p>"},{"location":"module4/partie1-developpement/#points-de-vigilance","title":"Points de vigilance","text":"<pre><code> - **S\u00e9curit\u00e9 :** Ne stockez jamais votre cl\u00e9 API directement dans le code\n - **R\u00e9activit\u00e9 :** Optimisez les temps de r\u00e9ponse, ajoutez des indicateurs de chargement\n - **Robustesse :** G\u00e9rez les erreurs (API indisponible, limite de requ\u00eates atteinte, etc.)\n - **Qualit\u00e9 des r\u00e9ponses :** Testez r\u00e9guli\u00e8rement avec des questions vari\u00e9es pour v\u00e9rifier la pertinence\n</code></pre>"},{"location":"module4/partie1-developpement/#livrables-intermediaires","title":"Livrables interm\u00e9diaires","text":"<p>\u00c0 la fin de cette phase, vous devriez avoir :</p> <ul> <li>Une interface conversationnelle fonctionnelle</li> <li>Un syst\u00e8me d'int\u00e9gration avec l'API Mistral AI</li> <li>Une base de connaissances structur\u00e9e</li> <li>Au moins deux fonctionnalit\u00e9s p\u00e9dagogiques impl\u00e9ment\u00e9es</li> </ul> <p>Retour \u00e0 la vue d'ensemble Continuer vers : Finalisation ```</p>"},{"location":"module4/partie2-finalisation/","title":"\ud83d\udee0\ufe0f Phase 2 : Finalisation et tests (1h)","text":""},{"location":"module4/partie2-finalisation/#objectif","title":"\ud83c\udfaf Objectif","text":"<p>Cette phase est d\u00e9di\u00e9e \u00e0 la finalisation, aux tests et \u00e0 la pr\u00e9paration de la documentation de votre chatbot p\u00e9dagogique. C'est ici que vous vous assurez que votre solution est fiable, performante et bien document\u00e9e.</p>"},{"location":"module4/partie2-finalisation/#1-tests-fonctionnels-20-min","title":"\ud83e\uddea 1. Tests fonctionnels (20 min)","text":""},{"location":"module4/partie2-finalisation/#protocole-de-test","title":"\ud83d\udccbProtocole de test","text":"<p>Mettez en place un protocole syst\u00e9matique pour tester votre chatbot avec des sc\u00e9narios r\u00e9els d'utilisation.</p> <p>\ud83d\udcca Cat\u00e9gories de tests \u00e0 effectuer :</p> <ol> <li> <p>\u2705Tests de base</p> <ul> <li>Dialogue simple (question-r\u00e9ponse)</li> <li>Gestion de l'historique de conversation</li> <li>Comportement face \u00e0 des requ\u00eates vides ou incompl\u00e8tes</li> </ul> </li> <li> <p>\u2705Tests de connaissances</p> <ul> <li>Questions sur chaque concept majeur du Deep Learning</li> <li>V\u00e9rification de l'exactitude des informations fournies</li> <li>Coh\u00e9rence dans les explications</li> </ul> </li> <li> <p>\u2705Tests d'usage p\u00e9dagogique</p> <ul> <li>Adaptation au niveau de l'utilisateur</li> <li>Clart\u00e9 des explications techniques</li> <li>Utilit\u00e9 des exemples et analogies</li> </ul> </li> <li> <p>\u2705Tests de robustesse</p> <ul> <li>Gestion des erreurs API</li> <li>Questions hors sujet</li> <li>Questions mal formul\u00e9es ou avec des fautes</li> </ul> </li> </ol> <p>Grille d'\u00e9valuation des tests :</p> Test Crit\u00e8re R\u00e9sultat Commentaire T1: Dialogue simple L'assistant r\u00e9pond de fa\u00e7on coh\u00e9rente \u2b1c OK \u2b1c NOK T2: Gestion historique Les r\u00e9ponses tiennent compte du contexte pr\u00e9c\u00e9dent \u2b1c OK \u2b1c NOK T3: Concept CNN L'explication est exacte et p\u00e9dagogique \u2b1c OK \u2b1c NOK T4: Concept gradient Formulation adapt\u00e9e au niveau d\u00e9butant \u2b1c OK \u2b1c NOK T5: Erreur API Message d'erreur appropri\u00e9 \u2b1c OK \u2b1c NOK ... ... ... ... <p>Pour chaque test qui \u00e9choue, notez le probl\u00e8me et priorisez les corrections.</p>"},{"location":"module4/partie2-finalisation/#profils-dutilisateurs-pour-les-tests","title":"Profils d'utilisateurs pour les tests","text":"<p>Testez votre chatbot avec diff\u00e9rents profils d'utilisateurs :      - D\u00e9butant complet : aucune connaissance pr\u00e9alable      - Niveau interm\u00e9diaire : connaissances de base en programmation      - Niveau avanc\u00e9 : familiarit\u00e9 avec l'IA et questions techniques d\u00e9taill\u00e9es</p>"},{"location":"module4/partie2-finalisation/#2-optimisation-des-performances-20-min","title":"2. Optimisation des performances (20 min)","text":""},{"location":"module4/partie2-finalisation/#optimisation-technique","title":"Optimisation technique","text":"<p>Am\u00e9liorez les performances techniques de votre chatbot :</p> <ol> <li>Temps de r\u00e9ponse</li> <li>R\u00e9duisez la taille des prompts envoy\u00e9s \u00e0 l'API</li> <li> <p>Ajoutez un syst\u00e8me de cache pour les questions fr\u00e9quentes    <pre><code># Exemple d'impl\u00e9mentation d'un cache simple\nresponse_cache = {}\n\ndef get_cached_response(question, user_level):\n    cache_key = f\"{question.lower().strip()}_{user_level}\"\n    return response_cache.get(cache_key)\n\ndef store_in_cache(question, user_level, response):\n    cache_key = f\"{question.lower().strip()}_{user_level}\"\n    response_cache[cache_key] = response\n</code></pre></p> </li> <li> <p>Efficacit\u00e9 de l'API</p> </li> <li>Utilisez des param\u00e8tres adapt\u00e9s pour chaque type de requ\u00eate</li> <li> <p>Optimisez la longueur des contextes transmis    <pre><code># Exemple de configuration par type de requ\u00eate\napi_configs = {\n    \"definition\": {\"temperature\": 0.3, \"max_tokens\": 100},  # D\u00e9finitions pr\u00e9cises\n    \"explanation\": {\"temperature\": 0.5, \"max_tokens\": 300}, # Explications d\u00e9taill\u00e9es\n    \"example\": {\"temperature\": 0.7, \"max_tokens\": 150}      # Exemples cr\u00e9atifs\n}\n</code></pre></p> </li> <li> <p>Gestion de la m\u00e9moire</p> </li> <li>Limitez la taille de l'historique de conversation</li> <li>Ajoutez un m\u00e9canisme de r\u00e9sum\u00e9 pour les longues conversations    <pre><code>class OptimizedConversationManager:\n    def __init__(self, max_history=10):\n        self.max_history = max_history\n        self.history = []\n\n    def add_message(self, role, content):\n        self.history.append({\"role\": role, \"content\": content})\n        # Si l'historique devient trop long, le r\u00e9sumer\n        if len(self.history) &gt; self.max_history + 5:  # +5 pour \u00e9viter de r\u00e9sumer trop souvent\n            self._summarize_history()\n\n    def _summarize_history(self):\n        # Demander \u00e0 l'API de r\u00e9sumer la conversation\n        # Puis remplacer l'historique par le r\u00e9sum\u00e9\n        # [Impl\u00e9mentation ici]\n</code></pre></li> </ol>"},{"location":"module4/partie2-finalisation/#optimisation-pedagogique","title":"Optimisation p\u00e9dagogique","text":"<p>Am\u00e9liorez la qualit\u00e9 p\u00e9dagogique des r\u00e9ponses :</p> <ol> <li> <p>Am\u00e9lioration des prompts</p> <ul> <li>Refinez les instructions syst\u00e8me pour des r\u00e9ponses plus p\u00e9dagogiques</li> <li>Ajoutez des directives sp\u00e9cifiques pour les explications techniques</li> </ul> </li> <li> <p>Enrichissement des r\u00e9ponses</p> <ul> <li>Ajoutez automatiquement des liens vers des ressources compl\u00e9mentaires</li> <li>Incluez des suggestions de questions de suivi pertinentes</li> </ul> </li> <li> <p>Adaptation au niveau</p> <ul> <li>Affinez la d\u00e9tection du niveau de l'utilisateur</li> <li>Personnalisez la complexit\u00e9 des r\u00e9ponses en fonction du niveau d\u00e9tect\u00e9</li> </ul> </li> </ol>"},{"location":"module4/partie2-finalisation/#3-documentation-20-min","title":"3. Documentation (20 min)","text":""},{"location":"module4/partie2-finalisation/#documentation-technique","title":"Documentation technique","text":"<p>Cr\u00e9ez une documentation technique claire et compl\u00e8te :</p> <ol> <li> <p>Architecture du syst\u00e8me</p> <ul> <li>Diagramme des composants principaux</li> <li>Description des interactions entre les composants</li> <li>Technologies et biblioth\u00e8ques utilis\u00e9es</li> </ul> </li> <li> <p>Structure du code</p> <ul> <li>Organisation des fichiers et dossiers</li> <li>Description des classes et fonctions principales</li> <li>Points d'extension pour de futures am\u00e9liorations</li> </ul> </li> <li> <p>API et int\u00e9grations</p> <ul> <li>Configuration requise pour l'API Mistral</li> <li>Param\u00e8tres d'API et leur impact</li> <li>Limites et quotas \u00e0 consid\u00e9rer</li> </ul> </li> </ol> <p>Mod\u00e8le de documentation technique :</p> <pre><code># Documentation technique - Chatbot p\u00e9dagogique Deep Learning\n\n## 1. Vue d'ensemble du syst\u00e8me\n[Diagramme d'architecture]\n\nNotre chatbot est compos\u00e9 de trois composants principaux :\n- Interface utilisateur (HTML/CSS/JS)\n- Serveur backend (Python/Flask)\n- Int\u00e9gration API Mistral AI\n\n## 2. Composants principaux\n\n### 2.1 Interface utilisateur\nL'interface est d\u00e9velopp\u00e9e en HTML/CSS/JS et permet :\n- L'affichage des messages dans un format conversationnel\n- La saisie et l'envoi de questions\n- L'affichage d'indicateurs de chargement\n- [...]\n\n### 2.2 Serveur backend\nLe serveur est d\u00e9velopp\u00e9 en Python avec Flask et g\u00e8re :\n- Les requ\u00eates de l'interface utilisateur\n- L'enrichissement des prompts avec la base de connaissances\n- Les appels \u00e0 l'API Mistral AI\n- [...]\n\n### 2.3 Base de connaissances\nLa base de connaissances est structur\u00e9e en JSON et comprend :\n- X concepts principaux\n- Y sous-concepts\n- Z exemples pratiques\n- [...]\n\n## 3. Flux d'ex\u00e9cution\n1. L'utilisateur envoie une question via l'interface\n2. Le serveur re\u00e7oit la question et l'historique\n3. [...]\n\n## 4. Guide d'installation et de d\u00e9ploiement\n[Instructions d\u00e9taill\u00e9es]\n</code></pre>"},{"location":"module4/partie2-finalisation/#guide-utilisateur","title":"Guide utilisateur","text":"<p>R\u00e9digez un guide utilisateur clair pour faciliter la prise en main :</p> <ol> <li> <p>Pr\u00e9sentation g\u00e9n\u00e9rale</p> <ul> <li>Objectif du chatbot</li> <li>Public cible</li> <li>Fonctionnalit\u00e9s principales</li> </ul> </li> <li> <p>Guide d'utilisation</p> <ul> <li>Comment poser des questions efficacement</li> <li>Exemples de questions pertinentes</li> <li>Commandes sp\u00e9ciales (si existantes)</li> </ul> </li> <li> <p>Conseils d'apprentissage</p> <ul> <li>Progression recommand\u00e9e dans les concepts</li> <li>Comment tester ses connaissances</li> <li>Ressources compl\u00e9mentaires</li> </ul> </li> </ol> <p>Mod\u00e8le de guide utilisateur :</p> <pre><code># Guide utilisateur - Chatbot p\u00e9dagogique Deep Learning\n\n## Bienvenue !\nCe chatbot a \u00e9t\u00e9 con\u00e7u pour vous aider \u00e0 comprendre les concepts du Deep Learning, \nde mani\u00e8re progressive et adapt\u00e9e \u00e0 votre niveau.\n\n## Comment utiliser le chatbot\n1. **Posez une question** dans la zone de texte en bas de l'\u00e9cran\n2. **Attendez la r\u00e9ponse** (g\u00e9n\u00e9ralement quelques secondes)\n3. **Poursuivez la conversation** en posant des questions compl\u00e9mentaires\n\n## Types de questions efficaces\n- \"Qu'est-ce qu'un r\u00e9seau de neurones convolutif ?\"\n- \"Explique-moi la descente de gradient comme si j'avais 12 ans\"\n- \"Quelles sont les diff\u00e9rences entre CNN et RNN ?\"\n- \"Montre-moi un exemple simple de code TensorFlow\"\n\n## Fonctionnalit\u00e9s sp\u00e9ciales\n- Tapez \"quiz\" pour g\u00e9n\u00e9rer un petit quiz sur le sujet de votre choix\n- Tapez \"progression\" pour voir votre avancement dans les concepts\n- [...]\n\n## Parcours d'apprentissage recommand\u00e9\nPour une progression optimale, nous vous sugg\u00e9rons d'explorer les concepts dans cet ordre :\n1. Introduction au Deep Learning\n2. R\u00e9seaux de neurones simples\n3. [...]\n</code></pre>"},{"location":"module4/partie2-finalisation/#4-preparation-de-la-demonstration-10-min","title":"4. Pr\u00e9paration de la d\u00e9monstration (10 min)","text":"<p>Pr\u00e9parez une d\u00e9monstration efficace pour pr\u00e9senter votre travail :</p> <ol> <li> <p>Sc\u00e9nario de d\u00e9monstration</p> <ul> <li>Identifiez un parcours utilisateur repr\u00e9sentatif</li> <li>Pr\u00e9parez 3-5 questions qui mettent en valeur diff\u00e9rentes fonctionnalit\u00e9s</li> <li>Anticipez les points qui pourraient impressionner l'audience</li> </ul> </li> <li> <p>Support visuel</p> <ul> <li>Cr\u00e9ez 2-3 diapositives pr\u00e9sentant l'architecture et les fonctionnalit\u00e9s</li> <li>Pr\u00e9parez un tableau r\u00e9capitulatif des d\u00e9fis rencontr\u00e9s et solutions trouv\u00e9es</li> </ul> </li> <li> <p>R\u00e9partition des r\u00f4les</p> <ul> <li>D\u00e9cidez qui pr\u00e9sentera quelle partie (si en bin\u00f4me)</li> <li>Planifiez les transitions entre les d\u00e9monstrations</li> </ul> </li> </ol> <p>Exemple de sc\u00e9nario de d\u00e9monstration :  1. Introduction du projet et objectifs (1 min)  2. Pr\u00e9sentation de l'architecture (1 min)  3. D\u00e9monstration d'une conversation basique (1 min)  4. D\u00e9monstration d'une fonctionnalit\u00e9 p\u00e9dagogique sp\u00e9ciale (1 min)  5. Explication d'un d\u00e9fi technique rencontr\u00e9 et sa solution (1 min)  6. Questions-r\u00e9ponses (1 min)</p>"},{"location":"module4/partie2-finalisation/#check-list-finale","title":"Check-list finale","text":"<p>Avant de terminer cette phase, v\u00e9rifiez les points suivants :</p> <ul> <li> Tous les tests fonctionnels critiques ont \u00e9t\u00e9 r\u00e9alis\u00e9s</li> <li> Les probl\u00e8mes prioritaires ont \u00e9t\u00e9 corrig\u00e9s</li> <li> La documentation technique est compl\u00e8te</li> <li> Le guide utilisateur est clair et informatif</li> <li> Le sc\u00e9nario de d\u00e9monstration est pr\u00eat</li> <li> Les livrables sont organis\u00e9s et accessibles</li> </ul> <p>Retour \u00e0 la vue d'ensemble Continuer vers la Phase 3: Pr\u00e9sentation des projets ```</p>"},{"location":"module4/partie3-presentation/","title":"\ud83d\udce2 Phase 3 : Pr\u00e9sentation des projets (30 min)","text":""},{"location":"module4/partie3-presentation/#objectif","title":"\ud83c\udfaf Objectif","text":"<p>Cette derni\u00e8re phase de la s\u00e9ance 4 est consacr\u00e9e \u00e0 la pr\u00e9sentation de votre chatbot p\u00e9dagogique. C'est l'aboutissement de votre travail et l'occasion de mettre en valeur votre solution devant la classe.</p>"},{"location":"module4/partie3-presentation/#deroulement-des-presentations","title":"\ud83c\udfaf D\u00e9roulement des pr\u00e9sentations","text":"<ul> <li> <p>Chaque \u00e9quipe dispose de 8 minutes au total :</p> </li> <li> <p>5 minutes de pr\u00e9sentation</p> </li> <li>3 minutes de questions-r\u00e9ponses</li> </ul>"},{"location":"module4/partie3-presentation/#structure-recommandee-pour-votre-presentation","title":"Structure recommand\u00e9e pour votre pr\u00e9sentation","text":""},{"location":"module4/partie3-presentation/#1-introduction-1-minute","title":"1. Introduction (1 minute)","text":"<ul> <li>Pr\u00e9sentez bri\u00e8vement votre \u00e9quipe</li> <li>Expliquez le concept g\u00e9n\u00e9ral de votre chatbot p\u00e9dagogique</li> <li>Pr\u00e9cisez le public cible et les objectifs p\u00e9dagogiques</li> </ul> <p>Exemple d'introduction :</p> <p>\"Bonjour, nous sommes [Pr\u00e9nom1] et [Pr\u00e9nom2]. Nous avons d\u00e9velopp\u00e9 'DeepLBot', un chatbot p\u00e9dagogique con\u00e7u pour accompagner les \u00e9tudiants de BTS SIO dans leur d\u00e9couverte du Deep Learning. Notre objectif est de rendre ces concepts complexes accessibles gr\u00e2ce \u00e0 des explications personnalis\u00e9es et interactives.\"</p>"},{"location":"module4/partie3-presentation/#2-architecture-et-choix-techniques-1-minute","title":"2. Architecture et choix techniques (1 minute)","text":"<ul> <li>Pr\u00e9sentez un sch\u00e9ma simple de l'architecture</li> <li>Justifiez rapidement vos choix techniques</li> <li>Mettez en avant les technologies principales utilis\u00e9es</li> </ul> <p>Exemple :</p> <p>\"Notre solution repose sur une architecture \u00e0 trois composants : une interface web en HTML/CSS/JavaScript, un backend Flask en Python, et l'int\u00e9gration de l'API Mistral AI. Nous avons structur\u00e9 notre base de connaissances en JSON pour faciliter la maintenance et l'enrichissement du contenu.\"</p>"},{"location":"module4/partie3-presentation/#3-demonstration-en-direct-2-minutes","title":"3. D\u00e9monstration en direct (2 minutes)","text":"<ul> <li>Montrez votre chatbot en action avec 2-3 sc\u00e9narios pr\u00e9d\u00e9finis</li> <li>Mettez en valeur les fonctionnalit\u00e9s distinctives</li> <li>Commentez les interactions pendant la d\u00e9monstration</li> </ul> <p>Conseils pour une d\u00e9monstration efficace :</p> <ul> <li>Pr\u00e9parez un script pr\u00e9cis avec des questions pertinentes</li> <li>Testez votre d\u00e9mo \u00e0 l'avance pour \u00e9viter les surprises</li> <li>Ayez un \"plan B\" en cas de probl\u00e8me technique (captures d'\u00e9cran)</li> </ul>"},{"location":"module4/partie3-presentation/#4-defis-et-solutions-1-minute","title":"4. D\u00e9fis et solutions (1 minute)","text":"<ul> <li>Pr\u00e9sentez 1-2 d\u00e9fis techniques majeurs rencontr\u00e9s</li> <li>Expliquez comment vous les avez surmont\u00e9s</li> <li>Partagez un apprentissage cl\u00e9 de ce processus</li> </ul> <p>Exemple :</p> <p>\"Notre principal d\u00e9fi a \u00e9t\u00e9 d'optimiser les prompts pour obtenir des r\u00e9ponses \u00e0 la fois p\u00e9dagogiquement pertinentes et concises. Nous avons r\u00e9solu ce probl\u00e8me en d\u00e9veloppant un syst\u00e8me de prompts dynamiques qui s'adaptent au niveau de l'utilisateur et au type de question pos\u00e9e.\"</p>"},{"location":"module4/partie3-presentation/#5-conclusion-et-perspectives-30-secondes","title":"5. Conclusion et perspectives (30 secondes)","text":"<ul> <li>R\u00e9capitulez les points forts de votre solution</li> <li>Indiquez les am\u00e9liorations futures envisag\u00e9es</li> <li>Terminez sur une note positive</li> </ul>"},{"location":"module4/partie3-presentation/#conseils-pour-une-presentation-reussie","title":"Conseils pour une pr\u00e9sentation r\u00e9ussie","text":""},{"location":"module4/partie3-presentation/#preparation","title":"Pr\u00e9paration","text":"<ul> <li>R\u00e9p\u00e9tez votre pr\u00e9sentation plusieurs fois en chronom\u00e9trant</li> <li>Simplifiez votre discours, \u00e9vitez le jargon technique excessif</li> <li>Synchronisez les r\u00f4les si vous pr\u00e9sentez en bin\u00f4me</li> <li>Pr\u00e9parez vos transitions entre les diff\u00e9rentes parties</li> </ul>"},{"location":"module4/partie3-presentation/#pendant-la-presentation","title":"Pendant la pr\u00e9sentation","text":"<ul> <li>Parlez clairement et \u00e0 un rythme mod\u00e9r\u00e9</li> <li>Faites face \u00e0 l'audience, pas \u00e0 l'\u00e9cran</li> <li>Mettez en valeur les fonctionnalit\u00e9s uniques de votre solution</li> <li>Respectez strictement le temps imparti</li> </ul>"},{"location":"module4/partie3-presentation/#support-visuel","title":"Support visuel","text":"<p>Si vous utilisez des diapositives, limitez-les \u00e0 3-4 maximum :</p> <ol> <li>Titre et pr\u00e9sentation de l'\u00e9quipe</li> <li>Architecture du chatbot (sch\u00e9ma)</li> <li>D\u00e9fis et solutions</li> <li>Perspectives futures</li> </ol>"},{"location":"module4/partie3-presentation/#grille-devaluation","title":"Grille d'\u00e9valuation","text":"<p>Votre pr\u00e9sentation sera \u00e9valu\u00e9e selon les crit\u00e8res suivants :</p> Crit\u00e8re Description Points Clart\u00e9 Explication claire du concept et de l'impl\u00e9mentation /4 D\u00e9monstration Qualit\u00e9 et pertinence de la d\u00e9monstration en direct /6 Technicit\u00e9 Ma\u00eetrise technique et pertinence des choix d'impl\u00e9mentation /5 Pr\u00e9sentation Organisation, respect du temps, qualit\u00e9 de l'\u00e9locution /3 R\u00e9ponses Qualit\u00e9 des r\u00e9ponses aux questions /2 Total /20"},{"location":"module4/partie3-presentation/#feedback-et-evaluation-par-les-pairs","title":"Feedback et \u00e9valuation par les pairs","text":"<p>Pendant que vos camarades pr\u00e9sentent, vous \u00eates encourag\u00e9s \u00e0 :</p> <ul> <li>Prendre des notes sur les id\u00e9es int\u00e9ressantes</li> <li>R\u00e9fl\u00e9chir \u00e0 une question pertinente \u00e0 poser</li> <li>Compl\u00e9ter la grille d'\u00e9valuation par les pairs qui vous sera distribu\u00e9e</li> </ul> <p>Cette \u00e9valuation par les pairs sera prise en compte dans l'\u00e9valuation finale, mais de mani\u00e8re anonyme.</p>"},{"location":"module4/partie3-presentation/#apres-les-presentations","title":"Apr\u00e8s les pr\u00e9sentations","text":"<p>Une fois toutes les pr\u00e9sentations termin\u00e9es :</p> <ul> <li>Un temps de d\u00e9briefing collectif sera organis\u00e9</li> <li>Les points forts de chaque projet seront mis en avant</li> <li>Des conseils d'am\u00e9lioration g\u00e9n\u00e9raux seront partag\u00e9s</li> </ul>"},{"location":"module4/partie3-presentation/#conclusion-et-prochaines-etapes","title":"Conclusion et prochaines \u00e9tapes","text":"<p>Cette pr\u00e9sentation marque la fin du projet et du parcours de 4 s\u00e9ances sur le Deep Learning. Vous avez acquis des comp\u00e9tences pr\u00e9cieuses en :</p> <ul> <li>Compr\u00e9hension des concepts du Deep Learning</li> <li>D\u00e9veloppement d'applications d'IA pratiques</li> <li>Int\u00e9gration d'API de mod\u00e8les de langage</li> <li>Conception d'interfaces interactives</li> </ul> <p>Ces comp\u00e9tences sont directement transf\u00e9rables dans votre future vie professionnelle, que ce soit en stage ou en emploi.</p>"},{"location":"module4/partie3-presentation/#remise-des-livrables-finaux","title":"Remise des livrables finaux","text":"<p>N'oubliez pas de d\u00e9poser tous vos livrables finaux sur la plateforme avant la date limite :</p> <ul> <li>Code source complet avec un lien GitHub</li> <li>Documentation technique sur Github en Markdown</li> <li>Guide utilisateur</li> <li>Support de pr\u00e9sentation</li> </ul> <p>Date limite de remise : [DATE_LIMITE]</p> <p>Retour \u00e0 la vue d'ensemble Retour \u00e0 la Phase 2: Finalisation</p> <p>```</p>"},{"location":"module4/preparation-projet/","title":"Pr\u00e9paration au Projet Final","text":""},{"location":"module4/preparation-projet/#objectifs-de-la-phase-de-preparation","title":"Objectifs de la phase de pr\u00e9paration","text":"<p>Cette phase vous permettra de : - Comprendre en d\u00e9tail les sp\u00e9cifications du projet de chatbot p\u00e9dagogique - Analyser des exemples concrets d'utilisation de chatbots similaires - Explorer l'API Mistral AI en profondeur - Planifier et organiser votre travail pour le d\u00e9veloppement</p>"},{"location":"module4/preparation-projet/#1-analyse-du-cahier-des-charges-15-min","title":"1. Analyse du cahier des charges (15 min)","text":"<p>Le cahier des charges de votre chatbot p\u00e9dagogique a \u00e9t\u00e9 pr\u00e9sent\u00e9 en d\u00e9tail dans le document Projet Chatbot P\u00e9dagogique. Prenez le temps de le lire attentivement et de vous poser les questions suivantes :</p> <ol> <li>Quelles fonctionnalit\u00e9s sont essentielles et lesquelles sont optionnelles ?</li> <li>Quels concepts du Deep Learning devront imp\u00e9rativement \u00eatre couverts dans la base de connaissances ?</li> <li>Quels sont les points techniques qui pourraient poser probl\u00e8me ?</li> <li>Comment adapter le niveau des explications aux diff\u00e9rents utilisateurs ?</li> <li>Quelles fonctionnalit\u00e9s p\u00e9dagogiques apporteraient une r\u00e9elle valeur ajout\u00e9e ?</li> </ol>"},{"location":"module4/preparation-projet/#exercice-de-priorisation","title":"Exercice de priorisation","text":"<p>\u00c9tablissez une liste des fonctionnalit\u00e9s \u00e0 d\u00e9velopper class\u00e9es par ordre de priorit\u00e9 :</p> <ol> <li>Fonctionnalit\u00e9s de base (MVP - Produit Minimum Viable)</li> <li>Fonctionnalit\u00e9s importantes </li> <li>Fonctionnalit\u00e9s optionnelles (si le temps le permet)</li> </ol>"},{"location":"module4/preparation-projet/#2-etude-de-cas-dentreprises-utilisant-des-chatbots-15-min","title":"2. \u00c9tude de cas d'entreprises utilisant des chatbots (15 min)","text":"<p>Avant de commencer le d\u00e9veloppement, examinons quelques exemples concrets d'entreprises qui ont mis en place des chatbots similaires \u00e0 celui que vous allez d\u00e9velopper.</p>"},{"location":"module4/preparation-projet/#cas-1-chatbot-pedagogique-pour-une-ecole-de-programmation","title":"Cas 1: Chatbot p\u00e9dagogique pour une \u00e9cole de programmation","text":"<p>Entreprise: CodeSchool (30 formateurs, 500+ \u00e9tudiants)</p> <p>Probl\u00e9matique: Les formateurs recevaient de nombreuses questions basiques identiques, ce qui limitait leur disponibilit\u00e9 pour des probl\u00e8mes plus complexes.</p> <p>Solution: D\u00e9veloppement d'un chatbot assistant bas\u00e9 sur une API de LLM, avec une base de connaissances construite \u00e0 partir du mat\u00e9riel de cours.</p> <p>Architecture: - Frontend: Interface web int\u00e9gr\u00e9e \u00e0 la plateforme d'apprentissage - Backend: API Flask avec mise en cache Redis - LLM: OpenAI API avec fine-tuning sp\u00e9cifique aux cours - Base de connaissances: Structur\u00e9e en JSON par modules de cours</p> <p>R\u00e9sultats: - R\u00e9duction de 40% des questions basiques aux formateurs - Satisfaction des \u00e9tudiants \u00e0 85% concernant les r\u00e9ponses du chatbot - ROI positif apr\u00e8s 4 mois d'utilisation - Cr\u00e9ation de 15 nouveaux modules de cours gr\u00e2ce au temps lib\u00e9r\u00e9</p> <p>Le\u00e7ons apprises: - Importance d'un syst\u00e8me de feedback imm\u00e9diat sur les r\u00e9ponses - N\u00e9cessit\u00e9 de maintenir la base de connaissances \u00e0 jour - Valeur des r\u00e9ponses comportant des exemples de code fonctionnels</p>"},{"location":"module4/preparation-projet/#cas-2-assistant-virtuel-pour-la-formation-interne","title":"Cas 2: Assistant virtuel pour la formation interne","text":"<p>Entreprise: TechConsult (cabinet de conseil IT, 120 employ\u00e9s)</p> <p>Probl\u00e9matique: Difficult\u00e9 \u00e0 former rapidement les nouveaux consultants sur les technologies sp\u00e9cifiques utilis\u00e9es par l'entreprise.</p> <p>Solution: Chatbot de formation accessible 24/7, int\u00e9gr\u00e9 \u00e0 l'intranet, avec connaissance des processus et technologies internes.</p> <p>Architecture: - Interface: Application web responsive - Backend: NodeJS avec FastAPI - LLM: Combinaison d'API locale et Mistral AI - Base de connaissances: Documents techniques convertis en embeddings vectoriels</p> <p>R\u00e9sultats: - R\u00e9duction du temps d'onboarding de 3 semaines \u00e0 10 jours - Augmentation de 25% du taux de r\u00e9ussite aux certifications internes - \u00c9conomie estim\u00e9e de 180 heures de formation par an - Adoption \u00e0 92% parmi les nouveaux employ\u00e9s</p> <p>Le\u00e7ons apprises: - L'importance d'utiliser le vocabulaire sp\u00e9cifique de l'entreprise - La valeur d'un historique de conversation persistant - L'utilit\u00e9 des prompts techniques bien formul\u00e9s</p>"},{"location":"module4/preparation-projet/#3-exploration-de-lapi-mistral-ai-20-min","title":"3. Exploration de l'API Mistral AI (20 min)","text":""},{"location":"module4/preparation-projet/#introduction-a-mistral-ai","title":"Introduction \u00e0 Mistral AI","text":"<p>Mistral AI est une entreprise fran\u00e7aise qui d\u00e9veloppe des mod\u00e8les de langage de pointe, particuli\u00e8rement adapt\u00e9s pour des usages en fran\u00e7ais et dans un contexte \u00e9ducatif. Son API permet d'acc\u00e9der \u00e0 ces mod\u00e8les pour g\u00e9n\u00e9rer du texte, r\u00e9pondre \u00e0 des questions, et plus encore.</p>"},{"location":"module4/preparation-projet/#creation-dun-compte-et-cle-api","title":"Cr\u00e9ation d'un compte et cl\u00e9 API","text":"<ol> <li>Rendez-vous sur console.mistral.ai</li> <li>Cr\u00e9ez un compte (gratuit)</li> <li>Une fois connect\u00e9, cliquez sur \"API Keys\" dans le menu</li> <li>Cliquez sur \"Create API Key\", donnez-lui un nom (ex: \"Projet Chatbot BTS\")</li> <li>Important: Copiez et sauvegardez la cl\u00e9 g\u00e9n\u00e9r\u00e9e, elle ne sera plus affich\u00e9e ensuite</li> </ol>"},{"location":"module4/preparation-projet/#premier-test-avec-lapi","title":"Premier test avec l'API","text":"<p>Commen\u00e7ons par un exemple simple pour tester l'API:</p> <pre><code>import os\nfrom mistralai.client import MistralClient\nfrom mistralai.models.chat_completion import ChatMessage\n\n# Configuration de l'API\napi_key = \"votre_cl\u00e9_api_ici\"  # Remplacez par votre cl\u00e9\nclient = MistralClient(api_key=api_key)\n\n# Messages\nmessages = [\n    ChatMessage(role=\"system\", content=\"Tu es un assistant p\u00e9dagogique sp\u00e9cialis\u00e9 dans l'explication du Deep Learning pour des \u00e9tudiants de BTS SIO.\"),\n    ChatMessage(role=\"user\", content=\"Peux-tu m'expliquer simplement ce qu'est un r\u00e9seau de neurones convolutif?\")\n]\n\n# Appel \u00e0 l'API\nchat_response = client.chat(\n    model=\"mistral-tiny\",  # Mod\u00e8le le plus l\u00e9ger\n    messages=messages,\n)\n\n# Affichage de la r\u00e9ponse\nprint(chat_response.choices[0].message.content)\n</code></pre>"},{"location":"module4/preparation-projet/#structure-de-lapi-mistral","title":"Structure de l'API Mistral","text":"<p>L'API Mistral AI fonctionne avec une structure simple :</p> <ol> <li>Messages : Liste de messages repr\u00e9sentant une conversation, chacun avec un r\u00f4le (system, user, assistant)</li> <li>Mod\u00e8le : Choix du mod\u00e8le Mistral \u00e0 utiliser (mistral-tiny, mistral-small, mistral-medium...)</li> <li>Param\u00e8tres : Configuration du comportement (temp\u00e9rature, nombre max de tokens, etc.)</li> </ol>"},{"location":"module4/preparation-projet/#gestion-du-contexte-conversationnel","title":"Gestion du contexte conversationnel","text":"<p>Pour maintenir un contexte de conversation, il suffit d'ajouter les messages pr\u00e9c\u00e9dents \u00e0 chaque requ\u00eate :</p> <pre><code># Fonction pour g\u00e9rer une conversation\ndef chat_with_context(messages, user_input):\n    # Ajouter le message de l'utilisateur\n    messages.append(ChatMessage(role=\"user\", content=user_input))\n\n    # Appel \u00e0 l'API\n    response = client.chat(\n        model=\"mistral-tiny\",\n        messages=messages,\n    )\n\n    # R\u00e9cup\u00e9rer la r\u00e9ponse\n    assistant_message = response.choices[0].message.content\n\n    # Ajouter la r\u00e9ponse au contexte\n    messages.append(ChatMessage(role=\"assistant\", content=assistant_message))\n\n    return assistant_message, messages\n\n# Initialiser la conversation\nconversation = [\n    ChatMessage(role=\"system\", content=\"Tu es un assistant p\u00e9dagogique sp\u00e9cialis\u00e9 dans l'explication du Deep Learning pour des \u00e9tudiants de BTS SIO.\")\n]\n\n# Premier \u00e9change\nresponse, conversation = chat_with_context(conversation, \"Qu'est-ce qu'un r\u00e9seau de neurones?\")\nprint(\"Assistant:\", response)\n\n# Deuxi\u00e8me \u00e9change (avec le contexte pr\u00e9c\u00e9dent)\nresponse, conversation = chat_with_context(conversation, \"Comment fonctionne l'apprentissage?\")\nprint(\"Assistant:\", response)\n</code></pre>"},{"location":"module4/preparation-projet/#optimisation-des-prompts","title":"Optimisation des prompts","text":"<p>La qualit\u00e9 des r\u00e9ponses d\u00e9pend beaucoup de la fa\u00e7on dont vous formulez vos instructions (prompts). Voici quelques conseils pour les optimiser :</p>"},{"location":"module4/preparation-projet/#1-instructions-systeme-claires-et-detaillees","title":"1. Instructions syst\u00e8me claires et d\u00e9taill\u00e9es","text":"<pre><code>system_prompt = \"\"\"\nTu es un assistant p\u00e9dagogique sp\u00e9cialis\u00e9 dans le Deep Learning pour des \u00e9tudiants de BTS SIO. \nQuand tu r\u00e9ponds:\n1. Utilise un langage simple et accessible\n2. Fournis toujours un exemple concret\n3. Structure tes explications en plusieurs points\n4. Si tu n'es pas s\u00fbr d'une information, indique-le clairement\n5. Adapte le niveau technique au profil de l'\u00e9tudiant (d\u00e9butant, interm\u00e9diaire, avanc\u00e9)\n\"\"\"\n</code></pre>"},{"location":"module4/preparation-projet/#2-enrichissement-avec-la-base-de-connaissances","title":"2. Enrichissement avec la base de connaissances","text":"<pre><code>def enrich_prompt_with_knowledge(user_query, knowledge_base, user_level=\"d\u00e9butant\"):\n    # Rechercher des informations pertinentes dans la base de connaissances\n    relevant_info = search_knowledge_base(user_query, knowledge_base)\n\n    # Enrichir le prompt avec ces informations\n    enriched_prompt = f\"\"\"\nQuestion de l'utilisateur: {user_query}\n\nInformations pertinentes (niveau: {user_level}):\n{relevant_info}\n\nR\u00e9ponds de mani\u00e8re p\u00e9dagogique en utilisant ces informations et en adaptant ton explication au niveau {user_level}.\n\"\"\"\n    return enriched_prompt\n</code></pre>"},{"location":"module4/preparation-projet/#3-parametrage-adapte","title":"3. Param\u00e9trage adapt\u00e9","text":"<pre><code># Pour des explications techniques (plus pr\u00e9cises, moins cr\u00e9atives)\ntechnical_params = {\n    \"temperature\": 0.3,  # Faible temp\u00e9rature pour des r\u00e9ponses plus d\u00e9terministes\n    \"max_tokens\": 500    # Limite de longueur raisonnable\n}\n\n# Pour des exemples et analogies (plus cr\u00e9atifs)\ncreative_params = {\n    \"temperature\": 0.7,  # Temp\u00e9rature plus \u00e9lev\u00e9e pour plus de cr\u00e9ativit\u00e9\n    \"max_tokens\": 300    # Limite de longueur adapt\u00e9e\n}\n\n# Fonction de choix de param\u00e8tres selon le contexte\ndef get_params_for_query(query):\n    if \"explique\" in query.lower() or \"d\u00e9finition\" in query.lower():\n        return technical_params\n    elif \"exemple\" in query.lower() or \"analogie\" in query.lower():\n        return creative_params\n    else:\n        return {\"temperature\": 0.5, \"max_tokens\": 400}  # Param\u00e8tres par d\u00e9faut\n</code></pre>"},{"location":"module4/preparation-projet/#4-planification-et-organisation-10-min","title":"4. Planification et organisation (10 min)","text":""},{"location":"module4/preparation-projet/#structure-recommandee-du-projet","title":"Structure recommand\u00e9e du projet","text":"<pre><code>chatbot-pedagogique/\n\u251c\u2500\u2500 app.py                   # Application principale Flask/FastAPI\n\u251c\u2500\u2500 config.py                # Configuration (cl\u00e9s API, param\u00e8tres)\n\u251c\u2500\u2500 templates/               # Templates HTML\n\u2502   \u2514\u2500\u2500 index.html           # Interface web\n\u251c\u2500\u2500 static/                  # Fichiers statiques (CSS, JS)\n\u251c\u2500\u2500 services/                # Services m\u00e9tier\n\u2502   \u251c\u2500\u2500 mistral_service.py   # Int\u00e9gration API Mistral\n\u2502   \u2514\u2500\u2500 knowledge_service.py # Gestion base de connaissances\n\u2514\u2500\u2500 knowledge_base/          # Base de connaissances\n    \u2514\u2500\u2500 concepts.json        # Structure des concepts DL\n</code></pre>"},{"location":"module4/preparation-projet/#repartition-des-taches","title":"R\u00e9partition des t\u00e2ches","text":"<p>Si vous travaillez en bin\u00f4me, une r\u00e9partition efficace des t\u00e2ches pourrait \u00eatre :</p> <p>Option 1: R\u00e9partition par couche - Membre 1: Backend (Python, API Mistral, logique m\u00e9tier) - Membre 2: Frontend (HTML/CSS/JS, interface, exp\u00e9rience utilisateur)</p> <p>Option 2: R\u00e9partition par fonctionnalit\u00e9 - Membre 1: Interface conversationnelle + int\u00e9gration API - Membre 2: Base de connaissances + fonctionnalit\u00e9s p\u00e9dagogiques</p>"},{"location":"module4/preparation-projet/#planning-recommande","title":"Planning recommand\u00e9","text":"<p>Pour le d\u00e9veloppement proprement dit (S\u00e9ance 4), voici un planning sugg\u00e9r\u00e9 :</p> Temps T\u00e2che Objectif 0h00-0h30 Mise en place de l'environnement Structure du projet, installation des d\u00e9pendances 0h30-1h15 D\u00e9veloppement du MVP Interface de base, int\u00e9gration API simple 1h15-2h00 Base de connaissances Structuration et int\u00e9gration 2h00-2h30 Fonctionnalit\u00e9s p\u00e9dagogiques Quiz, adaptation au niveau 2h30-3h00 Tests et corrections Validation fonctionnelle 3h00-3h30 Documentation et pr\u00e9paration Documentation technique et pr\u00e9sentation 3h30-4h00 Pr\u00e9sentations D\u00e9monstration du chatbot"},{"location":"module4/preparation-projet/#5-exemple-de-base-de-connaissances","title":"5. Exemple de base de connaissances","text":"<p>Voici un exemple simplifi\u00e9 de structure pour votre base de connaissances :</p> <pre><code>{\n  \"concepts\": [\n    {\n      \"id\": \"neural_network\",\n      \"title\": \"R\u00e9seau de neurones\",\n      \"description\": \"Mod\u00e8le de calcul inspir\u00e9 du fonctionnement des neurones biologiques.\",\n      \"levels\": {\n        \"beginner\": \"Un r\u00e9seau de neurones est comme un ensemble de filtres interconnect\u00e9s qui apprennent \u00e0 reconna\u00eetre des motifs dans les donn\u00e9es, un peu comme votre cerveau apprend \u00e0 reconna\u00eetre des visages ou des objets.\",\n        \"intermediate\": \"Syst\u00e8me compos\u00e9 de neurones artificiels organis\u00e9s en couches qui transforment des entr\u00e9es en sorties \u00e0 travers des poids et des fonctions d'activation, permettant d'approximer des fonctions complexes par apprentissage.\",\n        \"advanced\": \"Structure math\u00e9matique compos\u00e9e d'unit\u00e9s de calcul interconnect\u00e9es qui effectuent des transformations non-lin\u00e9aires successives sur les donn\u00e9es d'entr\u00e9e, optimis\u00e9es par r\u00e9tropropagation du gradient pour minimiser une fonction de co\u00fbt.\"\n      },\n      \"examples\": [\n        \"Reconnaissance d'images: un r\u00e9seau peut apprendre \u00e0 identifier des chats dans des photos\",\n        \"Traduction automatique: des r\u00e9seaux traduisent du texte d'une langue \u00e0 une autre\"\n      ],\n      \"analogies\": [\n        \"Un r\u00e9seau de neurones ressemble \u00e0 une cha\u00eene de traitement dans une usine, o\u00f9 chaque station (neurone) effectue une op\u00e9ration sp\u00e9cifique sur le produit qui passe.\",\n        \"Comme un orchestre o\u00f9 chaque musicien (neurone) joue une petite partie, et ensemble ils cr\u00e9ent une symphonie complexe (pr\u00e9diction).\"\n      ],\n      \"related_concepts\": [\"perceptron\", \"deep_learning\", \"activation_function\"]\n    }\n  ]\n}\n</code></pre>"},{"location":"module4/preparation-projet/#6-conclusion-et-preparation","title":"6. Conclusion et pr\u00e9paration","text":"<p>Pour vous pr\u00e9parer efficacement au d\u00e9veloppement du chatbot lors de la prochaine s\u00e9ance :</p> <ol> <li>Cr\u00e9ez votre compte Mistral AI et obtenez votre cl\u00e9 API</li> <li>Testez l'API avec quelques prompts simples pour vous familiariser</li> <li>R\u00e9fl\u00e9chissez \u00e0 la structure de votre base de connaissances</li> <li>Organisez votre travail en \u00e9quipe si applicable</li> <li>Pr\u00e9parez des questions sur les aspects techniques que vous ne ma\u00eetrisez pas encore compl\u00e8tement</li> </ol> <p>Cette phase de pr\u00e9paration est essentielle pour garantir un d\u00e9veloppement efficace lors de la s\u00e9ance finale. En anticipant les d\u00e9fis et en planifiant votre approche, vous maximiserez vos chances de cr\u00e9er un chatbot p\u00e9dagogique fonctionnel et de qualit\u00e9.</p> <p>Retour \u00e0 l'aper\u00e7u du module Commencer le d\u00e9veloppement</p>"},{"location":"module4/projet-chatbot/","title":"Projet Chatbot P\u00e9dagogique sur le Deep Learning","text":""},{"location":"module4/projet-chatbot/#introduction-au-projet","title":"Introduction au projet","text":"<p>Le projet de chatbot p\u00e9dagogique repr\u00e9sente l'aboutissement de votre parcours dans la formation sur le Deep Learning. Il vous permet de mettre en pratique l'ensemble des connaissances et comp\u00e9tences acquises tout en cr\u00e9ant un outil concret et utile.</p> <p>Ce document pr\u00e9sente en d\u00e9tail la vision, les objectifs et les sp\u00e9cifications du projet.</p>"},{"location":"module4/projet-chatbot/#vision-du-projet","title":"Vision du projet","text":""},{"location":"module4/projet-chatbot/#quest-ce-quun-chatbot-pedagogique","title":"Qu'est-ce qu'un chatbot p\u00e9dagogique ?","text":"<p>Un chatbot p\u00e9dagogique est un assistant virtuel con\u00e7u sp\u00e9cifiquement pour accompagner l'apprentissage. Contrairement aux chatbots commerciaux ou de service client, son objectif principal est de faciliter la compr\u00e9hension de concepts complexes en s'adaptant au niveau et aux besoins de l'apprenant.</p>"},{"location":"module4/projet-chatbot/#pourquoi-un-chatbot-sur-le-deep-learning","title":"Pourquoi un chatbot sur le Deep Learning ?","text":"<p>Le Deep Learning est un domaine technique complexe qui combine math\u00e9matiques, informatique et domaines d'application vari\u00e9s. Pour les \u00e9tudiants de BTS SIO, cette complexit\u00e9 peut repr\u00e9senter un obstacle \u00e0 l'apprentissage. Un assistant conversationnel permet de :</p> <ul> <li>Rendre les concepts abstraits plus accessibles gr\u00e2ce \u00e0 des explications personnalis\u00e9es</li> <li>Proposer des exemples concrets adapt\u00e9s au niveau de compr\u00e9hension de l'\u00e9tudiant</li> <li>Offrir une disponibilit\u00e9 permanente pour r\u00e9pondre aux questions</li> <li>Favoriser un apprentissage \u00e0 rythme personnalis\u00e9</li> </ul>"},{"location":"module4/projet-chatbot/#objectifs-pedagogiques","title":"Objectifs p\u00e9dagogiques","text":"<p>Votre chatbot doit permettre aux utilisateurs de :</p> <ol> <li>Comprendre les concepts fondamentaux du Deep Learning de mani\u00e8re progressive</li> <li>Explorer les diff\u00e9rentes architectures de r\u00e9seaux de neurones (CNN, RNN, etc.)</li> <li>Visualiser mentalement le fonctionnement des algorithmes gr\u00e2ce \u00e0 des analogies et des exemples</li> <li>Tester leurs connaissances \u00e0 travers des quiz et exercices interactifs</li> <li>Approfondir certaines notions selon leurs besoins et int\u00e9r\u00eats</li> </ol>"},{"location":"module4/projet-chatbot/#public-cible","title":"Public cible","text":"<p>Le chatbot s'adresse principalement aux :</p> <ul> <li>\u00c9tudiants de BTS SIO d\u00e9couvrant le Deep Learning</li> <li>D\u00e9veloppeurs d\u00e9butants souhaitant int\u00e9grer des fonctionnalit\u00e9s d'IA dans leurs projets</li> <li>Professionnels en reconversion vers les m\u00e9tiers de l'intelligence artificielle</li> </ul>"},{"location":"module4/projet-chatbot/#specifications-fonctionnelles","title":"Sp\u00e9cifications fonctionnelles","text":"<p>Pour la s\u00e9ance de 4h, nous allons nous concentrer sur le d\u00e9veloppement d'un prototype fonctionnel du chatbot avec les fonctionnalit\u00e9s essentielles :</p>"},{"location":"module4/projet-chatbot/#fonctionnalites-minimales-mvp","title":"Fonctionnalit\u00e9s minimales (MVP)","text":"<ul> <li>Interface conversationnelle basique</li> <li>Int\u00e9gration fonctionnelle de l'API Mistral</li> <li>Base de connaissances avec 3-5 concepts cl\u00e9s</li> <li>Ajustement du niveau des r\u00e9ponses (d\u00e9butant/avanc\u00e9)</li> </ul>"},{"location":"module4/projet-chatbot/#fonctionnalites-optionnelles-si-le-temps-le-permet","title":"Fonctionnalit\u00e9s optionnelles (si le temps le permet)","text":"<ul> <li>G\u00e9n\u00e9rateur de quiz simple</li> <li>Syst\u00e8me d'authentification (option SLAM)</li> <li>Arbre de diagnostic (option SISR)</li> <li>Visualisations ou exemples interactifs</li> </ul> <p>\u00c0 la fin de la s\u00e9ance, vous aurez un prototype fonctionnel que vous pourrez continuer \u00e0 am\u00e9liorer ult\u00e9rieurement. L'\u00e9valuation tiendra compte du temps limit\u00e9 et se concentrera sur la qualit\u00e9 de l'impl\u00e9mentation des fonctionnalit\u00e9s de base.</p>"},{"location":"module4/projet-chatbot/#4-integration-technique","title":"4. Int\u00e9gration technique","text":"<ul> <li>Backend en Python avec Flask ou FastAPI</li> <li>Interface frontend en HTML/CSS/JavaScript</li> <li>Int\u00e9gration de l'API Mistral AI pour la g\u00e9n\u00e9ration de r\u00e9ponses</li> <li>Optimisation des prompts par enrichissement via la base de connaissances</li> <li>Gestion du contexte de conversation</li> </ul>"},{"location":"module4/projet-chatbot/#specifications-techniques","title":"Sp\u00e9cifications techniques","text":""},{"location":"module4/projet-chatbot/#architecture-recommandee","title":"Architecture recommand\u00e9e","text":"<pre><code>chatbot-pedagogique/\n\u251c\u2500\u2500 app.py                   # Application principale Flask/FastAPI\n\u251c\u2500\u2500 config.py                # Configuration (cl\u00e9s API, param\u00e8tres)\n\u251c\u2500\u2500 templates/               # Templates HTML\n\u2502   \u2514\u2500\u2500 index.html           # Interface web\n\u251c\u2500\u2500 static/                  # Fichiers statiques (CSS, JS)\n\u2502   \u251c\u2500\u2500 css/\n\u2502   \u2502   \u2514\u2500\u2500 style.css\n\u2502   \u2514\u2500\u2500 js/\n\u2502       \u2514\u2500\u2500 app.js\n\u251c\u2500\u2500 services/                # Services m\u00e9tier\n\u2502   \u251c\u2500\u2500 mistral_service.py   # Int\u00e9gration API Mistral\n\u2502   \u2514\u2500\u2500 knowledge_service.py # Gestion base de connaissances\n\u2514\u2500\u2500 knowledge_base/          # Base de connaissances\n    \u2514\u2500\u2500 concepts.json        # Structure des concepts DL\n</code></pre>"},{"location":"module4/projet-chatbot/#technologies-a-utiliser","title":"Technologies \u00e0 utiliser","text":"<ul> <li>Backend: Python 3.8+ avec Flask ou FastAPI</li> <li>Frontend: HTML5, CSS3, JavaScript (ES6+)</li> <li>API: Mistral AI (via la biblioth\u00e8que officielle)</li> <li>Base de connaissances: Format JSON structur\u00e9</li> <li>D\u00e9ploiement: Local (optionnel: conteneurisation Docker)</li> </ul>"},{"location":"module4/projet-chatbot/#structure-de-la-base-de-connaissances","title":"Structure de la base de connaissances","text":"<p>La base de connaissances doit suivre une structure JSON standardis\u00e9e :</p> <pre><code>{\n  \"concepts\": [\n    {\n      \"id\": \"neural_network\",\n      \"title\": \"R\u00e9seau de neurones\",\n      \"description\": \"Description g\u00e9n\u00e9rale du concept\",\n      \"levels\": {\n        \"beginner\": \"Explication pour d\u00e9butants\",\n        \"intermediate\": \"Explication pour niveau interm\u00e9diaire\",\n        \"advanced\": \"Explication technique avanc\u00e9e\"\n      },\n      \"examples\": [\n        \"Exemple 1 avec description\",\n        \"Exemple 2 avec description\"\n      ],\n      \"analogies\": [\n        \"Analogie 1 expliquant le concept\",\n        \"Analogie 2 expliquant le concept\"\n      ],\n      \"related_concepts\": [\"perceptron\", \"deep_learning\", \"activation_function\"],\n      \"quiz\": [\n        {\n          \"question\": \"Question sur ce concept\",\n          \"options\": [\"Option A\", \"Option B\", \"Option C\", \"Option D\"],\n          \"correct_answer\": 2,\n          \"explanation\": \"Explication de la r\u00e9ponse correcte\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"module4/projet-chatbot/#prompt-engineering","title":"Prompt engineering","text":"<p>Pour obtenir des r\u00e9ponses p\u00e9dagogiques de qualit\u00e9, il faudra soigner le prompt engineering :</p> <pre><code>Tu es un assistant p\u00e9dagogique sp\u00e9cialis\u00e9 dans l'enseignement du Deep Learning aux \u00e9tudiants de BTS SIO.\nNiveau de l'\u00e9tudiant : {niveau}\nConcepts d\u00e9j\u00e0 abord\u00e9s : {concepts_vus}\n\nVoici des informations pertinentes tir\u00e9es de notre base de connaissances :\n{context_from_knowledge_base}\n\nQuestion de l'\u00e9tudiant : {question}\n\nR\u00e9ponds de mani\u00e8re claire et p\u00e9dagogique en :\n1. Donnant une explication adapt\u00e9e au niveau de l'\u00e9tudiant\n2. Utilisant des analogies concr\u00e8tes quand c'est pertinent\n3. Fournissant des exemples pratiques\n4. Reliant ce concept aux notions d\u00e9j\u00e0 abord\u00e9es\n5. Sugg\u00e9rant une prochaine notion \u00e0 explorer\n</code></pre>"},{"location":"module4/projet-chatbot/#criteres-devaluation","title":"Crit\u00e8res d'\u00e9valuation","text":"<p>Votre chatbot sera \u00e9valu\u00e9 selon les crit\u00e8res suivants :</p> Crit\u00e8re Pond\u00e9ration Description Fonctionnalit\u00e9 30% Interface utilisable, r\u00e9ponses coh\u00e9rentes, absence de bugs Qualit\u00e9 p\u00e9dagogique 25% Pertinence des explications, adaptation au niveau, exemples appropri\u00e9s Int\u00e9gration technique 20% Utilisation efficace de l'API, gestion du contexte, optimisation Base de connaissances 15% Structure, couverture des concepts, pr\u00e9cision technique Documentation 10% Guide utilisateur, documentation technique, commentaires code"},{"location":"module4/projet-chatbot/#livrables-attendus","title":"Livrables attendus","text":"<ol> <li>Code source complet du chatbot p\u00e9dagogique</li> <li>Base de connaissances structur\u00e9e sur le Deep Learning</li> <li>Documentation technique expliquant l'architecture et les choix d'impl\u00e9mentation</li> <li>Guide utilisateur pour la prise en main</li> <li>Pr\u00e9sentation de 5 minutes du projet finalis\u00e9</li> </ol>"},{"location":"module4/projet-chatbot/#conseils-pour-reussir","title":"Conseils pour r\u00e9ussir","text":"<ol> <li>Commencez simple: D\u00e9veloppez d'abord les fonctionnalit\u00e9s de base avant d'ajouter des fonctionnalit\u00e9s avanc\u00e9es</li> <li>Organisez votre travail: R\u00e9partissez clairement les t\u00e2ches si vous travaillez en \u00e9quipe</li> <li>Testez r\u00e9guli\u00e8rement: V\u00e9rifiez chaque fonctionnalit\u00e9 d\u00e8s qu'elle est impl\u00e9ment\u00e9e</li> <li>Soignez les prompts: La qualit\u00e9 des r\u00e9ponses d\u00e9pend beaucoup de la qualit\u00e9 des prompts</li> <li>Pensez \u00e0 l'exp\u00e9rience utilisateur: Un chatbot doit \u00eatre intuitif et agr\u00e9able \u00e0 utiliser</li> </ol> <p>Retour \u00e0 l'aper\u00e7u du module Pr\u00e9paration au projet ```</p>"},{"location":"module4/qcm-evaluation-module4/","title":"\ud83d\udcdd QCM d'\u00e9valuation - Module 4 : Projet int\u00e9grateur Chatbot p\u00e9dagogique","text":"<p>Ce QCM vous permettra d'\u00e9valuer votre compr\u00e9hension des concepts et techniques mis en \u0153uvre dans le projet int\u00e9grateur de chatbot p\u00e9dagogique.</p>"},{"location":"module4/qcm-evaluation-module4/#instructions","title":"Instructions","text":"<ul> <li>Cochez la ou les r\u00e9ponses correctes pour chaque question</li> <li>Certaines questions peuvent avoir plusieurs r\u00e9ponses correctes</li> <li>Dur\u00e9e recommand\u00e9e : 15 minutes</li> </ul>"},{"location":"module4/qcm-evaluation-module4/#questions","title":"Questions","text":""},{"location":"module4/qcm-evaluation-module4/#1-quels-sont-les-composants-essentiels-de-larchitecture-du-chatbot-pedagogique-plusieurs-reponses-possibles","title":"1. Quels sont les composants essentiels de l'architecture du chatbot p\u00e9dagogique ? (plusieurs r\u00e9ponses possibles)","text":"<ul> <li> Interface conversationnelle</li> <li> Moteur graphique 3D</li> <li> Backend Python (Flask/FastAPI)</li> <li> Base de connaissances structur\u00e9e</li> <li> Int\u00e9gration avec l'API Mistral AI</li> </ul>"},{"location":"module4/qcm-evaluation-module4/#2-quest-ce-que-le-prompt-engineering-dans-le-contexte-dun-chatbot-base-sur-une-api-de-llm","title":"2. Qu'est-ce que le \"prompt engineering\" dans le contexte d'un chatbot bas\u00e9 sur une API de LLM ?","text":"<ul> <li> La cr\u00e9ation d'une interface utilisateur rapide et r\u00e9active</li> <li> La conception d'instructions pr\u00e9cises pour guider le mod\u00e8le de langage \u00e0 g\u00e9n\u00e9rer les r\u00e9ponses souhait\u00e9es</li> <li> L'optimisation des performances du serveur backend</li> <li> La programmation de r\u00e8gles grammaticales strictes</li> </ul>"},{"location":"module4/qcm-evaluation-module4/#3-pourquoi-est-il-important-de-structurer-la-base-de-connaissances-du-chatbot-en-niveaux-debutant-intermediaire-avance","title":"3. Pourquoi est-il important de structurer la base de connaissances du chatbot en niveaux (d\u00e9butant, interm\u00e9diaire, avanc\u00e9) ?","text":"<ul> <li> Pour \u00e9conomiser de l'espace de stockage</li> <li> Pour adapter les explications au niveau de compr\u00e9hension de l'utilisateur</li> <li> Pour respecter les limitations de l'API</li> <li> Pour faciliter le d\u00e9veloppement du frontend</li> </ul>"},{"location":"module4/qcm-evaluation-module4/#4-quelles-fonctionnalites-pedagogiques-peuvent-ameliorer-lefficacite-dun-chatbot-dapprentissage-plusieurs-reponses-possibles","title":"4. Quelles fonctionnalit\u00e9s p\u00e9dagogiques peuvent am\u00e9liorer l'efficacit\u00e9 d'un chatbot d'apprentissage ? (plusieurs r\u00e9ponses possibles)","text":"<ul> <li> G\u00e9n\u00e9ration de quiz interactifs</li> <li> Adaptation du niveau technique au profil de l'utilisateur</li> <li> Suggestions de concepts \u00e0 explorer</li> <li> Affichage du code source du chatbot</li> <li> Utilisation d'analogies et d'exemples concrets</li> </ul>"},{"location":"module4/qcm-evaluation-module4/#5-dans-lintegration-avec-lapi-mistral-ai-que-contient-generalement-un-message-system","title":"5. Dans l'int\u00e9gration avec l'API Mistral AI, que contient g\u00e9n\u00e9ralement un message \"system\" ?","text":"<ul> <li> Les erreurs techniques rencontr\u00e9es</li> <li> Des instructions g\u00e9n\u00e9rales sur le comportement que le mod\u00e8le doit adopter</li> <li> Les informations de facturation de l'API</li> <li> L'historique complet des conversations pr\u00e9c\u00e9dentes</li> </ul>"},{"location":"module4/qcm-evaluation-module4/#6-comment-peut-on-optimiser-les-performances-dun-chatbot-utilisant-une-api-externe-comme-mistral-ai-plusieurs-reponses-possibles","title":"6. Comment peut-on optimiser les performances d'un chatbot utilisant une API externe comme Mistral AI ? (plusieurs r\u00e9ponses possibles)","text":"<ul> <li> Mettre en cache les r\u00e9ponses aux questions fr\u00e9quentes</li> <li> Limiter la taille de l'historique de conversation envoy\u00e9 \u00e0 l'API</li> <li> Utiliser des param\u00e8tres de requ\u00eate adapt\u00e9s selon le type de question</li> <li> Augmenter syst\u00e9matiquement la temp\u00e9rature du mod\u00e8le</li> <li> R\u00e9duire la qualit\u00e9 des images g\u00e9n\u00e9r\u00e9es</li> </ul>"},{"location":"module4/qcm-evaluation-module4/#7-quelle-est-la-meilleure-approche-pour-gerer-les-erreurs-de-lapi-dans-un-chatbot-pedagogique","title":"7. Quelle est la meilleure approche pour g\u00e9rer les erreurs de l'API dans un chatbot p\u00e9dagogique ?","text":"<ul> <li> Ignorer les erreurs pour ne pas perturber l'utilisateur</li> <li> Afficher les messages d'erreur techniques bruts</li> <li> Impl\u00e9menter un m\u00e9canisme de fallback avec des r\u00e9ponses pr\u00e9d\u00e9finies</li> <li> Red\u00e9marrer automatiquement la conversation</li> </ul>"},{"location":"module4/qcm-evaluation-module4/#8-comment-enrichir-un-prompt-avec-la-base-de-connaissances-pour-obtenir-des-reponses-plus-pertinentes","title":"8. Comment enrichir un prompt avec la base de connaissances pour obtenir des r\u00e9ponses plus pertinentes ?","text":"<ul> <li> Ajouter l'int\u00e9gralit\u00e9 de la base de connaissances \u00e0 chaque requ\u00eate</li> <li> S\u00e9lectionner les informations pertinentes en fonction de la question de l'utilisateur</li> <li> Demander \u00e0 l'utilisateur de pr\u00e9ciser le concept qu'il souhaite explorer</li> <li> Utiliser exclusivement des mots-cl\u00e9s sans contexte</li> </ul>"},{"location":"module4/qcm-evaluation-module4/#9-quels-elements-doivent-etre-inclus-dans-la-documentation-technique-dun-chatbot-pedagogique-plusieurs-reponses-possibles","title":"9. Quels \u00e9l\u00e9ments doivent \u00eatre inclus dans la documentation technique d'un chatbot p\u00e9dagogique ? (plusieurs r\u00e9ponses possibles)","text":"<ul> <li> Architecture du syst\u00e8me</li> <li> Choix techniques et justifications</li> <li> Instructions d'installation et de configuration</li> <li> Historique complet des conversations des utilisateurs</li> <li> Flux d'ex\u00e9cution des principales fonctionnalit\u00e9s</li> </ul>"},{"location":"module4/qcm-evaluation-module4/#10-quelle-pratique-est-recommandee-pour-gerer-la-cle-api-de-mistral-ai-dans-le-code-du-chatbot","title":"10. Quelle pratique est recommand\u00e9e pour g\u00e9rer la cl\u00e9 API de Mistral AI dans le code du chatbot ?","text":"<ul> <li> La coder en dur dans le fichier principal de l'application</li> <li> La stocker dans un fichier de configuration public</li> <li> Utiliser des variables d'environnement ou un fichier .env</li> <li> La r\u00e9g\u00e9n\u00e9rer \u00e0 chaque lancement de l'application</li> </ul>"},{"location":"module4/qcm-evaluation-module4/#11-quelle-approche-est-la-plus-efficace-pour-adapter-le-niveau-technique-des-reponses-au-profil-de-lutilisateur","title":"11. Quelle approche est la plus efficace pour adapter le niveau technique des r\u00e9ponses au profil de l'utilisateur ?","text":"<ul> <li> Demander syst\u00e9matiquement \u00e0 l'utilisateur son niveau avant chaque question</li> <li> D\u00e9duire le niveau \u00e0 partir du vocabulaire et de la complexit\u00e9 des questions pos\u00e9es</li> <li> Proposer uniquement du contenu de niveau d\u00e9butant</li> <li> Proposer le m\u00eame contenu \u00e0 tous les utilisateurs pour garantir l'\u00e9quit\u00e9</li> </ul>"},{"location":"module4/qcm-evaluation-module4/#12-dans-le-contexte-dun-chatbot-pedagogique-quest-ce-quune-fonction-daide-a-lapprentissage-efficace","title":"12. Dans le contexte d'un chatbot p\u00e9dagogique, qu'est-ce qu'une fonction d'aide \u00e0 l'apprentissage efficace ?","text":"<ul> <li> Une fonction qui g\u00e9n\u00e8re des quiz sur les concepts abord\u00e9s</li> <li> Une fonction qui affiche les statistiques d'utilisation du chatbot</li> <li> Une fonction qui limite le temps de session</li> <li> Une fonction qui corrige automatiquement l'orthographe de l'utilisateur</li> </ul>"},{"location":"module4/qcm-evaluation-module4/#13-quels-parametres-sont-generalement-configurables-lors-des-appels-a-lapi-mistral-ai-plusieurs-reponses-possibles","title":"13. Quels param\u00e8tres sont g\u00e9n\u00e9ralement configurables lors des appels \u00e0 l'API Mistral AI ? (plusieurs r\u00e9ponses possibles)","text":"<ul> <li> La temp\u00e9rature (pour contr\u00f4ler la cr\u00e9ativit\u00e9/d\u00e9terminisme)</li> <li> Le nombre maximum de tokens de la r\u00e9ponse</li> <li> Le mod\u00e8le utilis\u00e9 (mistral-tiny, mistral-small, mistral-medium...)</li> <li> La langue d'entr\u00e9e/sortie</li> <li> La polarit\u00e9 \u00e9motionnelle des r\u00e9ponses</li> </ul>"},{"location":"module4/qcm-evaluation-module4/#14-quelle-structure-est-recommandee-pour-organiser-la-base-de-connaissances-dun-chatbot-pedagogique","title":"14. Quelle structure est recommand\u00e9e pour organiser la base de connaissances d'un chatbot p\u00e9dagogique ?","text":"<ul> <li> Un fichier texte unique contenant toutes les informations</li> <li> Une structure JSON hi\u00e9rarchique avec concepts, niveaux et exemples</li> <li> Une base de donn\u00e9es SQL avec tables normalis\u00e9es</li> <li> Un ensemble de documents Word</li> </ul>"},{"location":"module4/qcm-evaluation-module4/#15-comment-optimiser-lexperience-utilisateur-dans-linterface-conversationnelle-dun-chatbot-plusieurs-reponses-possibles","title":"15. Comment optimiser l'exp\u00e9rience utilisateur dans l'interface conversationnelle d'un chatbot ? (plusieurs r\u00e9ponses possibles)","text":"<ul> <li> Afficher un indicateur pendant le chargement des r\u00e9ponses</li> <li> Proposer des suggestions de questions</li> <li> Permettre de r\u00e9initialiser la conversation</li> <li> Imposer un d\u00e9lai minimum entre chaque message</li> <li> Afficher l'historique des conversations</li> </ul>"},{"location":"module4/qcm-evaluation-module4/#reponses","title":"R\u00e9ponses","text":"<ol> <li>Interface conversationnelle; Backend Python (Flask/FastAPI); Base de connaissances structur\u00e9e; Int\u00e9gration avec l'API Mistral AI</li> <li>La conception d'instructions pr\u00e9cises pour guider le mod\u00e8le de langage \u00e0 g\u00e9n\u00e9rer les r\u00e9ponses souhait\u00e9es</li> <li>Pour adapter les explications au niveau de compr\u00e9hension de l'utilisateur</li> <li>G\u00e9n\u00e9ration de quiz interactifs; Adaptation du niveau technique au profil de l'utilisateur; Suggestions de concepts \u00e0 explorer; Utilisation d'analogies et d'exemples concrets</li> <li>Des instructions g\u00e9n\u00e9rales sur le comportement que le mod\u00e8le doit adopter</li> <li>Mettre en cache les r\u00e9ponses aux questions fr\u00e9quentes; Limiter la taille de l'historique de conversation envoy\u00e9 \u00e0 l'API; Utiliser des param\u00e8tres de requ\u00eate adapt\u00e9s selon le type de question</li> <li>Impl\u00e9menter un m\u00e9canisme de fallback avec des r\u00e9ponses pr\u00e9d\u00e9finies</li> <li>S\u00e9lectionner les informations pertinentes en fonction de la question de l'utilisateur</li> <li>Architecture du syst\u00e8me; Choix techniques et justifications; Instructions d'installation et de configuration; Flux d'ex\u00e9cution des principales fonctionnalit\u00e9s</li> <li>Utiliser des variables d'environnement ou un fichier .env</li> <li>D\u00e9duire le niveau \u00e0 partir du vocabulaire et de la complexit\u00e9 des questions pos\u00e9es</li> <li>Une fonction qui g\u00e9n\u00e8re des quiz sur les concepts abord\u00e9s</li> <li>La temp\u00e9rature (pour contr\u00f4ler la cr\u00e9ativit\u00e9/d\u00e9terminisme); Le nombre maximum de tokens de la r\u00e9ponse; Le mod\u00e8le utilis\u00e9 (mistral-tiny, mistral-small, mistral-medium...)</li> <li>Une structure JSON hi\u00e9rarchique avec concepts, niveaux et exemples</li> <li>Afficher un indicateur pendant le chargement des r\u00e9ponses; Proposer des suggestions de questions; Permettre de r\u00e9initialiser la conversation; Afficher l'historique des conversations</li> </ol>"},{"location":"module4/ressources/api-integration-template/","title":"Api integration template","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nTemplate d'int\u00e9gration API Mistral pour le chatbot p\u00e9dagogique\nProjet BTS SIO - S\u00e9ance de 4h\n\"\"\"\n</pre> \"\"\" Template d'int\u00e9gration API Mistral pour le chatbot p\u00e9dagogique Projet BTS SIO - S\u00e9ance de 4h \"\"\" In\u00a0[\u00a0]: Copied! <pre>from flask import Flask, request, jsonify\nimport os\nfrom mistralai.client import MistralClient\nfrom mistralai.models.chat_completion import ChatMessage\nimport json\n</pre> from flask import Flask, request, jsonify import os from mistralai.client import MistralClient from mistralai.models.chat_completion import ChatMessage import json In\u00a0[\u00a0]: Copied! <pre># Configuration de l'application Flask\napp = Flask(__name__)\napp.config['JSON_AS_ASCII'] = False  # Pour g\u00e9rer les accents correctement\n</pre> # Configuration de l'application Flask app = Flask(__name__) app.config['JSON_AS_ASCII'] = False  # Pour g\u00e9rer les accents correctement In\u00a0[\u00a0]: Copied! <pre># TODO: Remplacez par votre cl\u00e9 API\nMISTRAL_API_KEY = \"votre_cl\u00e9_api_ici\"\n</pre> # TODO: Remplacez par votre cl\u00e9 API MISTRAL_API_KEY = \"votre_cl\u00e9_api_ici\" In\u00a0[\u00a0]: Copied! <pre># Initialisation du client Mistral\nclient = MistralClient(api_key=MISTRAL_API_KEY)\n</pre> # Initialisation du client Mistral client = MistralClient(api_key=MISTRAL_API_KEY) In\u00a0[\u00a0]: Copied! <pre># Configuration du mod\u00e8le\nMODEL = \"mistral-tiny\"  # Options: mistral-tiny, mistral-small, mistral-medium\n</pre> # Configuration du mod\u00e8le MODEL = \"mistral-tiny\"  # Options: mistral-tiny, mistral-small, mistral-medium In\u00a0[\u00a0]: Copied! <pre>def load_knowledge_base():\n    \"\"\"Charge la base de connaissances depuis le fichier JSON\"\"\"\n    try:\n        # TODO: Changez le chemin vers votre base de connaissances\n        with open('knowledge/knowledge_base.json', 'r', encoding='utf-8') as f:\n            return json.load(f)\n    except Exception as e:\n        print(f\"Erreur lors du chargement de la base de connaissances: {e}\")\n        # Retourner une base vide en cas d'erreur\n        return {\"concepts\": []}\n</pre> def load_knowledge_base():     \"\"\"Charge la base de connaissances depuis le fichier JSON\"\"\"     try:         # TODO: Changez le chemin vers votre base de connaissances         with open('knowledge/knowledge_base.json', 'r', encoding='utf-8') as f:             return json.load(f)     except Exception as e:         print(f\"Erreur lors du chargement de la base de connaissances: {e}\")         # Retourner une base vide en cas d'erreur         return {\"concepts\": []} In\u00a0[\u00a0]: Copied! <pre>def find_relevant_concepts(question, knowledge_base, max_concepts=2):\n    \"\"\"\n    Recherche les concepts pertinents dans la base de connaissances\n    en fonction de la question pos\u00e9e\n    \"\"\"\n    # Version simple: recherche par mots-cl\u00e9s\n    relevant_concepts = []\n    \n    # Extraction des mots-cl\u00e9s de la question\n    keywords = question.lower().split()\n    \n    # Parcours des concepts\n    for concept in knowledge_base.get(\"concepts\", []):\n        # Calcul d'un score simple de pertinence\n        score = sum(1 for keyword in keywords if keyword in concept[\"title\"].lower())\n        \n        if score &gt; 0:\n            relevant_concepts.append({\n                \"concept\": concept,\n                \"score\": score\n            })\n    \n    # Tri par score de pertinence\n    relevant_concepts.sort(key=lambda x: x[\"score\"], reverse=True)\n    \n    # Retourne les concepts les plus pertinents\n    return [item[\"concept\"] for item in relevant_concepts[:max_concepts]]\n</pre> def find_relevant_concepts(question, knowledge_base, max_concepts=2):     \"\"\"     Recherche les concepts pertinents dans la base de connaissances     en fonction de la question pos\u00e9e     \"\"\"     # Version simple: recherche par mots-cl\u00e9s     relevant_concepts = []          # Extraction des mots-cl\u00e9s de la question     keywords = question.lower().split()          # Parcours des concepts     for concept in knowledge_base.get(\"concepts\", []):         # Calcul d'un score simple de pertinence         score = sum(1 for keyword in keywords if keyword in concept[\"title\"].lower())                  if score &gt; 0:             relevant_concepts.append({                 \"concept\": concept,                 \"score\": score             })          # Tri par score de pertinence     relevant_concepts.sort(key=lambda x: x[\"score\"], reverse=True)          # Retourne les concepts les plus pertinents     return [item[\"concept\"] for item in relevant_concepts[:max_concepts]] In\u00a0[\u00a0]: Copied! <pre>def create_system_prompt(user_level=\"beginner\"):\n    \"\"\"\n    Cr\u00e9e le prompt syst\u00e8me avec les instructions pour l'assistant\n    \"\"\"\n    # TODO: Personnalisez ce prompt syst\u00e8me selon votre option (SISR/SLAM)\n    if os.getenv(\"CHATBOT_OPTION\") == \"sisr\":\n        # Prompt pour l'option SISR\n        return f\"\"\"\n        Vous \u00eates un assistant p\u00e9dagogique sp\u00e9cialis\u00e9 en r\u00e9seau, syst\u00e8me et Deep Learning.\n        Votre objectif est d'aider les techniciens \u00e0 diagnostiquer et r\u00e9soudre des probl\u00e8mes\n        tout en leur expliquant les concepts de Deep Learning.\n        \n        Niveau de l'utilisateur: {user_level}\n        \n        Directives:\n        1. Utilisez un langage clair et adapt\u00e9 au niveau technique de l'utilisateur\n        2. Pour les questions de diagnostic, proc\u00e9dez par \u00e9tapes syst\u00e9matiques\n        3. Illustrez les concepts avec des exemples concrets du domaine r\u00e9seau/syst\u00e8me\n        4. Soyez concis mais pr\u00e9cis dans vos explications\n        \"\"\"\n    else:\n        # Prompt pour l'option SLAM\n        return f\"\"\"\n        Vous \u00eates un assistant p\u00e9dagogique sp\u00e9cialis\u00e9 en d\u00e9veloppement logiciel et Deep Learning.\n        Votre objectif est d'aider les d\u00e9veloppeurs \u00e0 comprendre des concepts de programmation\n        tout en leur expliquant les concepts de Deep Learning.\n        \n        Niveau de l'utilisateur: {user_level}\n        \n        Directives:\n        1. Utilisez un langage clair et adapt\u00e9 au niveau technique de l'utilisateur\n        2. Incluez des exemples de code quand c'est pertinent\n        3. Expliquez les concepts en faisant des parall\u00e8les avec le d\u00e9veloppement logiciel\n        4. Soyez concis mais pr\u00e9cis dans vos explications\n        \"\"\"\n</pre> def create_system_prompt(user_level=\"beginner\"):     \"\"\"     Cr\u00e9e le prompt syst\u00e8me avec les instructions pour l'assistant     \"\"\"     # TODO: Personnalisez ce prompt syst\u00e8me selon votre option (SISR/SLAM)     if os.getenv(\"CHATBOT_OPTION\") == \"sisr\":         # Prompt pour l'option SISR         return f\"\"\"         Vous \u00eates un assistant p\u00e9dagogique sp\u00e9cialis\u00e9 en r\u00e9seau, syst\u00e8me et Deep Learning.         Votre objectif est d'aider les techniciens \u00e0 diagnostiquer et r\u00e9soudre des probl\u00e8mes         tout en leur expliquant les concepts de Deep Learning.                  Niveau de l'utilisateur: {user_level}                  Directives:         1. Utilisez un langage clair et adapt\u00e9 au niveau technique de l'utilisateur         2. Pour les questions de diagnostic, proc\u00e9dez par \u00e9tapes syst\u00e9matiques         3. Illustrez les concepts avec des exemples concrets du domaine r\u00e9seau/syst\u00e8me         4. Soyez concis mais pr\u00e9cis dans vos explications         \"\"\"     else:         # Prompt pour l'option SLAM         return f\"\"\"         Vous \u00eates un assistant p\u00e9dagogique sp\u00e9cialis\u00e9 en d\u00e9veloppement logiciel et Deep Learning.         Votre objectif est d'aider les d\u00e9veloppeurs \u00e0 comprendre des concepts de programmation         tout en leur expliquant les concepts de Deep Learning.                  Niveau de l'utilisateur: {user_level}                  Directives:         1. Utilisez un langage clair et adapt\u00e9 au niveau technique de l'utilisateur         2. Incluez des exemples de code quand c'est pertinent         3. Expliquez les concepts en faisant des parall\u00e8les avec le d\u00e9veloppement logiciel         4. Soyez concis mais pr\u00e9cis dans vos explications         \"\"\" In\u00a0[\u00a0]: Copied! <pre>def enrich_prompt_with_knowledge(question, relevant_concepts, user_level=\"beginner\"):\n    \"\"\"\n    Enrichit la question utilisateur avec des informations de la base de connaissances\n    \"\"\"\n    if not relevant_concepts:\n        return question\n    \n    enriched_prompt = f\"Question utilisateur: {question}\\n\\n\"\n    enriched_prompt += \"Informations contextuelles pour vous aider \u00e0 r\u00e9pondre:\\n\\n\"\n    \n    for concept in relevant_concepts:\n        # Ajouter le titre et la description g\u00e9n\u00e9rale\n        enriched_prompt += f\"Concept: {concept['title']}\\n\"\n        if \"description\" in concept:\n            enriched_prompt += f\"Description: {concept['description']}\\n\"\n        \n        # Ajouter l'explication adapt\u00e9e au niveau de l'utilisateur\n        if \"levels\" in concept and user_level in concept[\"levels\"]:\n            enriched_prompt += f\"Explication ({user_level}): {concept['levels'][user_level]}\\n\"\n        \n        # Ajouter un exemple pertinent\n        if \"examples\" in concept and concept[\"examples\"]:\n            example = concept[\"examples\"][0]\n            if isinstance(example, dict) and \"description\" in example:\n                enriched_prompt += f\"Exemple: {example['description']}\\n\"\n            elif isinstance(example, str):\n                enriched_prompt += f\"Exemple: {example}\\n\"\n        \n        enriched_prompt += \"\\n\"\n    \n    enriched_prompt += f\"R\u00e9ponds maintenant \u00e0 la question de l'utilisateur en te basant sur ces informations, adapt\u00e9 au niveau {user_level}.\"\n    \n    return enriched_prompt\n</pre> def enrich_prompt_with_knowledge(question, relevant_concepts, user_level=\"beginner\"):     \"\"\"     Enrichit la question utilisateur avec des informations de la base de connaissances     \"\"\"     if not relevant_concepts:         return question          enriched_prompt = f\"Question utilisateur: {question}\\n\\n\"     enriched_prompt += \"Informations contextuelles pour vous aider \u00e0 r\u00e9pondre:\\n\\n\"          for concept in relevant_concepts:         # Ajouter le titre et la description g\u00e9n\u00e9rale         enriched_prompt += f\"Concept: {concept['title']}\\n\"         if \"description\" in concept:             enriched_prompt += f\"Description: {concept['description']}\\n\"                  # Ajouter l'explication adapt\u00e9e au niveau de l'utilisateur         if \"levels\" in concept and user_level in concept[\"levels\"]:             enriched_prompt += f\"Explication ({user_level}): {concept['levels'][user_level]}\\n\"                  # Ajouter un exemple pertinent         if \"examples\" in concept and concept[\"examples\"]:             example = concept[\"examples\"][0]             if isinstance(example, dict) and \"description\" in example:                 enriched_prompt += f\"Exemple: {example['description']}\\n\"             elif isinstance(example, str):                 enriched_prompt += f\"Exemple: {example}\\n\"                  enriched_prompt += \"\\n\"          enriched_prompt += f\"R\u00e9ponds maintenant \u00e0 la question de l'utilisateur en te basant sur ces informations, adapt\u00e9 au niveau {user_level}.\"          return enriched_prompt In\u00a0[\u00a0]: Copied! <pre>@app.route('/api/chat', methods=['POST'])\ndef chat():\n    \"\"\"\n    Point d'entr\u00e9e principal pour les requ\u00eates de chat\n    Attend un JSON avec:\n    - message: le message utilisateur\n    - history: l'historique de conversation (optionnel)\n    - user_level: le niveau de l'utilisateur (optionnel)\n    \"\"\"\n    try:\n        data = request.json\n        \n        if not data or 'message' not in data:\n            return jsonify({\"error\": \"Le message est requis\"}), 400\n        \n        user_message = data['message']\n        history = data.get('history', [])\n        user_level = data.get('user_level', 'beginner')\n        \n        # Chargement de la base de connaissances\n        knowledge_base = load_knowledge_base()\n        \n        # Recherche des concepts pertinents\n        relevant_concepts = find_relevant_concepts(user_message, knowledge_base)\n        \n        # Construction des messages pour l'API\n        messages = []\n        \n        # Ajout du prompt syst\u00e8me\n        system_prompt = create_system_prompt(user_level)\n        messages.append(ChatMessage(role=\"system\", content=system_prompt))\n        \n        # Ajout de l'historique\n        for msg in history:\n            role = msg.get('role', 'user')\n            content = msg.get('content', '')\n            if role and content:\n                messages.append(ChatMessage(role=role, content=content))\n        \n        # Ajout du message utilisateur enrichi\n        enriched_prompt = enrich_prompt_with_knowledge(user_message, relevant_concepts, user_level)\n        messages.append(ChatMessage(role=\"user\", content=enriched_prompt))\n        \n        # Appel \u00e0 l'API Mistral\n        chat_response = client.chat(\n            model=MODEL,\n            messages=messages,\n            temperature=0.7,\n            max_tokens=1000\n        )\n        \n        # Extraction de la r\u00e9ponse\n        response_content = chat_response.choices[0].message.content\n        \n        return jsonify({\n            \"response\": response_content,\n            \"enriched\": bool(relevant_concepts)  # Indique si la r\u00e9ponse a \u00e9t\u00e9 enrichie\n        })\n        \n    except Exception as e:\n        print(f\"Erreur lors de l'appel \u00e0 l'API: {e}\")\n        return jsonify({\n            \"error\": \"Une erreur est survenue lors du traitement de votre demande\",\n            \"details\": str(e)\n        }), 500\n</pre> @app.route('/api/chat', methods=['POST']) def chat():     \"\"\"     Point d'entr\u00e9e principal pour les requ\u00eates de chat     Attend un JSON avec:     - message: le message utilisateur     - history: l'historique de conversation (optionnel)     - user_level: le niveau de l'utilisateur (optionnel)     \"\"\"     try:         data = request.json                  if not data or 'message' not in data:             return jsonify({\"error\": \"Le message est requis\"}), 400                  user_message = data['message']         history = data.get('history', [])         user_level = data.get('user_level', 'beginner')                  # Chargement de la base de connaissances         knowledge_base = load_knowledge_base()                  # Recherche des concepts pertinents         relevant_concepts = find_relevant_concepts(user_message, knowledge_base)                  # Construction des messages pour l'API         messages = []                  # Ajout du prompt syst\u00e8me         system_prompt = create_system_prompt(user_level)         messages.append(ChatMessage(role=\"system\", content=system_prompt))                  # Ajout de l'historique         for msg in history:             role = msg.get('role', 'user')             content = msg.get('content', '')             if role and content:                 messages.append(ChatMessage(role=role, content=content))                  # Ajout du message utilisateur enrichi         enriched_prompt = enrich_prompt_with_knowledge(user_message, relevant_concepts, user_level)         messages.append(ChatMessage(role=\"user\", content=enriched_prompt))                  # Appel \u00e0 l'API Mistral         chat_response = client.chat(             model=MODEL,             messages=messages,             temperature=0.7,             max_tokens=1000         )                  # Extraction de la r\u00e9ponse         response_content = chat_response.choices[0].message.content                  return jsonify({             \"response\": response_content,             \"enriched\": bool(relevant_concepts)  # Indique si la r\u00e9ponse a \u00e9t\u00e9 enrichie         })              except Exception as e:         print(f\"Erreur lors de l'appel \u00e0 l'API: {e}\")         return jsonify({             \"error\": \"Une erreur est survenue lors du traitement de votre demande\",             \"details\": str(e)         }), 500 In\u00a0[\u00a0]: Copied! <pre>@app.route('/api/health', methods=['GET'])\ndef health_check():\n    \"\"\"\n    Route simple pour v\u00e9rifier que l'API fonctionne\n    \"\"\"\n    return jsonify({\"status\": \"OK\", \"message\": \"API op\u00e9rationnelle\"})\n</pre> @app.route('/api/health', methods=['GET']) def health_check():     \"\"\"     Route simple pour v\u00e9rifier que l'API fonctionne     \"\"\"     return jsonify({\"status\": \"OK\", \"message\": \"API op\u00e9rationnelle\"}) In\u00a0[\u00a0]: Copied! <pre>@app.route('/api/diagnostic', methods=['POST'])\ndef diagnostic():\n    \"\"\"\n    Route sp\u00e9cifique pour le diagnostic (option SISR)\n    \"\"\"\n    # TODO: Impl\u00e9mentez la logique de diagnostic pour SISR\n    if os.getenv(\"CHATBOT_OPTION\") != \"sisr\":\n        return jsonify({\"error\": \"Cette fonctionnalit\u00e9 est disponible uniquement pour l'option SISR\"}), 400\n    \n    try:\n        data = request.json\n        problem_type = data.get('problem_type')\n        current_step = data.get('current_step', 'start')\n        \n        # Exemple simple d'arbre de d\u00e9cision pour WiFi\n        if problem_type == \"wifi\":\n            if current_step == \"start\":\n                return jsonify({\n                    \"question\": \"Est-ce que le voyant WiFi est allum\u00e9 sur votre appareil?\",\n                    \"options\": [\"Oui\", \"Non\", \"Je ne sais pas\"],\n                    \"next_step\": \"wifi_light\"\n                })\n            elif current_step == \"wifi_light\":\n                answer = data.get('answer')\n                if answer == \"Non\":\n                    return jsonify({\n                        \"solution\": \"Activez le WiFi en utilisant le bouton ou la combinaison de touches (souvent Fn+F2/F3).\",\n                        \"finished\": True\n                    })\n                else:\n                    return jsonify({\n                        \"question\": \"Voyez-vous le nom de votre r\u00e9seau WiFi dans la liste des r\u00e9seaux disponibles?\",\n                        \"options\": [\"Oui\", \"Non\"],\n                        \"next_step\": \"wifi_network_visible\"\n                    })\n        \n        # R\u00e9ponse par d\u00e9faut\n        return jsonify({\n            \"error\": \"Diagnostic non impl\u00e9ment\u00e9 pour ce type de probl\u00e8me\",\n            \"finished\": True\n        })\n        \n    except Exception as e:\n        return jsonify({\n            \"error\": \"Erreur lors du diagnostic\",\n            \"details\": str(e)\n        }), 500\n</pre> @app.route('/api/diagnostic', methods=['POST']) def diagnostic():     \"\"\"     Route sp\u00e9cifique pour le diagnostic (option SISR)     \"\"\"     # TODO: Impl\u00e9mentez la logique de diagnostic pour SISR     if os.getenv(\"CHATBOT_OPTION\") != \"sisr\":         return jsonify({\"error\": \"Cette fonctionnalit\u00e9 est disponible uniquement pour l'option SISR\"}), 400          try:         data = request.json         problem_type = data.get('problem_type')         current_step = data.get('current_step', 'start')                  # Exemple simple d'arbre de d\u00e9cision pour WiFi         if problem_type == \"wifi\":             if current_step == \"start\":                 return jsonify({                     \"question\": \"Est-ce que le voyant WiFi est allum\u00e9 sur votre appareil?\",                     \"options\": [\"Oui\", \"Non\", \"Je ne sais pas\"],                     \"next_step\": \"wifi_light\"                 })             elif current_step == \"wifi_light\":                 answer = data.get('answer')                 if answer == \"Non\":                     return jsonify({                         \"solution\": \"Activez le WiFi en utilisant le bouton ou la combinaison de touches (souvent Fn+F2/F3).\",                         \"finished\": True                     })                 else:                     return jsonify({                         \"question\": \"Voyez-vous le nom de votre r\u00e9seau WiFi dans la liste des r\u00e9seaux disponibles?\",                         \"options\": [\"Oui\", \"Non\"],                         \"next_step\": \"wifi_network_visible\"                     })                  # R\u00e9ponse par d\u00e9faut         return jsonify({             \"error\": \"Diagnostic non impl\u00e9ment\u00e9 pour ce type de probl\u00e8me\",             \"finished\": True         })              except Exception as e:         return jsonify({             \"error\": \"Erreur lors du diagnostic\",             \"details\": str(e)         }), 500 In\u00a0[\u00a0]: Copied! <pre>@app.route('/api/auth/login', methods=['POST'])\ndef login():\n    \"\"\"\n    Route sp\u00e9cifique pour l'authentification (option SLAM)\n    \"\"\"\n    # TODO: Impl\u00e9mentez la logique d'authentification pour SLAM\n    if os.getenv(\"CHATBOT_OPTION\") != \"slam\":\n        return jsonify({\"error\": \"Cette fonctionnalit\u00e9 est disponible uniquement pour l'option SLAM\"}), 400\n    \n    try:\n        data = request.json\n        username = data.get('username')\n        password = data.get('password')\n        \n        # Authentification simplifi\u00e9e pour la d\u00e9monstration\n        # En production, utilisez une base de donn\u00e9es et hashage des mots de passe\n        valid_users = {\n            \"etudiant\": \"bts2025\",\n            \"admin\": \"admin2025\"\n        }\n        \n        if username in valid_users and valid_users[username] == password:\n            return jsonify({\n                \"success\": True,\n                \"username\": username,\n                \"message\": \"Connexion r\u00e9ussie\"\n            })\n        else:\n            return jsonify({\n                \"success\": False,\n                \"message\": \"Nom d'utilisateur ou mot de passe incorrect\"\n            }), 401\n            \n    except Exception as e:\n        return jsonify({\n            \"error\": \"Erreur lors de l'authentification\",\n            \"details\": str(e)\n        }), 500\n</pre> @app.route('/api/auth/login', methods=['POST']) def login():     \"\"\"     Route sp\u00e9cifique pour l'authentification (option SLAM)     \"\"\"     # TODO: Impl\u00e9mentez la logique d'authentification pour SLAM     if os.getenv(\"CHATBOT_OPTION\") != \"slam\":         return jsonify({\"error\": \"Cette fonctionnalit\u00e9 est disponible uniquement pour l'option SLAM\"}), 400          try:         data = request.json         username = data.get('username')         password = data.get('password')                  # Authentification simplifi\u00e9e pour la d\u00e9monstration         # En production, utilisez une base de donn\u00e9es et hashage des mots de passe         valid_users = {             \"etudiant\": \"bts2025\",             \"admin\": \"admin2025\"         }                  if username in valid_users and valid_users[username] == password:             return jsonify({                 \"success\": True,                 \"username\": username,                 \"message\": \"Connexion r\u00e9ussie\"             })         else:             return jsonify({                 \"success\": False,                 \"message\": \"Nom d'utilisateur ou mot de passe incorrect\"             }), 401                  except Exception as e:         return jsonify({             \"error\": \"Erreur lors de l'authentification\",             \"details\": str(e)         }), 500 In\u00a0[\u00a0]: Copied! <pre>if __name__ == '__main__':\n    # D\u00e9termination de l'option (SISR ou SLAM)\n    option = input(\"Quelle option ? (sisr/slam): \").lower()\n    if option not in [\"sisr\", \"slam\"]:\n        print(\"Option non reconnue, utilisation de l'option par d\u00e9faut: sisr\")\n        option = \"sisr\"\n    \n    # Configuration de l'environnement\n    os.environ[\"CHATBOT_OPTION\"] = option\n    \n    print(f\"D\u00e9marrage du serveur en mode {option.upper()}...\")\n    \n    # Lancement du serveur sur le port 5001 (pour \u00e9viter les conflits avec d'autres services)\n    app.run(debug=True, port=5001)\n</pre> if __name__ == '__main__':     # D\u00e9termination de l'option (SISR ou SLAM)     option = input(\"Quelle option ? (sisr/slam): \").lower()     if option not in [\"sisr\", \"slam\"]:         print(\"Option non reconnue, utilisation de l'option par d\u00e9faut: sisr\")         option = \"sisr\"          # Configuration de l'environnement     os.environ[\"CHATBOT_OPTION\"] = option          print(f\"D\u00e9marrage du serveur en mode {option.upper()}...\")          # Lancement du serveur sur le port 5001 (pour \u00e9viter les conflits avec d'autres services)     app.run(debug=True, port=5001)"},{"location":"module4/ressources/api-integration-template/#configuration-de-lapi-mistral","title":"========================================== CONFIGURATION DE L'API MISTRAL\u00b6","text":""},{"location":"module4/ressources/api-integration-template/#fonctions-de-base-de-connaissances","title":"========================================== FONCTIONS DE BASE DE CONNAISSANCES\u00b6","text":""},{"location":"module4/ressources/api-integration-template/#fonctions-denrichissement-des-prompts","title":"========================================== FONCTIONS D'ENRICHISSEMENT DES PROMPTS\u00b6","text":""},{"location":"module4/ressources/api-integration-template/#routes-de-lapi","title":"========================================== ROUTES DE L'API\u00b6","text":""},{"location":"module4/ressources/api-integration-template/#route-specifique-sisr-diagnostic","title":"========================================== ROUTE SP\u00c9CIFIQUE SISR: DIAGNOSTIC\u00b6","text":""},{"location":"module4/ressources/api-integration-template/#route-specifique-slam-authentification","title":"========================================== ROUTE SP\u00c9CIFIQUE SLAM: AUTHENTIFICATION\u00b6","text":""},{"location":"module4/ressources/api-integration-template/#lancement-de-lapplication","title":"========================================== LANCEMENT DE L'APPLICATION\u00b6","text":""},{"location":"module4/ressources/checklist-preparation/","title":"Checklist de pr\u00e9paration pour le projet chatbot (4h)","text":"<p>Ce document contient les \u00e9l\u00e9ments \u00e0 pr\u00e9parer avant la s\u00e9ance pour maximiser le temps productif lors du d\u00e9veloppement du chatbot p\u00e9dagogique.</p>"},{"location":"module4/ressources/checklist-preparation/#1-compte-et-api-mistral","title":"1. Compte et API Mistral","text":"<ul> <li> Cr\u00e9er un compte sur console.mistral.ai</li> <li> G\u00e9n\u00e9rer une cl\u00e9 API (Menu \"API Keys\" &gt; \"Create API Key\")</li> <li> Noter la cl\u00e9 API dans un endroit s\u00e9curis\u00e9</li> <li> Tester la cl\u00e9 API avec un exemple simple (code fourni ci-dessous)</li> </ul> <p>```python</p>"},{"location":"module4/ressources/checklist-preparation/#script-de-test-de-lapi-mistral-ai","title":"Script de test de l'API Mistral AI","text":"<p>from mistralai.client import MistralClient from mistralai.models.chat_completion import ChatMessage</p>"},{"location":"module4/ressources/checklist-preparation/#remplacez-par-votre-cle-api","title":"Remplacez par votre cl\u00e9 API","text":"<p>api_key = \"votre_cl\u00e9_api_ici\"</p>"},{"location":"module4/ressources/checklist-preparation/#initialisation-du-client","title":"Initialisation du client","text":"<p>client = MistralClient(api_key=api_key)</p>"},{"location":"module4/ressources/checklist-preparation/#message-test","title":"Message test","text":"<p>messages = [     ChatMessage(role=\"system\", content=\"Vous \u00eates un assistant technique sp\u00e9cialis\u00e9 en informatique.\"),     ChatMessage(role=\"user\", content=\"Expliquez-moi ce qu'est un r\u00e9seau de neurones en une phrase simple.\") ]</p>"},{"location":"module4/ressources/checklist-preparation/#test-de-lapi","title":"Test de l'API","text":"<p>try:     chat_response = client.chat(model=\"mistral-tiny\", messages=messages)     print(\"\u2705 API Mistral fonctionnelle!\")     print(\"R\u00e9ponse:\", chat_response.choices[0].message.content) except Exception as e:     print(\"\u274c Erreur lors du test de l'API:\", e)</p> <ol> <li> <p>Environnement de d\u00e9veloppement</p> </li> <li> <p>[ ]Python 3.7+ install\u00e9 et fonctionnel</p> </li> <li>[ ]Pip install\u00e9 et \u00e0 jour</li> <li>[ ]Biblioth\u00e8ques requises install\u00e9es: pip install mistralai flask requests</li> </ol> <p>\u00c9diteur de code configur\u00e9 (VS Code recommand\u00e9)  Extension Live Server pour VS Code (ou \u00e9quivalent)</p> <ol> <li> <p>Connaissances \u00e0 r\u00e9viser</p> </li> <li> <p> Bases de HTML/CSS/JavaScript (manipulation du DOM)</p> </li> <li>[ ]Requ\u00eates AJAX ou Fetch API</li> <li>[ ]Bases de Flask (routes, templates, API)</li> <li>[ ]Format JSON (structure, manipulation)</li> <li> <p>[ ]Prompt engineering (concepts de base)</p> </li> <li> <p>T\u00e9l\u00e9chargements pr\u00e9alables</p> </li> <li> <p>[ ]Kit de d\u00e9marrage correspondant \u00e0 votre option (SISR/SLAM)</p> </li> <li>[ ]Document de projet avec les instructions d\u00e9taill\u00e9es</li> <li> <p>[ ]Grille d'auto-\u00e9valuation</p> </li> <li> <p>\u00c9quipe et organisation</p> </li> <li> <p>[ ]Former votre \u00e9quipe (2-3 personnes recommand\u00e9)</p> </li> <li>[ ]Identifier les points forts de chaque membre</li> <li>[ ]Choisir votre option (SISR ou SLAM)</li> <li>[ ]Pr\u00e9voir un moyen de partage de code rapide (Git, \u00e9change de fichiers)</li> </ol> <p>Notes importantes</p> <ul> <li>La s\u00e9ance dure seulement 4 heures - Chaque minute compte!</li> <li>L'objectif est un MVP fonctionnel - Visez la simplicit\u00e9 et l'efficacit\u00e9</li> <li>Utilisez le kit de d\u00e9marrage - Ne partez pas de z\u00e9ro</li> <li>Priorisez les fonctionnalit\u00e9s - Commencez par les \u00e9l\u00e9ments essentiels</li> </ul>"},{"location":"module4/ressources/grille-evaluation-projet/","title":"Grille d'\u00e9valuation - Projet chatbot p\u00e9dagogique","text":"<p>Cette grille d\u00e9taille les crit\u00e8res d'\u00e9valuation de votre projet final de chatbot p\u00e9dagogique sur le Deep Learning.</p>"},{"location":"module4/ressources/grille-evaluation-projet/#1-fonctionnalite-30","title":"1. Fonctionnalit\u00e9 (30%)","text":"Crit\u00e8re Description Bar\u00e8me Note Interface utilisateur Qualit\u00e9, intuitivit\u00e9 et r\u00e9activit\u00e9 de l'interface conversationnelle /5 Qualit\u00e9 des r\u00e9ponses Pertinence, pr\u00e9cision et clart\u00e9 des r\u00e9ponses g\u00e9n\u00e9r\u00e9es /10 Gestion du contexte Capacit\u00e9 \u00e0 maintenir une conversation coh\u00e9rente et contextuelle /5 Robustesse Gestion des erreurs et des cas impr\u00e9vus /5 Fonctionnalit\u00e9s sp\u00e9ciales Mise en \u0153uvre des fonctionnalit\u00e9s p\u00e9dagogiques (quiz, progression, etc.) /5 Sous-total /30"},{"location":"module4/ressources/grille-evaluation-projet/#2-qualite-pedagogique-25","title":"2. Qualit\u00e9 p\u00e9dagogique (25%)","text":"Crit\u00e8re Description Bar\u00e8me Note Adaptation au niveau Capacit\u00e9 \u00e0 ajuster les explications selon le niveau de l'utilisateur /5 Clart\u00e9 des explications Facilit\u00e9 de compr\u00e9hension des concepts complexes /5 Exemples pertinents Qualit\u00e9 et pertinence des exemples fournis /5 Analogies efficaces Utilisation d'analogies pour faciliter la compr\u00e9hension /5 Progression d'apprentissage Organisation logique des concepts et suivi de la progression /5 Sous-total /25"},{"location":"module4/ressources/grille-evaluation-projet/#3-integration-technique-20","title":"3. Int\u00e9gration technique (20%)","text":"Crit\u00e8re Description Bar\u00e8me Note Architecture Qualit\u00e9 de l'architecture et des choix techniques /5 Int\u00e9gration API Mistral Optimisation des requ\u00eates et des prompts /5 Performance Temps de r\u00e9ponse et optimisations /5 Code Qualit\u00e9, lisibilit\u00e9 et organisation du code /5 Sous-total /20"},{"location":"module4/ressources/grille-evaluation-projet/#4-base-de-connaissances-15","title":"4. Base de connaissances (15%)","text":"Crit\u00e8re Description Bar\u00e8me Note Structure Organisation logique et hi\u00e9rarchique des concepts /5 Couverture \u00c9tendue des concepts couverts /5 Pr\u00e9cision technique Exactitude des informations /5 Sous-total /15"},{"location":"module4/ressources/grille-evaluation-projet/#5-documentation-10","title":"5. Documentation (10%)","text":"Crit\u00e8re Description Bar\u00e8me Note Documentation technique Clart\u00e9 et compl\u00e9tude de la documentation technique /5 Guide utilisateur Qualit\u00e9 et pertinence du guide utilisateur /5 Sous-total /10"},{"location":"module4/ressources/grille-evaluation-projet/#note-finale-100","title":"Note finale : /100","text":""},{"location":"module4/ressources/grille-evaluation-projet/#observations-et-commentaires","title":"Observations et commentaires","text":"<p>[Espace r\u00e9serv\u00e9 pour les commentaires de l'\u00e9valuateur]</p>"},{"location":"module4/ressources/grille-evaluation-projet/#points-forts","title":"Points forts","text":""},{"location":"module4/ressources/grille-evaluation-projet/#-","title":"-","text":""},{"location":"module4/ressources/grille-evaluation-projet/#points-a-ameliorer","title":"Points \u00e0 am\u00e9liorer","text":""},{"location":"module4/ressources/grille-evaluation-projet/#-_1","title":"-","text":""},{"location":"module4/ressources/grille-evaluation-projet/#conseils-pour-lavenir","title":"Conseils pour l'avenir","text":""},{"location":"module4/ressources/grille-evaluation-projet/#-_2","title":"-","text":"<p>-</p>"},{"location":"module4/ressources/kit-demarrage-readme/","title":"Kit de d\u00e9marrage pour le projet chatbot (4h)","text":"<p>Ce kit contient tous les \u00e9l\u00e9ments essentiels pour d\u00e9velopper rapidement un chatbot p\u00e9dagogique fonctionnel dans le cadre d'une s\u00e9ance de 4 heures. Choisissez la version correspondant \u00e0 votre option (SISR ou SLAM).</p>"},{"location":"module4/ressources/kit-demarrage-readme/#structure-du-kit","title":"Structure du kit","text":"<p>kit-demarrage-[sisr/slam]/ \u251c\u2500\u2500 frontend/ \u2502   \u251c\u2500\u2500 index.html         # Interface du chatbot \u2502   \u251c\u2500\u2500 css/ \u2502   \u2502   \u2514\u2500\u2500 style.css      # Styles pr\u00e9-configur\u00e9s \u2502   \u2514\u2500\u2500 js/ \u2502       \u2514\u2500\u2500 app.js         # Logique c\u00f4t\u00e9 client \u2502 \u251c\u2500\u2500 backend/ \u2502   \u251c\u2500\u2500 app.py             # Serveur Flask et int\u00e9gration API \u2502   \u251c\u2500\u2500 knowledge_base.py  # Gestion de la base de connaissances \u2502   \u2514\u2500\u2500 prompt_utils.py    # Utilities pour l'enrichissement des prompts \u2502 \u251c\u2500\u2500 knowledge/ \u2502   \u251c\u2500\u2500 base_structure.json  # Structure JSON vide \u00e0 compl\u00e9ter \u2502   \u2514\u2500\u2500 examples/ \u2502       \u251c\u2500\u2500 dl_general.json  # Exemple pour concept g\u00e9n\u00e9ral \u2502       \u2514\u2500\u2500 concept_[sisr/slam].json  # Exemple sp\u00e9cifique \u00e0 l'option \u2502 \u2514\u2500\u2500 README.md              # Instructions d'utilisation</p>"},{"location":"module4/ressources/kit-demarrage-readme/#guide-de-demarrage-rapide","title":"Guide de d\u00e9marrage rapide","text":"<ol> <li>Configuration</li> <li>Placez votre cl\u00e9 API Mistral dans le fichier <code>backend/config.py</code></li> <li> <p>Ex\u00e9cutez <code>pip install -r requirements.txt</code> pour installer les d\u00e9pendances</p> </li> <li> <p>Lancement</p> </li> <li>D\u00e9marrez le backend : <code>python backend/app.py</code></li> <li> <p>Ouvrez <code>frontend/index.html</code> avec Live Server ou \u00e9quivalent</p> </li> <li> <p>D\u00e9veloppement rapide</p> </li> <li>Frontend : L'interface est d\u00e9j\u00e0 fonctionnelle, modifiez seulement l'apparence</li> <li>Backend : Les fonctions sont pr\u00e9-impl\u00e9ment\u00e9es, ajoutez votre logique sp\u00e9cifique</li> <li>Base de connaissances : Compl\u00e9tez les fichiers JSON dans le dossier <code>knowledge/</code></li> </ol>"},{"location":"module4/ressources/kit-demarrage-readme/#points-de-personnalisation","title":"Points de personnalisation","text":"<p>Dans chaque fichier, vous trouverez des commentaires <code>TODO</code> indiquant les sections \u00e0 modifier pour votre projet.</p>"},{"location":"module4/ressources/kit-demarrage-readme/#specificites-sisr","title":"Sp\u00e9cificit\u00e9s SISR","text":"<p>Pour l'option SISR, concentrez-vous sur : - Le syst\u00e8me d'arbre de d\u00e9cision dans <code>backend/diagnostic_tree.py</code> - Les r\u00e9ponses adapt\u00e9es aux probl\u00e8mes r\u00e9seau dans <code>knowledge/concept_sisr.json</code></p>"},{"location":"module4/ressources/kit-demarrage-readme/#specificites-slam","title":"Sp\u00e9cificit\u00e9s SLAM","text":"<p>Pour l'option SLAM, concentrez-vous sur : - Le syst\u00e8me d'authentification simple dans <code>frontend/js/auth.js</code> - La persistance locale dans <code>frontend/js/storage.js</code> - Les concepts de d\u00e9veloppement dans <code>knowledge/concept_slam.json</code></p>"},{"location":"module4/ressources/modele-documentation-technique/","title":"Mod\u00e8le de documentation technique - Chatbot p\u00e9dagogique Deep Learning","text":"<p>Ce document est un template que vous pouvez utiliser pour documenter votre projet de chatbot p\u00e9dagogique. Remplacez chaque section par vos propres informations.</p>"},{"location":"module4/ressources/modele-documentation-technique/#1-vue-densemble-du-systeme","title":"1. Vue d'ensemble du syst\u00e8me","text":""},{"location":"module4/ressources/modele-documentation-technique/#11-introduction","title":"1.1 Introduction","text":"<p>[D\u00e9crivez bri\u00e8vement votre chatbot, son objectif et son public cible. Expliquez pourquoi il a \u00e9t\u00e9 d\u00e9velopp\u00e9 et quels probl\u00e8mes il r\u00e9sout.]</p>"},{"location":"module4/ressources/modele-documentation-technique/#12-architecture-globale","title":"1.2 Architecture globale","text":"<p>[Ins\u00e9rez ici un diagramme d'architecture]</p> <p>Notre chatbot p\u00e9dagogique est compos\u00e9 des composants principaux suivants :</p> <ul> <li>Interface utilisateur : Interface web conversationnelle permettant aux utilisateurs d'interagir avec le chatbot</li> <li>Backend : Serveur Python qui g\u00e8re la logique m\u00e9tier et les interactions avec l'API</li> <li>API Mistral AI : Service externe fournissant les capacit\u00e9s de compr\u00e9hension et g\u00e9n\u00e9ration de langage naturel</li> <li>Base de connaissances : Structure de donn\u00e9es contenant les concepts, exemples et quiz sur le Deep Learning</li> </ul>"},{"location":"module4/ressources/modele-documentation-technique/#13-technologies-utilisees","title":"1.3 Technologies utilis\u00e9es","text":"Composant Technologies Justification Frontend HTML5, CSS3, JavaScript Technologies web standard pour une compatibilit\u00e9 maximale Backend Python 3.x, Flask/FastAPI \u00c9cosyst\u00e8me riche pour l'IA/ML, facilit\u00e9 d'int\u00e9gration avec les API Base de donn\u00e9es JSON structur\u00e9 Format l\u00e9ger et flexible, adapt\u00e9 \u00e0 une base de connaissances hi\u00e9rarchique API Mistral AI Mod\u00e8le de langage avanc\u00e9 avec bonnes performances en fran\u00e7ais D\u00e9ploiement [Pr\u00e9cisez ici : local, Docker, etc.] [Justification du choix]"},{"location":"module4/ressources/modele-documentation-technique/#2-composants-detailles","title":"2. Composants d\u00e9taill\u00e9s","text":""},{"location":"module4/ressources/modele-documentation-technique/#21-interface-utilisateur","title":"2.1 Interface utilisateur","text":""},{"location":"module4/ressources/modele-documentation-technique/#211-structure-des-fichiers","title":"2.1.1 Structure des fichiers","text":"<pre><code>static/\n\u251c\u2500\u2500 css/\n\u2502   \u2514\u2500\u2500 styles.css     # Styles de l'interface\n\u2514\u2500\u2500 js/\n    \u2514\u2500\u2500 app.js         # Logique client et communication avec l'API\n\ntemplates/\n\u2514\u2500\u2500 index.html         # Structure HTML de l'interface\n</code></pre>"},{"location":"module4/ressources/modele-documentation-technique/#212-fonctionnalites-principales","title":"2.1.2 Fonctionnalit\u00e9s principales","text":"<ul> <li>Affichage des messages dans un format conversationnel</li> <li>Saisie et envoi de questions</li> <li>Affichage des indicateurs de chargement</li> <li>Gestion de l'historique de conversation</li> <li>[Autres fonctionnalit\u00e9s sp\u00e9cifiques \u00e0 votre interface...]</li> </ul>"},{"location":"module4/ressources/modele-documentation-technique/#213-communication-avec-le-backend","title":"2.1.3 Communication avec le backend","text":"<p>[D\u00e9crivez comment l'interface communique avec le backend, les formats de donn\u00e9es, etc.]</p>"},{"location":"module4/ressources/modele-documentation-technique/#22-backend","title":"2.2 Backend","text":""},{"location":"module4/ressources/modele-documentation-technique/#221-structure-des-fichiers","title":"2.2.1 Structure des fichiers","text":"<pre><code>app.py                 # Point d'entr\u00e9e de l'application\nconfig.py              # Configuration (cl\u00e9s API, param\u00e8tres)\nservices/\n\u251c\u2500\u2500 mistral_service.py # Service d'interaction avec l'API Mistral\n\u2514\u2500\u2500 knowledge_service.py # Service de gestion de la base de connaissances\n</code></pre>"},{"location":"module4/ressources/modele-documentation-technique/#222-points-dapi","title":"2.2.2 Points d'API","text":"Endpoint M\u00e9thode Description Param\u00e8tres R\u00e9ponse <code>/api/chat</code> POST Traite une question utilisateur <code>{\"message\": string, \"history\": array}</code> <code>{\"response\": string}</code> <code>/api/quiz</code> GET G\u00e9n\u00e8re un quiz sur un sujet <code>?topic=string</code> <code>{\"questions\": array}</code> [Autres endpoints...]"},{"location":"module4/ressources/modele-documentation-technique/#223-classes-et-fonctions-principales","title":"2.2.3 Classes et fonctions principales","text":"<p>[D\u00e9crivez les classes et fonctions principales de votre backend, leur r\u00f4le et leurs interactions]</p>"},{"location":"module4/ressources/modele-documentation-technique/#23-integration-avec-mistral-ai","title":"2.3 Int\u00e9gration avec Mistral AI","text":""},{"location":"module4/ressources/modele-documentation-technique/#231-configuration-de-lapi","title":"2.3.1 Configuration de l'API","text":"<p>[Expliquez comment vous avez configur\u00e9 l'API Mistral, les param\u00e8tres utilis\u00e9s, etc.]</p>"},{"location":"module4/ressources/modele-documentation-technique/#232-gestion-du-contexte-conversationnel","title":"2.3.2 Gestion du contexte conversationnel","text":"<p>[D\u00e9crivez comment vous g\u00e9rez le contexte des conversations avec l'API Mistral]</p>"},{"location":"module4/ressources/modele-documentation-technique/#233-optimisation-des-prompts","title":"2.3.3 Optimisation des prompts","text":"<p>[D\u00e9taillez vos techniques de prompt engineering pour obtenir des r\u00e9ponses p\u00e9dagogiques]</p>"},{"location":"module4/ressources/modele-documentation-technique/#24-base-de-connaissances","title":"2.4 Base de connaissances","text":""},{"location":"module4/ressources/modele-documentation-technique/#241-structure-des-donnees","title":"2.4.1 Structure des donn\u00e9es","text":"<p>[Pr\u00e9sentez la structure JSON de votre base de connaissances avec un exemple]</p>"},{"location":"module4/ressources/modele-documentation-technique/#242-mecanisme-de-recherche-et-enrichissement","title":"2.4.2 M\u00e9canisme de recherche et enrichissement","text":"<p>[Expliquez comment vous recherchez et utilisez les informations de la base de connaissances]</p>"},{"location":"module4/ressources/modele-documentation-technique/#3-flux-dexecution","title":"3. Flux d'ex\u00e9cution","text":""},{"location":"module4/ressources/modele-documentation-technique/#31-traitement-dune-question-utilisateur","title":"3.1 Traitement d'une question utilisateur","text":"<ol> <li>L'utilisateur saisit une question dans l'interface</li> <li>La requ\u00eate est envoy\u00e9e au backend via l'API <code>/api/chat</code></li> <li>Le backend identifie les concepts pertinents dans la base de connaissances</li> <li>Ces informations sont utilis\u00e9es pour enrichir le prompt envoy\u00e9 \u00e0 l'API Mistral</li> <li>La r\u00e9ponse de l'API est trait\u00e9e et renvoy\u00e9e \u00e0 l'interface utilisateur</li> <li>L'interface affiche la r\u00e9ponse dans la conversation</li> </ol>"},{"location":"module4/ressources/modele-documentation-technique/#32-generation-dun-quiz","title":"3.2 G\u00e9n\u00e9ration d'un quiz","text":"<p>[D\u00e9crivez le flux d'ex\u00e9cution pour la g\u00e9n\u00e9ration de quiz]</p>"},{"location":"module4/ressources/modele-documentation-technique/#33-autres-flux-specifiques-a-votre-application","title":"3.3 [Autres flux sp\u00e9cifiques \u00e0 votre application]","text":""},{"location":"module4/ressources/modele-documentation-technique/#4-securite-et-performance","title":"4. S\u00e9curit\u00e9 et performance","text":""},{"location":"module4/ressources/modele-documentation-technique/#41-gestion-des-cles-api","title":"4.1 Gestion des cl\u00e9s API","text":"<p>[Expliquez comment vous g\u00e9rez et prot\u00e9gez les cl\u00e9s API]</p>"},{"location":"module4/ressources/modele-documentation-technique/#42-optimisations-de-performance","title":"4.2 Optimisations de performance","text":"<p>[D\u00e9taillez les optimisations mises en place pour am\u00e9liorer les performances]</p>"},{"location":"module4/ressources/modele-documentation-technique/#43-gestion-des-erreurs","title":"4.3 Gestion des erreurs","text":"<p>[D\u00e9crivez comment vous g\u00e9rez les erreurs \u00e0 diff\u00e9rents niveaux]</p>"},{"location":"module4/ressources/modele-documentation-technique/#5-guide-dinstallation-et-deploiement","title":"5. Guide d'installation et d\u00e9ploiement","text":""},{"location":"module4/ressources/modele-documentation-technique/#51-prerequis","title":"5.1 Pr\u00e9requis","text":"<ul> <li>Python 3.8 ou sup\u00e9rieur</li> <li>Connexion Internet (pour l'API Mistral)</li> <li>[Autres pr\u00e9requis...]</li> </ul>"},{"location":"module4/ressources/modele-documentation-technique/#52-installation","title":"5.2 Installation","text":"<pre><code># Cloner le d\u00e9p\u00f4t\ngit clone https://github.com/votre-compte/chatbot-pedagogique.git\ncd chatbot-pedagogique\n\n# Installer les d\u00e9pendances\npip install -r requirements.txt\n\n# Configurer les variables d'environnement\ncp .env.example .env\n# \u00c9ditez le fichier .env pour ajouter votre cl\u00e9 API Mistral\n</code></pre>"},{"location":"module4/ressources/modele-documentation-technique/#53-configuration","title":"5.3 Configuration","text":"<p>[D\u00e9taillez les \u00e9tapes de configuration n\u00e9cessaires]</p>"},{"location":"module4/ressources/modele-documentation-technique/#54-lancement","title":"5.4 Lancement","text":"<pre><code># Lancer l'application\npython app.py\n</code></pre>"},{"location":"module4/ressources/modele-documentation-technique/#55-tests","title":"5.5 Tests","text":"<p>[Expliquez comment ex\u00e9cuter les tests]</p>"},{"location":"module4/ressources/modele-documentation-technique/#6-extensions-et-ameliorations-futures","title":"6. Extensions et am\u00e9liorations futures","text":"<p>[Listez les am\u00e9liorations potentielles et extensions pr\u00e9vues pour le chatbot]</p>"},{"location":"module4/ressources/modele-documentation-technique/#7-problemes-connus-et-limitations","title":"7. Probl\u00e8mes connus et limitations","text":"<p>[Documentez les probl\u00e8mes connus et les limitations actuelles]</p>"},{"location":"module4/ressources/modele-documentation-technique/#8-annexes","title":"8. Annexes","text":""},{"location":"module4/ressources/modele-documentation-technique/#81-glossaire","title":"8.1 Glossaire","text":"Terme D\u00e9finition [Terme 1] [D\u00e9finition] [Terme 2] [D\u00e9finition]"},{"location":"module4/ressources/modele-documentation-technique/#82-references","title":"8.2 R\u00e9f\u00e9rences","text":"<p>[Listez les r\u00e9f\u00e9rences, biblioth\u00e8ques, articles ou autres ressources utilis\u00e9es] <pre><code>Pour le dossier des ressources, je vais maintenant cr\u00e9er un exemple de base de connaissances en JSON.\n\n## ressources/exemple-base-connaissances.json\n\n```json\n{\n  \"concepts\": [\n    {\n      \"id\": \"neural_network\",\n      \"title\": \"R\u00e9seau de neurones\",\n      \"description\": \"Mod\u00e8le informatique inspir\u00e9 du fonctionnement des neurones biologiques, capable d'apprendre \u00e0 partir de donn\u00e9es.\",\n      \"levels\": {\n        \"beginner\": \"Un r\u00e9seau de neurones est comme un ensemble de filtres intelligents interconnect\u00e9s qui apprennent progressivement \u00e0 reconna\u00eetre des motifs dans les donn\u00e9es. Imaginez un groupe de personnes qui se passent des informations et les transforment petit \u00e0 petit pour arriver \u00e0 une d\u00e9cision finale.\",\n        \"intermediate\": \"Un r\u00e9seau de neurones est un syst\u00e8me compos\u00e9 de neurones artificiels organis\u00e9s en couches qui transforment des donn\u00e9es d'entr\u00e9e en sorties au travers de poids et de fonctions d'activation. Ces poids sont ajust\u00e9s durant l'apprentissage pour minimiser l'erreur entre les pr\u00e9dictions et les valeurs r\u00e9elles.\",\n        \"advanced\": \"Un r\u00e9seau de neurones est un mod\u00e8le param\u00e9trique compos\u00e9 d'unit\u00e9s de calcul interconnect\u00e9es qui effectuent des transformations non-lin\u00e9aires sur les donn\u00e9es d'entr\u00e9e. Ces transformations sont d\u00e9finies par des matrices de poids et des biais, optimis\u00e9s par descente de gradient et r\u00e9tropropagation pour minimiser une fonction de co\u00fbt d\u00e9finie sur l'ensemble d'apprentissage.\"\n      },\n      \"examples\": [\n        \"Reconnaissance d'images: un r\u00e9seau peut apprendre \u00e0 identifier des chats dans des photos\",\n        \"Traduction automatique: des r\u00e9seaux traduisent du texte d'une langue \u00e0 une autre\",\n        \"G\u00e9n\u00e9ration de musique: des r\u00e9seaux peuvent composer des morceaux originaux en s'inspirant d'un style particulier\"\n      ],\n      \"analogies\": [\n        \"Un r\u00e9seau de neurones est comme une cha\u00eene de transformation dans une usine: chaque station (neurone) effectue une op\u00e9ration sp\u00e9cifique sur le produit qui passe, et ensemble, elles transforment la mati\u00e8re premi\u00e8re (donn\u00e9es d'entr\u00e9e) en produit fini (pr\u00e9diction).\",\n        \"C'est comme un orchestre o\u00f9 chaque musicien (neurone) joue sa partition, et ensemble ils cr\u00e9ent une symphonie (pr\u00e9diction). Le chef d'orchestre (algorithme d'apprentissage) guide les musiciens pour am\u00e9liorer leur performance.\"\n      ],\n      \"related_concepts\": [\"perceptron\", \"deep_learning\", \"activation_function\"],\n      \"quiz\": [\n        {\n          \"question\": \"Quelle caract\u00e9ristique fondamentale permet aux r\u00e9seaux de neurones d'apprendre?\",\n          \"options\": [\n            \"Leur capacit\u00e9 \u00e0 m\u00e9moriser tous les exemples d'entra\u00eenement\",\n            \"L'ajustement automatique des poids en fonction des erreurs\",\n            \"Leur architecture toujours fixe et d\u00e9termin\u00e9e \u00e0 l'avance\",\n            \"La pr\u00e9sence syst\u00e9matique de nombreuses couches\"\n          ],\n          \"correct_answer\": 1,\n          \"explanation\": \"Les r\u00e9seaux de neurones apprennent en ajustant progressivement leurs poids en fonction des erreurs commises sur les donn\u00e9es d'entra\u00eenement, ce qui leur permet de s'am\u00e9liorer au fil du temps.\"\n        }\n      ]\n    },\n    {\n      \"id\": \"cnn\",\n      \"title\": \"R\u00e9seau de neurones convolutif (CNN)\",\n      \"description\": \"Type de r\u00e9seau sp\u00e9cialis\u00e9 dans le traitement des donn\u00e9es en grille comme les images, utilisant des op\u00e9rations de convolution pour d\u00e9tecter des motifs spatiaux.\",\n      \"levels\": {\n        \"beginner\": \"Les CNN sont des r\u00e9seaux sp\u00e9cialis\u00e9s pour analyser les images. Ils fonctionnent un peu comme notre vision: ils d\u00e9tectent d'abord des \u00e9l\u00e9ments simples (lignes, contours), puis les combinent pour reconna\u00eetre des formes plus complexes (visages, objets). C'est comme si vous aviez des d\u00e9tectives qui cherchent des indices sp\u00e9cifiques dans une image.\",\n        \"intermediate\": \"Les CNN utilisent des filtres de convolution qui glissent sur l'image pour d\u00e9tecter des motifs locaux. Ces r\u00e9seaux sont organis\u00e9s en couches successives: les premi\u00e8res d\u00e9tectent des caract\u00e9ristiques basiques (contours, textures) et les suivantes combinent ces informations pour identifier des structures plus complexes. Le pooling permet de r\u00e9duire la dimension tout en conservant l'information importante.\",\n        \"advanced\": \"Les CNN exploitent trois id\u00e9es fondamentales: les champs r\u00e9ceptifs locaux, le partage de poids et le sous-\u00e9chantillonnage. La convolution est une op\u00e9ration qui applique un filtre \u00e0 une r\u00e9gion locale, produisant une carte d'activation (feature map). L'architecture typique alterne couches de convolution (extraction de caract\u00e9ristiques) et couches de pooling (r\u00e9duction de dimension et invariance), suivies de couches enti\u00e8rement connect\u00e9es pour la classification.\"\n      },\n      \"examples\": [\n        \"Classification d'images: identification d'objets dans une photo\",\n        \"D\u00e9tection d'objets: localisation pr\u00e9cise d'objets dans une image avec des bo\u00eetes englobantes\",\n        \"Vision par ordinateur m\u00e9dicale: d\u00e9tection de tumeurs dans des images m\u00e9dicales\",\n        \"Reconnaissance faciale: identification de personnes \u00e0 partir de leurs traits faciaux\"\n      ],\n      \"analogies\": [\n        \"Un CNN est comme un d\u00e9tective qui examine une sc\u00e8ne de crime: d'abord il note les d\u00e9tails \u00e9vidents (couches initiales), puis il \u00e9tablit des liens entre ces indices pour comprendre ce qui s'est pass\u00e9 (couches profondes).\",\n        \"Les filtres de convolution sont comme des pochoirs: quand on place un pochoir sur une image et qu'on colorie dessus, on fait ressortir certains motifs sp\u00e9cifiques.\"\n      ],\n      \"related_concepts\": [\"convolution\", \"pooling\", \"feature_map\", \"computer_vision\"],\n      \"quiz\": [\n        {\n          \"question\": \"Quelle est la principale innovation des CNN par rapport aux r\u00e9seaux de neurones classiques?\",\n          \"options\": [\n            \"Ils utilisent plus de neurones\",\n            \"Ils exploitent la structure spatiale des donn\u00e9es\",\n            \"Ils s'entra\u00eenent plus rapidement\",\n            \"Ils n\u00e9cessitent moins de donn\u00e9es d'entra\u00eenement\"\n          ],\n          \"correct_answer\": 1,\n          \"explanation\": \"Les CNN exploitent la structure spatiale des donn\u00e9es en utilisant des op\u00e9rations de convolution qui permettent de d\u00e9tecter des motifs locaux, ce qui est particuli\u00e8rement adapt\u00e9 aux images o\u00f9 les pixels voisins sont fortement corr\u00e9l\u00e9s.\"\n        }\n      ]\n    },\n    {\n      \"id\": \"rnn\",\n      \"title\": \"R\u00e9seau de neurones r\u00e9current (RNN)\",\n      \"description\": \"Architecture de r\u00e9seau sp\u00e9cialis\u00e9e dans le traitement des donn\u00e9es s\u00e9quentielles comme le texte ou les s\u00e9ries temporelles, avec des connexions formant des cycles.\",\n      \"levels\": {\n        \"beginner\": \"Les RNN sont des r\u00e9seaux qui ont une m\u00e9moire, comme lorsque vous lisez un livre et vous vous souvenez des pages pr\u00e9c\u00e9dentes pour comprendre le contexte. Ils sont parfaits pour traiter du texte car ils peuvent se rappeler ce qui a \u00e9t\u00e9 dit avant.\",\n        \"intermediate\": \"Les RNN poss\u00e8dent des connexions r\u00e9currentes qui leur permettent de transmettre de l'information d'une \u00e9tape \u00e0 l'autre. \u00c0 chaque nouvel \u00e9l\u00e9ment de la s\u00e9quence, le r\u00e9seau utilise \u00e0 la fois cet \u00e9l\u00e9ment et sa m\u00e9moire interne pour faire une pr\u00e9diction et mettre \u00e0 jour son \u00e9tat. Cela leur conf\u00e8re une capacit\u00e9 \u00e0 capturer des d\u00e9pendances temporelles.\",\n        \"advanced\": \"Les RNN sont caract\u00e9ris\u00e9s par des connexions cycliques dans leur graphe computationnel, permettant la persistance de l'information via un \u00e9tat cach\u00e9. Lors de l'entra\u00eenement par r\u00e9tropropagation \u00e0 travers le temps (BPTT), ils peuvent souffrir du probl\u00e8me de disparition ou d'explosion du gradient, limitant leur capacit\u00e9 \u00e0 capturer des d\u00e9pendances \u00e0 long terme. Pour pallier ce probl\u00e8me, des architectures comme LSTM et GRU ont \u00e9t\u00e9 d\u00e9velopp\u00e9es avec des m\u00e9canismes de portes contr\u00f4lant le flux d'information.\"\n      },\n      \"examples\": [\n        \"Traduction automatique: traduire des phrases d'une langue \u00e0 une autre\",\n        \"G\u00e9n\u00e9ration de texte: cr\u00e9er du contenu texte coh\u00e9rent\",\n        \"Analyse de sentiment: d\u00e9terminer si un commentaire est positif ou n\u00e9gatif\",\n        \"Pr\u00e9diction de s\u00e9ries temporelles: pr\u00e9voir l'\u00e9volution du cours des actions\"\n      ],\n      \"analogies\": [\n        \"Un RNN est comme une personne qui lit un livre: \u00e0 chaque mot, elle utilise sa compr\u00e9hension des mots pr\u00e9c\u00e9dents pour interpr\u00e9ter le mot actuel.\",\n        \"C'est comme un musicien qui improvise: chaque note qu'il joue d\u00e9pend des notes qu'il a jou\u00e9es avant, cr\u00e9ant ainsi une m\u00e9lodie coh\u00e9rente.\"\n      ],\n      \"related_concepts\": [\"lstm\", \"gru\", \"sequence_processing\", \"natural_language_processing\"],\n      \"quiz\": [\n        {\n          \"question\": \"Pourquoi les RNN sont-ils particuli\u00e8rement adapt\u00e9s au traitement du langage naturel?\",\n          \"options\": [\n            \"Parce qu'ils utilisent moins de m\u00e9moire que les autres r\u00e9seaux\",\n            \"Parce qu'ils peuvent traiter des images en m\u00eame temps que du texte\",\n            \"Parce qu'ils peuvent m\u00e9moriser le contexte dans une s\u00e9quence\",\n            \"Parce qu'ils sont plus rapides \u00e0 entra\u00eener que les CNN\"\n          ],\n          \"correct_answer\": 2,\n          \"explanation\": \"Les RNN sont particuli\u00e8rement adapt\u00e9s au traitement du langage naturel car ils peuvent conserver en m\u00e9moire le contexte des mots pr\u00e9c\u00e9dents dans une phrase, ce qui est essentiel pour comprendre le sens des mots qui suivent.\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre></p>"},{"location":"ressources/","title":"Ressources pour le Deep Learning","text":""},{"location":"ressources/#bienvenue-dans-lespace-des-ressources","title":"Bienvenue dans l'espace des ressources","text":"<p>Cette section regroupe l'ensemble des ressources disponibles pour vous accompagner dans votre apprentissage du Deep Learning et dans le d\u00e9veloppement de votre projet de chatbot p\u00e9dagogique.</p>"},{"location":"ressources/#documentation-des-apis","title":"Documentation des APIs","text":""},{"location":"ressources/#api-mistral","title":"API Mistral","text":"<p>L'API Mistral AI est au c\u0153ur du projet de chatbot p\u00e9dagogique. Cette documentation vous explique :</p> <ul> <li>Comment cr\u00e9er et configurer votre compte Mistral AI</li> <li>Comment envoyer des requ\u00eates et traiter les r\u00e9ponses</li> <li>Les bonnes pratiques pour le prompt engineering</li> <li>Des exemples d'int\u00e9gration dans diff\u00e9rents contextes</li> </ul>"},{"location":"ressources/#instructions-dintegration","title":"Instructions d'int\u00e9gration","text":"<p>Ce guide d\u00e9taille les \u00e9tapes \u00e0 suivre pour int\u00e9grer les diff\u00e9rents composants du chatbot :</p> <ul> <li>Configuration de l'environnement de d\u00e9veloppement</li> <li>Installation des d\u00e9pendances requises</li> <li>Proc\u00e9dures de d\u00e9ploiement pour les environnements de test et de production</li> <li>R\u00e9solution des probl\u00e8mes courants d'int\u00e9gration</li> </ul>"},{"location":"ressources/#structure-des-donnees","title":"Structure des donn\u00e9es","text":""},{"location":"ressources/#base-de-connaissances","title":"Base de connaissances","text":"<p>La base de connaissances est le fondement de votre chatbot p\u00e9dagogique. Cette documentation couvre :</p> <ul> <li>Les concepts fondamentaux du Deep Learning \u00e0 int\u00e9grer</li> <li>L'organisation hi\u00e9rarchique des connaissances</li> <li>Les diff\u00e9rents niveaux d'explication (d\u00e9butant, interm\u00e9diaire, avanc\u00e9)</li> <li>Des exemples concrets et analogies pour l'enseignement</li> </ul>"},{"location":"ressources/#schemas-json","title":"Sch\u00e9mas JSON","text":"<p>Les sch\u00e9mas JSON d\u00e9finissent la structure des donn\u00e9es utilis\u00e9es par le chatbot :</p> <ul> <li>Format de la base de connaissances</li> <li>Structure des \u00e9changes avec l'API</li> <li>Organisation des conversations et de l'historique</li> <li>Mod\u00e8les pour les quiz et exercices</li> </ul>"},{"location":"ressources/#guides-pratiques","title":"Guides pratiques","text":""},{"location":"ressources/#guide-dutilisation-de-google-colab","title":"Guide d'utilisation de Google Colab","text":"<p>Un guide complet pour tirer le meilleur parti de Google Colab dans vos projets de Deep Learning :</p> <ul> <li>Configuration de l'environnement</li> <li>Utilisation des GPUs gratuits</li> <li>Sauvegarde et partage de notebooks</li> <li>Astuces pour l'optimisation des performances</li> </ul>"},{"location":"ressources/#guide-du-developpeur-pour-le-chatbot","title":"Guide du d\u00e9veloppeur pour le chatbot","text":"<p>Un ensemble de bonnes pratiques pour le d\u00e9veloppement du chatbot :</p> <ul> <li>Architecture recommand\u00e9e</li> <li>Gestion des erreurs et des cas limites</li> <li>Optimisation des performances</li> <li>Tests et validation</li> </ul>"},{"location":"ressources/#ressources-complementaires","title":"Ressources compl\u00e9mentaires","text":""},{"location":"ressources/#bibliotheques-et-frameworks-recommandes","title":"Biblioth\u00e8ques et frameworks recommand\u00e9s","text":"<p>Une s\u00e9lection de biblioth\u00e8ques et frameworks utiles pour le projet :</p> <ul> <li>TensorFlow/Keras pour le d\u00e9veloppement de mod\u00e8les</li> <li>Flask pour le d\u00e9veloppement d'API</li> <li>React pour l'interface utilisateur</li> <li>MongoDB pour le stockage des donn\u00e9es</li> </ul>"},{"location":"ressources/#lectures-recommandees","title":"Lectures recommand\u00e9es","text":"<p>Une bibliographie s\u00e9lective pour approfondir vos connaissances :</p> <ul> <li>Livres de r\u00e9f\u00e9rence sur le Deep Learning</li> <li>Articles scientifiques pertinents</li> <li>Blogs et newsletters \u00e0 suivre</li> <li>Tutoriels vid\u00e9o recommand\u00e9s</li> </ul>"},{"location":"ressources/#telechargements","title":"T\u00e9l\u00e9chargements","text":""},{"location":"ressources/#code-source-des-exemples","title":"Code source des exemples","text":"<p>Acc\u00e9dez au code source des exemples pr\u00e9sent\u00e9s lors des s\u00e9ances :</p> <ul> <li>Notebooks des mini-projets CNN et RNN</li> <li>Scripts d'int\u00e9gration avec l'API Mistral</li> <li>Templates pour la base de connaissances</li> <li>Exemples d'interfaces conversationnelles</li> </ul>"},{"location":"ressources/#templates-et-modeles","title":"Templates et mod\u00e8les","text":"<p>Des templates pr\u00eats \u00e0 l'emploi pour acc\u00e9l\u00e9rer votre d\u00e9veloppement :</p> <ul> <li>Structure de base du chatbot</li> <li>Template de documentation technique</li> <li>Mod\u00e8les pour la base de connaissances</li> <li>Grilles d'\u00e9valuation des performances</li> </ul>"},{"location":"ressources/#support-et-assistance","title":"Support et assistance","text":"<p>Si vous rencontrez des difficult\u00e9s ou avez des questions, plusieurs options s'offrent \u00e0 vous :</p> <ul> <li>Consultez la FAQ ci-dessous</li> <li>Posez vos questions sur le forum d\u00e9di\u00e9</li> <li>Contactez votre formateur pendant les heures de permanence</li> <li>\u00c9changez avec vos pairs dans les canaux de discussion</li> </ul>"},{"location":"ressources/#faq","title":"FAQ","text":"<p>Q: Comment obtenir une cl\u00e9 API Mistral gratuitement ? R: Consultez la section \"Cr\u00e9ation de compte\" dans la documentation de l'API Mistral. Un quota gratuit suffisant pour le d\u00e9veloppement est disponible.</p> <p>Q: Puis-je utiliser un autre framework que TensorFlow pour le projet ? R: Oui, vous pouvez utiliser PyTorch ou d'autres frameworks, mais les exemples et le support seront principalement orient\u00e9s TensorFlow/Keras.</p> <p>Q: Comment g\u00e9rer les limites de l'API Mistral dans mon chatbot ? R: La documentation de l'API Mistral contient une section d\u00e9di\u00e9e \u00e0 la gestion des limites et des quotas, avec des strat\u00e9gies d'optimisation.</p> <p>Q: Comment structurer efficacement ma base de connaissances ? R: R\u00e9f\u00e9rez-vous au document \"Structure de la base de connaissances\" qui propose une organisation hi\u00e9rarchique et des exemples concrets.</p> <p>Ces ressources ont \u00e9t\u00e9 con\u00e7ues pour vous accompagner tout au long de votre parcours d'apprentissage et du d\u00e9veloppement de votre projet de chatbot p\u00e9dagogique. N'h\u00e9sitez pas \u00e0 les consulter r\u00e9guli\u00e8rement et \u00e0 nous faire part de vos suggestions d'am\u00e9lioration.</p> <p>Retour \u00e0 l'accueil</p>"},{"location":"ressources/api-mistral/","title":"Int\u00e9gration de l'API Mistral avec FastAPI - Premier test","text":""},{"location":"ressources/api-mistral/#bts-sio-seance-2-types-de-reseaux-et-applications","title":"BTS SIO  - S\u00e9ance 2: Types de r\u00e9seaux et applications","text":"<pre><code>import requests\nimport json\nimport os\nfrom dotenv import load_dotenv\nfrom fastapi import FastAPI, Request, Form, HTTPException\nfrom fastapi.templating import Jinja2Templates\nfrom fastapi.staticfiles import StaticFiles\nfrom fastapi.responses import HTMLResponse, JSONResponse\nimport uvicorn\nfrom pydantic import BaseModel\n\n# Charger les variables d'environnement\nload_dotenv()\n\n# Configuration de l'API Mistral\nMISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\", \"votre_cl\u00e9_api\")  # \u00c0 remplacer par votre cl\u00e9 API\nMISTRAL_API_URL = \"https://api.mistral.ai/v1/chat/completions\"\n\n# Initialisation de l'application FastAPI\napp = FastAPI(title=\"Explorateur de concepts Deep Learning\", \n              description=\"Une API pour explorer les concepts du Deep Learning avec Mistral AI\")\n\n# Configuration des templates\ntemplates = Jinja2Templates(directory=\"templates\")\n\n# 1. Fonction simple pour appeler l'API Mistral\ndef mistral_chat_completion(prompt, model=\"mistral-tiny\", max_tokens=1000):\n    \"\"\"\n    Appelle l'API Mistral pour g\u00e9n\u00e9rer une r\u00e9ponse \u00e0 partir d'un prompt.\n\n    Args:\n        prompt (str): Le message \u00e0 envoyer \u00e0 l'API\n        model (str): Le mod\u00e8le \u00e0 utiliser (mistral-tiny, mistral-small, mistral-medium, etc.)\n        max_tokens (int): Nombre maximum de tokens pour la r\u00e9ponse\n\n    Returns:\n        dict: La r\u00e9ponse de l'API\n    \"\"\"\n    headers = {\n        \"Authorization\": f\"Bearer {MISTRAL_API_KEY}\",\n        \"Content-Type\": \"application/json\"\n    }\n\n    data = {\n        \"model\": model,\n        \"messages\": [\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        \"max_tokens\": max_tokens\n    }\n\n    try:\n        response = requests.post(MISTRAL_API_URL, headers=headers, data=json.dumps(data))\n        response.raise_for_status()  # Lever une exception si la requ\u00eate \u00e9choue\n        return response.json()\n    except requests.exceptions.RequestException as e:\n        print(f\"Erreur lors de l'appel \u00e0 l'API Mistral: {e}\")\n        return {\"error\": str(e)}\n\n# 2. Test simple de l'API\ndef test_mistral_api():\n    \"\"\"\n    Teste l'API Mistral avec un prompt simple.\n    \"\"\"\n    prompt = \"Explique-moi ce qu'est le Deep Learning en 3 phrases simples.\"\n\n    print(f\"Envoi du prompt \u00e0 Mistral: '{prompt}'\")\n    response = mistral_chat_completion(prompt)\n\n    if \"error\" in response:\n        print(f\"Erreur: {response['error']}\")\n        return\n\n    # Extraire et afficher la r\u00e9ponse\n    try:\n        message_content = response[\"choices\"][0][\"message\"][\"content\"]\n        print(\"\\nR\u00e9ponse de Mistral:\")\n        print(message_content)\n\n        # Informations suppl\u00e9mentaires sur la r\u00e9ponse\n        if \"usage\" in response:\n            usage = response[\"usage\"]\n            print(\"\\nUtilisation de tokens:\")\n            print(f\"- Prompt: {usage.get('prompt_tokens', 'N/A')} tokens\")\n            print(f\"- R\u00e9ponse: {usage.get('completion_tokens', 'N/A')} tokens\")\n            print(f\"- Total: {usage.get('total_tokens', 'N/A')} tokens\")\n    except (KeyError, IndexError) as e:\n        print(f\"Erreur lors du traitement de la r\u00e9ponse: {e}\")\n        print(\"R\u00e9ponse brute:\", response)\n\n# 3. Fonction avanc\u00e9e pour l'explication de concepts de Deep Learning\ndef explain_deep_learning_concept(concept, difficulty=\"d\u00e9butant\"):\n    \"\"\"\n    Demande \u00e0 l'API Mistral d'expliquer un concept de Deep Learning.\n\n    Args:\n        concept (str): Le concept \u00e0 expliquer\n        difficulty (str): Le niveau de difficult\u00e9 (d\u00e9butant, interm\u00e9diaire, avanc\u00e9)\n\n    Returns:\n        str: L'explication g\u00e9n\u00e9r\u00e9e\n    \"\"\"\n    # Construire un prompt \u00e9ducatif structur\u00e9\n    prompt = f\"\"\"\n    En tant que tuteur p\u00e9dagogique sp\u00e9cialis\u00e9 en Deep Learning, explique le concept de '{concept}' \n    \u00e0 un \u00e9tudiant de BTS SIO  (niveau {difficulty}).\n\n    Ton explication doit inclure:\n    1. Une d\u00e9finition simple et claire\n    2. Un exemple concret d'application\n    3. Comment ce concept est utilis\u00e9 dans le d\u00e9veloppement d'applications\n\n    Utilise un langage accessible mais techniquement pr\u00e9cis.\n    \"\"\"\n\n    response = mistral_chat_completion(prompt, model=\"mistral-small\")\n\n    if \"error\" in response:\n        return f\"Erreur: {response['error']}\"\n\n    try:\n        return response[\"choices\"][0][\"message\"][\"content\"]\n    except (KeyError, IndexError):\n        return \"Erreur lors de la r\u00e9cup\u00e9ration de la r\u00e9ponse.\"\n\n# 4. Mod\u00e8les Pydantic pour les requ\u00eates\nclass ConceptRequest(BaseModel):\n    concept: str\n    difficulty: str = \"d\u00e9butant\"\n\n# 5. Routes FastAPI\n@app.get(\"/\", response_class=HTMLResponse)\nasync def home(request: Request):\n    return templates.TemplateResponse(\"index.html\", {\"request\": request})\n\n@app.post(\"/api/explain\")\nasync def api_explain(request: ConceptRequest):\n    if not request.concept:\n        raise HTTPException(status_code=400, detail=\"Concept manquant\")\n\n    explanation = explain_deep_learning_concept(request.concept, request.difficulty)\n    return {\"explanation\": explanation}\n\n# 6. Template HTML simple pour l'interface\ndef create_template_directory():\n    \"\"\"Cr\u00e9e un r\u00e9pertoire templates et un fichier index.html\"\"\"\n    import os\n    os.makedirs('templates', exist_ok=True)\n\n    with open('templates/index.html', 'w') as f:\n        f.write(\"\"\"\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"fr\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;Explorateur de concepts Deep Learning&lt;/title&gt;\n    &lt;style&gt;\n        body {\n            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n            line-height: 1.6;\n            max-width: 800px;\n            margin: 0 auto;\n            padding: 20px;\n            background-color: #f5f8fa;\n            color: #333;\n        }\n        h1 {\n            color: #2c3e50;\n            border-bottom: 2px solid #3498db;\n            padding-bottom: 10px;\n        }\n        .container {\n            background-color: white;\n            border-radius: 8px;\n            padding: 20px;\n            box-shadow: 0 2px 10px rgba(0,0,0,0.1);\n        }\n        label {\n            display: block;\n            margin-top: 15px;\n            font-weight: bold;\n            color: #2c3e50;\n        }\n        input, select, button {\n            width: 100%;\n            padding: 10px;\n            margin-top: 5px;\n            border: 1px solid #ddd;\n            border-radius: 4px;\n            box-sizing: border-box;\n        }\n        button {\n            background-color: #3498db;\n            color: white;\n            border: none;\n            padding: 12px;\n            margin-top: 20px;\n            cursor: pointer;\n            font-weight: bold;\n            transition: background-color 0.3s;\n        }\n        button:hover {\n            background-color: #2980b9;\n        }\n        #result {\n            margin-top: 20px;\n            padding: 20px;\n            background-color: #f8f9fa;\n            border-left: 4px solid #3498db;\n            border-radius: 4px;\n            white-space: pre-wrap;\n        }\n        .loading {\n            text-align: center;\n            margin-top: 20px;\n            display: none;\n        }\n        .hint {\n            font-size: 0.8em;\n            color: #7f8c8d;\n            margin-top: 5px;\n        }\n    &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;div class=\"container\"&gt;\n        &lt;h1&gt;Explorateur de concepts Deep Learning&lt;/h1&gt;\n        &lt;p&gt;Utilisez cet outil pour explorer et comprendre les concepts cl\u00e9s du Deep Learning, expliqu\u00e9s par l'IA.&lt;/p&gt;\n\n        &lt;form id=\"explainForm\"&gt;\n            &lt;label for=\"concept\"&gt;Concept \u00e0 expliquer:&lt;/label&gt;\n            &lt;input type=\"text\" id=\"concept\" required placeholder=\"Ex: r\u00e9seaux de neurones convolutifs, LSTM, dropout...\"&gt;\n            &lt;div class=\"hint\"&gt;Essayez des concepts comme: convolution, pooling, fonction d'activation, r\u00e9tropropagation...&lt;/div&gt;\n\n            &lt;label for=\"difficulty\"&gt;Niveau de difficult\u00e9:&lt;/label&gt;\n            &lt;select id=\"difficulty\"&gt;\n                &lt;option value=\"d\u00e9butant\"&gt;D\u00e9butant&lt;/option&gt;\n                &lt;option value=\"interm\u00e9diaire\"&gt;Interm\u00e9diaire&lt;/option&gt;\n                &lt;option value=\"avanc\u00e9\"&gt;Avanc\u00e9&lt;/option&gt;\n            &lt;/select&gt;\n\n            &lt;button type=\"submit\"&gt;Expliquer&lt;/button&gt;\n        &lt;/form&gt;\n\n        &lt;div class=\"loading\" id=\"loading\"&gt;\n            &lt;p&gt;Chargement de l'explication...&lt;/p&gt;\n        &lt;/div&gt;\n\n        &lt;div id=\"result\"&gt;&lt;/div&gt;\n    &lt;/div&gt;\n\n    &lt;script&gt;\n        document.getElementById('explainForm').addEventListener('submit', async function(e) {\n            e.preventDefault();\n\n            const concept = document.getElementById('concept').value.trim();\n            const difficulty = document.getElementById('difficulty').value;\n            const resultDiv = document.getElementById('result');\n            const loadingDiv = document.getElementById('loading');\n\n            if (!concept) {\n                resultDiv.innerHTML = \"Veuillez entrer un concept \u00e0 expliquer.\";\n                return;\n            }\n\n            // Afficher l'indicateur de chargement\n            loadingDiv.style.display = 'block';\n            resultDiv.innerHTML = \"\";\n\n            try {\n                const response = await fetch('/api/explain', {\n                    method: 'POST',\n                    headers: {\n                        'Content-Type': 'application/json'\n                    },\n                    body: JSON.stringify({ concept, difficulty })\n                });\n\n                const data = await response.json();\n\n                if (data.error) {\n                    resultDiv.innerHTML = `&lt;p style=\"color: red\"&gt;Erreur: ${data.error}&lt;/p&gt;`;\n                } else {\n                    resultDiv.innerHTML = data.explanation.replace(/\\\\n/g, '&lt;br&gt;');\n                }\n            } catch (error) {\n                resultDiv.innerHTML = `&lt;p style=\"color: red\"&gt;Erreur: ${error.message}&lt;/p&gt;`;\n            } finally {\n                // Cacher l'indicateur de chargement\n                loadingDiv.style.display = 'none';\n            }\n        });\n    &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n        \"\"\")\n\n    print(\"Template index.html cr\u00e9\u00e9 avec succ\u00e8s!\")\n\n# 7. Fonction principale pour ex\u00e9cuter l'application\ndef main():\n    \"\"\"Fonction principale\"\"\"\n    print(\"=== EXPLORATION DE L'API MISTRAL POUR LE CHATBOT P\u00c9DAGOGIQUE ===\")\n\n    # Tester si la cl\u00e9 API est configur\u00e9e\n    if MISTRAL_API_KEY == \"votre_cl\u00e9_api\":\n        print(\"\\nERREUR: Vous devez configurer votre cl\u00e9 API Mistral!\")\n        print(\"1. Cr\u00e9ez un fichier .env dans le m\u00eame r\u00e9pertoire que ce script\")\n        print(\"2. Ajoutez la ligne: MISTRAL_API_KEY=votre_cl\u00e9_api_r\u00e9elle\")\n        print(\"3. Relancez le script\")\n        return\n\n    # Test simple de l'API\n    print(\"\\n1. Test simple de l'API Mistral\")\n    test_mistral_api()\n\n    # Cr\u00e9ation du r\u00e9pertoire et du fichier template\n    print(\"\\n2. Cr\u00e9ation du template pour l'application web\")\n    create_template_directory()\n    print(\"   Template cr\u00e9\u00e9 dans le r\u00e9pertoire 'templates/'\")\n\n    # Lancement de l'application FastAPI\n    print(\"\\n3. D\u00e9marrage de l'application web\")\n    print(\"   URL: http://localhost:8000\")\n    print(\"   Documentation de l'API: http://localhost:8000/docs\")\n    print(\"   Appuyez sur Ctrl+C pour quitter\")\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"ressources/base-connaissances/","title":"Concepts fondamentaux du Deep Learning","text":""},{"location":"ressources/base-connaissances/#1-terminologie-de-base","title":"1. Terminologie de base","text":"Terme D\u00e9finition Ce que vous avez exp\u00e9riment\u00e9 Neurone artificiel Unit\u00e9 de calcul qui applique une fonction d'activation \u00e0 une somme pond\u00e9r\u00e9e d'entr\u00e9es Les n\u0153uds dans les visualisations qui transforment les entr\u00e9es en sorties Poids (weights) Param\u00e8tres ajustables qui d\u00e9terminent l'importance de chaque entr\u00e9e d'un neurone Les valeurs que vous avez modifi\u00e9es pour am\u00e9liorer la performance du mod\u00e8le Biais (bias) Param\u00e8tre suppl\u00e9mentaire qui permet au neurone de s'activer m\u00eame si toutes les entr\u00e9es sont nulles Le d\u00e9calage que vous avez observ\u00e9 dans les fronti\u00e8res de d\u00e9cision Fonction d'activation Fonction non-lin\u00e9aire appliqu\u00e9e \u00e0 la somme pond\u00e9r\u00e9e pour introduire la complexit\u00e9 ReLU, Sigmoid, etc. que vous avez test\u00e9es pour am\u00e9liorer l'apprentissage Couche (layer) Groupe de neurones qui traitent l'information au m\u00eame niveau Les diff\u00e9rentes \u00e9tapes de traitement dans votre r\u00e9seau Couche cach\u00e9e Couche situ\u00e9e entre la couche d'entr\u00e9e et la couche de sortie Les couches interm\u00e9diaires que vous avez ajout\u00e9es/modifi\u00e9es Forward propagation Processus de calcul de la sortie \u00e0 partir des entr\u00e9es L'ex\u00e9cution de votre mod\u00e8le pour obtenir une pr\u00e9diction"},{"location":"ressources/base-connaissances/#concepts-dapprentissage","title":"Concepts d'apprentissage","text":"Terme D\u00e9finition Ce que vous avez exp\u00e9riment\u00e9 Descente de gradient Algorithme d'optimisation qui ajuste les poids pour minimiser l'erreur Le processus d'am\u00e9lioration qui se produisait pendant l'entra\u00eenement Taux d'apprentissage Param\u00e8tre qui contr\u00f4le l'ampleur des ajustements de poids \u00e0 chaque it\u00e9ration La valeur que vous avez modifi\u00e9e pour acc\u00e9l\u00e9rer ou stabiliser l'apprentissage \u00c9poque (epoch) Un passage complet \u00e0 travers l'ensemble des donn\u00e9es d'entra\u00eenement Le nombre d'it\u00e9rations d'entra\u00eenement que vous avez d\u00e9fini Batch Sous-ensemble des donn\u00e9es trait\u00e9 avant une mise \u00e0 jour des poids La taille des groupes d'exemples utilis\u00e9s pendant l'entra\u00eenement Fonction de perte (loss) Mesure de l'\u00e9cart entre les pr\u00e9dictions et les valeurs r\u00e9elles La courbe descendante que vous avez observ\u00e9e pendant l'entra\u00eenement Surapprentissage (overfitting) Situation o\u00f9 le mod\u00e8le performe bien sur les donn\u00e9es d'entra\u00eenement mais mal sur de nouvelles donn\u00e9es La baisse de performance sur les donn\u00e9es de test que certains ont pu observer R\u00e9gularisation Techniques pour pr\u00e9venir le surapprentissage Dropout, L1/L2 que certains groupes ont peut-\u00eatre utilis\u00e9s"},{"location":"ressources/base-connaissances/#architecture-des-reseaux","title":"Architecture des r\u00e9seaux","text":"Type Caract\u00e9ristiques Applications typiques R\u00e9seau dense (fully connected) Chaque neurone connect\u00e9 \u00e0 tous les neurones de la couche pr\u00e9c\u00e9dente Le type de r\u00e9seau que vous avez utilis\u00e9 aujourd'hui R\u00e9seau convolutif (CNN) Utilise des filtres qui glissent sur les donn\u00e9es d'entr\u00e9e Traitement d'images (que nous verrons plus tard) R\u00e9seau r\u00e9current (RNN) Poss\u00e8de des connexions en boucle pour traiter des s\u00e9quences Traitement de texte, s\u00e9ries temporelles (s\u00e9ance future)"},{"location":"ressources/base-connaissances/#hyperparametres-cles","title":"Hyperparam\u00e8tres cl\u00e9s","text":"Hyperparam\u00e8tre Impact Plage typique Nombre de couches D\u00e9termine la profondeur du r\u00e9seau et sa capacit\u00e9 \u00e0 apprendre des repr\u00e9sentations complexes 1-5 pour probl\u00e8mes simples, &gt;5 pour probl\u00e8mes complexes Nombre de neurones par couche Influence la capacit\u00e9 d'apprentissage et le risque de surapprentissage D\u00e9pend du probl\u00e8me (32-128 souvent utilis\u00e9 pour d\u00e9buter) Taux d'apprentissage Contr\u00f4le la vitesse et la stabilit\u00e9 de l'apprentissage 0.1 \u00e0 0.0001 (souvent 0.01 ou 0.001) Fonction d'activation D\u00e9termine le type de relations que peut mod\u00e9liser le r\u00e9seau ReLU pour couches cach\u00e9es, Sigmoid/Softmax pour sortie Taille de batch Influence la vitesse et la stabilit\u00e9 de l'apprentissage 16 \u00e0 128 typiquement"},{"location":"ressources/base-connaissances/#2-deep-learning-vs-machine-learning-classique","title":"2. Deep Learning vs Machine Learning Classique","text":""},{"location":"ressources/base-connaissances/#tableau-comparatif","title":"Tableau comparatif","text":"Crit\u00e8re Machine Learning Classique Deep Learning Extraction des caract\u00e9ristiques Manuelle (feature engineering) Automatique Volume de donn\u00e9es requis Peut fonctionner avec moins de donn\u00e9es N\u00e9cessite g\u00e9n\u00e9ralement de grands volumes de donn\u00e9es Interpr\u00e9tabilit\u00e9 Souvent plus interpr\u00e9table Fonctionne comme une \"bo\u00eete noire\" Puissance de calcul Moins intensive N\u00e9cessite souvent des GPU Pr\u00e9cision sur des t\u00e2ches complexes Limit\u00e9e pour les donn\u00e9es non structur\u00e9es Excellente pour les images, texte, son Pr\u00e9traitement des donn\u00e9es Souvent complexe et sp\u00e9cifique Plus simple, mais normalisation importante"},{"location":"ressources/base-connaissances/#illustration-concrete","title":"Illustration concr\u00e8te","text":"<p>Le Machine Learning classique n\u00e9cessite une extraction manuelle des caract\u00e9ristiques, tandis que le Deep Learning les apprend automatiquement.</p>"},{"location":"ressources/base-connaissances/#3-fonctions-dactivation-courantes","title":"3. Fonctions d'activation courantes","text":""},{"location":"ressources/base-connaissances/#role-des-fonctions-dactivation","title":"R\u00f4le des fonctions d'activation","text":"<p>Les fonctions d'activation introduisent des non-lin\u00e9arit\u00e9s dans le r\u00e9seau, permettant d'apprendre des relations complexes dans les donn\u00e9es. Sans elles, le r\u00e9seau serait \u00e9quivalent \u00e0 une simple r\u00e9gression lin\u00e9aire.</p>"},{"location":"ressources/base-connaissances/#types-principaux","title":"Types principaux","text":"Fonction Description simple Utilisation typique ReLU Si valeur n\u00e9gative, sortie = 0; sinon, sortie = valeur d'entr\u00e9e Couches cach\u00e9es (standard) Sigmoid Transforme n'importe quel nombre en valeur entre 0 et 1 Sortie pour classification binaire Tanh Similaire \u00e0 Sigmoid mais avec des valeurs entre -1 et 1 Alternative \u00e0 ReLU pour certains r\u00e9seaux Softmax Transforme un groupe de nombres en probabilit\u00e9s qui somment \u00e0 1 Sortie pour classification multi-classes Leaky ReLU Version am\u00e9lior\u00e9e de ReLU qui permet un petit gradient pour les valeurs n\u00e9gatives Alternative \u00e0 ReLU pour \u00e9viter les \"neurones morts\""},{"location":"ressources/base-connaissances/#choix-de-la-fonction-dactivation","title":"Choix de la fonction d'activation","text":"<ul> <li>Couches cach\u00e9es : ReLU est g\u00e9n\u00e9ralement le premier choix pour sa simplicit\u00e9 et efficacit\u00e9</li> <li>Couche de sortie : </li> <li>Sigmoid pour classification binaire (0-1)</li> <li>Softmax pour classification multi-classes (probabilit\u00e9s qui somment \u00e0 1)</li> <li>Lin\u00e9aire pour r\u00e9gression</li> </ul>"},{"location":"ressources/base-connaissances/#4-processus-dentrainement-explique","title":"4. Processus d'entra\u00eenement expliqu\u00e9","text":""},{"location":"ressources/base-connaissances/#etapes-du-processus-dapprentissage","title":"\u00c9tapes du processus d'apprentissage","text":"<ol> <li>Initialisation : Les poids sont initialis\u00e9s avec de petites valeurs al\u00e9atoires</li> <li>Forward Propagation : Les donn\u00e9es traversent le r\u00e9seau pour g\u00e9n\u00e9rer une pr\u00e9diction</li> <li>Calcul de l'erreur : La fonction de perte mesure l'\u00e9cart entre pr\u00e9diction et r\u00e9alit\u00e9</li> <li>Backpropagation : L'erreur est propag\u00e9e en arri\u00e8re pour calculer les gradients</li> <li>Mise \u00e0 jour des poids : Les poids sont ajust\u00e9s dans la direction qui r\u00e9duit l'erreur</li> <li>It\u00e9ration : Les \u00e9tapes 2-5 sont r\u00e9p\u00e9t\u00e9es jusqu'\u00e0 convergence ou nombre maximum d'\u00e9poques</li> </ol>"},{"location":"ressources/base-connaissances/#visualisation-du-processus","title":"Visualisation du processus","text":"<p>Une visualisation montrerait le flux des donn\u00e9es \u00e0 travers le r\u00e9seau, le calcul de l'erreur, et la mise \u00e0 jour des poids.</p>"},{"location":"ressources/base-connaissances/#fonction-de-perte","title":"Fonction de perte","text":"<p>La fonction de perte quantifie l'\u00e9cart entre les pr\u00e9dictions et les valeurs r\u00e9elles. Les plus communes sont :</p> Fonction de perte Usage Description simple Erreur quadratique moyenne (MSE) R\u00e9gression Moyenne des carr\u00e9s des diff\u00e9rences entre pr\u00e9dictions et valeurs r\u00e9elles Entropie crois\u00e9e binaire Classification binaire Mesure \u00e0 quel point les pr\u00e9dictions de probabilit\u00e9 divergent des valeurs r\u00e9elles (0 ou 1) Entropie crois\u00e9e cat\u00e9gorielle Classification multi-classes Version multi-classes de l'entropie crois\u00e9e binaire"},{"location":"ressources/base-connaissances/#5-architectures-cnn-expliquees","title":"5. Architectures CNN expliqu\u00e9es","text":""},{"location":"ressources/base-connaissances/#structure-dun-cnn","title":"Structure d'un CNN","text":"<p>Les CNN (Convolutional Neural Networks) sont sp\u00e9cialement con\u00e7us pour traiter des donn\u00e9es structur\u00e9es en grille, comme les images. Leur architecture typique comprend :</p> <ol> <li>Couche d'entr\u00e9e : Prend l'image brute (pixels)</li> <li>Couches de convolution : Appliquent des filtres pour d\u00e9tecter des caract\u00e9ristiques</li> <li>Couches de pooling : R\u00e9duisent les dimensions tout en pr\u00e9servant l'information importante</li> <li>Couches enti\u00e8rement connect\u00e9es : Effectuent la classification finale</li> </ol>"},{"location":"ressources/base-connaissances/#fonctionnement-des-convolutions","title":"Fonctionnement des convolutions","text":"<p>La convolution consiste \u00e0 faire glisser un filtre (noyau) sur l'image pour d\u00e9tecter des motifs sp\u00e9cifiques. Imaginez une petite fen\u00eatre qui se d\u00e9place sur l'image et cherche des motifs comme des contours, des textures, etc.</p>"},{"location":"ressources/base-connaissances/#hierarchie-des-caracteristiques","title":"Hi\u00e9rarchie des caract\u00e9ristiques","text":"<p>Les CNN apprennent une hi\u00e9rarchie de caract\u00e9ristiques :</p> <ul> <li>Premi\u00e8res couches : D\u00e9tection de bordures et contours simples</li> <li>Couches interm\u00e9diaires : Motifs, textures et formes</li> <li>Couches profondes : Objets et concepts de haut niveau</li> </ul>"},{"location":"ressources/base-connaissances/#6-architectures-rnn-expliquees","title":"6. Architectures RNN expliqu\u00e9es","text":""},{"location":"ressources/base-connaissances/#structure-dun-rnn","title":"Structure d'un RNN","text":"<p>Les RNN (Recurrent Neural Networks) sont con\u00e7us pour traiter des donn\u00e9es s\u00e9quentielles comme le texte, la parole ou les s\u00e9ries temporelles.</p> <p>La caract\u00e9ristique cl\u00e9 des RNN est leur m\u00e9moire interne qui permet de conserver l'information des \u00e9tapes pr\u00e9c\u00e9dentes.</p>"},{"location":"ressources/base-connaissances/#types-de-rnn","title":"Types de RNN","text":"Type Caract\u00e9ristiques Avantages Applications RNN simple Structure de base avec boucle de r\u00e9troaction Simple \u00e0 impl\u00e9menter S\u00e9quences courtes LSTM (Long Short-Term Memory) Cellules sp\u00e9ciales avec \"portes\" pour contr\u00f4ler la m\u00e9moire Meilleure m\u00e9moire \u00e0 long terme Traduction, g\u00e9n\u00e9ration de texte GRU (Gated Recurrent Unit) Version simplifi\u00e9e du LSTM Plus l\u00e9ger que LSTM, performances similaires Applications avec contraintes de ressources Bidirectionnel Traite la s\u00e9quence dans les deux sens (avant et arri\u00e8re) Utilise le contexte futur et pass\u00e9 Compr\u00e9hension du langage"},{"location":"ressources/base-connaissances/#probleme-du-gradient-qui-sevanouitexplose","title":"Probl\u00e8me du gradient qui s'\u00e9vanouit/explose","text":"<p>Les RNN classiques souffrent du probl\u00e8me de la disparition du gradient, ce qui limite leur capacit\u00e9 \u00e0 apprendre des d\u00e9pendances \u00e0 long terme. Les architectures LSTM et GRU ont \u00e9t\u00e9 con\u00e7ues pour r\u00e9soudre ce probl\u00e8me.</p>"},{"location":"ressources/base-connaissances/#7-techniques-doptimisation-avancees","title":"7. Techniques d'optimisation avanc\u00e9es","text":""},{"location":"ressources/base-connaissances/#optimiseurs","title":"Optimiseurs","text":"Optimiseur Description Avantages Inconv\u00e9nients SGD (Stochastic Gradient Descent) Met \u00e0 jour les poids apr\u00e8s chaque exemple Simple Convergence lente, sensible au taux d'apprentissage Adam Adapte le taux d'apprentissage pour chaque param\u00e8tre Rapide, bonne convergence Peut surpasser les minima locaux RMSprop Normalise le gradient par moyenne mobile Bon pour les RNN Sensible aux hyperparam\u00e8tres Adagrad Adapte le taux d'apprentissage en fonction de l'historique Bon pour les donn\u00e9es \u00e9parses Accumulation excessive du d\u00e9nominateur"},{"location":"ressources/base-connaissances/#techniques-de-regularisation","title":"Techniques de r\u00e9gularisation","text":"Technique Description Effet Dropout D\u00e9sactive al\u00e9atoirement des neurones pendant l'entra\u00eenement Emp\u00eache les neurones de trop se sp\u00e9cialiser L1/L2 R\u00e9gularisation Ajoute une p\u00e9nalit\u00e9 bas\u00e9e sur la magnitude des poids Encourage les poids \u00e0 rester petits Batch Normalization Normalise les activations de chaque mini-batch Stabilise et acc\u00e9l\u00e8re l'apprentissage Early Stopping Arr\u00eate l'entra\u00eenement quand la performance sur la validation cesse de s'am\u00e9liorer \u00c9vite le surapprentissage"},{"location":"ressources/base-connaissances/#8-applications-pratiques-du-deep-learning","title":"8. Applications pratiques du Deep Learning","text":""},{"location":"ressources/base-connaissances/#par-domaine-dapplication","title":"Par domaine d'application","text":"Domaine Applications Architectures courantes Vision par ordinateur Reconnaissance d'objets, d\u00e9tection faciale, segmentation d'images CNN, R-CNN, YOLO Traitement du langage naturel Traduction automatique, analyse de sentiment, chatbots RNN, LSTM, Transformers Reconnaissance vocale Transcription automatique, assistants vocaux RNN, LSTM, Transformers S\u00e9ries temporelles Pr\u00e9vision financi\u00e8re, pr\u00e9vision m\u00e9t\u00e9orologique LSTM, GRU, TCN Syst\u00e8mes de recommandation Recommandations de produits, de contenu R\u00e9seaux de neurones profonds, factorisation matricielle G\u00e9n\u00e9ration de contenu G\u00e9n\u00e9ration d'images, de texte, de musique GANs, VAEs, Transformers"},{"location":"ressources/base-connaissances/#exemples-concrets-en-entreprise","title":"Exemples concrets en entreprise","text":"<ol> <li>E-commerce : Recommandation de produits bas\u00e9e sur l'historique d'achat</li> <li>Finance : D\u00e9tection de fraudes en temps r\u00e9el</li> <li>Sant\u00e9 : Aide au diagnostic via l'analyse d'images m\u00e9dicales</li> <li>Industrie : Maintenance pr\u00e9dictive des \u00e9quipements</li> <li>M\u00e9dia : Sous-titrage automatique et traduction de vid\u00e9os</li> </ol>"},{"location":"ressources/base-connaissances/#9-frameworks-et-outils-du-deep-learning","title":"9. Frameworks et outils du Deep Learning","text":""},{"location":"ressources/base-connaissances/#principaux-frameworks","title":"Principaux frameworks","text":"Framework D\u00e9veloppeur Points forts Utilisations typiques TensorFlow Google D\u00e9ploiement en production, TensorFlow Lite pour mobile Applications industrielles, d\u00e9ploiement \u00e0 grande \u00e9chelle Keras Initialement Fran\u00e7ois Chollet, maintenant int\u00e9gr\u00e9 \u00e0 TensorFlow API simple et intuitive Prototypage rapide, enseignement PyTorch Facebook (Meta) D\u00e9bogage facile, dynamique Recherche, prototypage, projets acad\u00e9miques JAX Google Calcul diff\u00e9rentiable haute performance Recherche avanc\u00e9e, mod\u00e8les tr\u00e8s larges"},{"location":"ressources/base-connaissances/#ecosysteme-doutils","title":"\u00c9cosyst\u00e8me d'outils","text":"<ul> <li>Google Colab : Environnement notebook avec GPU/TPU gratuits</li> <li>Jupyter Notebooks : D\u00e9veloppement interactif</li> <li>TensorBoard : Visualisation des m\u00e9triques d'entra\u00eenement</li> <li>MLflow : Gestion du cycle de vie des mod\u00e8les ML</li> <li>Hugging Face : Biblioth\u00e8que de mod\u00e8les pr\u00e9-entra\u00een\u00e9s pour NLP</li> <li>ONNX : Standard d'interop\u00e9rabilit\u00e9 entre frameworks</li> </ul>"},{"location":"ressources/base-connaissances/#10-conseils-pratiques-pour-limplementation","title":"10. Conseils pratiques pour l'impl\u00e9mentation","text":""},{"location":"ressources/base-connaissances/#bonnes-pratiques","title":"Bonnes pratiques","text":"<ol> <li>Commencer simple puis augmenter la complexit\u00e9</li> <li>Normaliser les donn\u00e9es d'entr\u00e9e (moyenne 0, \u00e9cart-type 1)</li> <li>Visualiser les donn\u00e9es avant de construire le mod\u00e8le</li> <li>Surveiller les m\u00e9triques sur un ensemble de validation</li> <li>Utiliser des techniques de r\u00e9gularisation pour \u00e9viter le surapprentissage</li> <li>Adopter un taux d'apprentissage adaptatif (learning rate schedules)</li> <li>Impl\u00e9menter l'early stopping pour \u00e9viter le surapprentissage</li> <li>Faire des sauvegardes r\u00e9guli\u00e8res du mod\u00e8le pendant l'entra\u00eenement</li> </ol>"},{"location":"ressources/base-connaissances/#erreurs-courantes-a-eviter","title":"Erreurs courantes \u00e0 \u00e9viter","text":"<ol> <li>\u274c Fuites de donn\u00e9es entre ensembles d'entra\u00eenement et de test</li> <li>\u274c Surapprentissage en utilisant trop de param\u00e8tres pour peu de donn\u00e9es</li> <li>\u274c Taux d'apprentissage inadapt\u00e9 (trop grand ou trop petit)</li> <li>\u274c Fonction de perte inappropri\u00e9e pour le probl\u00e8me</li> <li>\u274c Mauvaise initialisation des poids causant la saturation des neurones</li> <li>\u274c D\u00e9s\u00e9quilibre des classes non pris en compte dans les donn\u00e9es</li> </ol>"},{"location":"ressources/base-connaissances/#processus-de-developpement-recommande","title":"Processus de d\u00e9veloppement recommand\u00e9","text":"<ol> <li>Explorer et comprendre les donn\u00e9es</li> <li>\u00c9tablir une baseline (mod\u00e8le simple)</li> <li>It\u00e9rer en am\u00e9liorant progressivement</li> <li>Surveiller les performances et \u00e9viter le surapprentissage</li> <li>Optimiser les hyperparam\u00e8tres</li> <li>\u00c9valuer sur des donn\u00e9es de test ind\u00e9pendantes</li> </ol> <p>Ce guide de base de connaissances vous servira tout au long du cours pour comprendre et appliquer les concepts du Deep Learning dans vos projets pratiques.</p>"},{"location":"ressources/competences-stage-sio/","title":"Comp\u00e9tences Deep Learning recherch\u00e9es en stage BTS SIO","text":"<p>Ce document pr\u00e9sente les comp\u00e9tences en Deep Learning les plus demand\u00e9es, bas\u00e9es sur une analyse des offres de stage et des retours d'entreprises.</p>"},{"location":"ressources/competences-stage-sio/#competences-techniques-prioritaires","title":"Comp\u00e9tences techniques prioritaires","text":"Comp\u00e9tence Niveau attendu Application en entreprise Utilisation de frameworks Savoir utiliser TensorFlow/Keras pour des cas simples Int\u00e9gration de fonctionnalit\u00e9s IA dans des applications existantes API REST Cr\u00e9er et documenter une API exposant des mod\u00e8les ML Cr\u00e9ation de services accessibles par d'autres applications Mod\u00e8les pr\u00e9-entra\u00een\u00e9s Adapter des mod\u00e8les existants \u00e0 des besoins sp\u00e9cifiques Reconnaissance d'images, classification de textes, etc. Int\u00e9gration web Connecter des mod\u00e8les ML \u00e0 des interfaces web Applications web avec fonctionnalit\u00e9s intelligentes Documentation technique Documenter clairement le fonctionnement d'un syst\u00e8me IA Faciliter la maintenance et le transfert de connaissances"},{"location":"ressources/competences-stage-sio/#competences-differenciantes","title":"Comp\u00e9tences diff\u00e9renciantes","text":"<p>Ces comp\u00e9tences ne sont pas syst\u00e9matiquement demand\u00e9es mais constituent un avantage significatif :</p> <ul> <li>D\u00e9ploiement de mod\u00e8les : Mettre en production un mod\u00e8le (Docker, cloud)</li> <li>Optimisation de performances : Am\u00e9liorer la vitesse d'inf\u00e9rence d'un mod\u00e8le</li> <li>D\u00e9veloppement de chatbots : Cr\u00e9er des assistants conversationnels simples</li> <li>Visualisation de donn\u00e9es : Pr\u00e9senter efficacement les r\u00e9sultats d'un mod\u00e8le</li> <li>Tests et validation : Assurer la fiabilit\u00e9 d'un syst\u00e8me d'IA</li> </ul>"},{"location":"ressources/competences-stage-sio/#competences-non-techniques-valorisees","title":"Comp\u00e9tences non-techniques valoris\u00e9es","text":"Comp\u00e9tence Description Pourquoi c'est important Vulgarisation technique Expliquer simplement des concepts complexes Communication avec les \u00e9quipes non-techniques \u00c9valuation des limites Identifier ce qui est r\u00e9alisable ou non avec l'IA \u00c9viter les promesses irr\u00e9alistes Veille technologique Se tenir inform\u00e9 des nouvelles possibilit\u00e9s Proposer des solutions innovantes \u00c9thique de l'IA Conscience des implications \u00e9thiques D\u00e9velopper des solutions responsables Autonomie d'apprentissage Capacit\u00e9 \u00e0 s'autoformer sur de nouveaux outils S'adapter rapidement dans un domaine en \u00e9volution"},{"location":"ressources/competences-stage-sio/#exemples-de-missions-de-stage","title":"Exemples de missions de stage","text":""},{"location":"ressources/competences-stage-sio/#pme-startup","title":"PME / Startup","text":"<ul> <li>D\u00e9veloppement d'un syst\u00e8me de reconnaissance de documents (factures, BL)</li> <li>Int\u00e9gration d'un chatbot d'assistance client sur un site e-commerce</li> <li>Cr\u00e9ation d'un module d'analyse de sentiments pour les avis clients</li> </ul>"},{"location":"ressources/competences-stage-sio/#esn-agences-digitales","title":"ESN / Agences digitales","text":"<ul> <li>Cr\u00e9ation d'une API d'analyse d'images pour une application mobile</li> <li>D\u00e9veloppement d'une preuve de concept utilisant un LLM pour l'assistance utilisateur</li> <li>Int\u00e9gration d'un syst\u00e8me de recommandation dans une application existante</li> </ul>"},{"location":"ressources/competences-stage-sio/#grandes-entreprises","title":"Grandes entreprises","text":"<ul> <li>Am\u00e9lioration d'un syst\u00e8me existant de d\u00e9tection d'anomalies</li> <li>Automatisation de t\u00e2ches de classification de tickets support</li> <li>D\u00e9veloppement d'un dashboard de suivi de performances de mod\u00e8les ML</li> </ul>"},{"location":"ressources/competences-stage-sio/#technologies-couramment-utilisees","title":"Technologies couramment utilis\u00e9es","text":"Cat\u00e9gorie Technologies Niveau d'expertise attendu Frameworks ML TensorFlow/Keras, Hugging Face Interm\u00e9diaire Langages Python, JavaScript Interm\u00e9diaire D\u00e9ploiement Flask, FastAPI, Docker D\u00e9butant/Interm\u00e9diaire Front-end React, Vue.js D\u00e9butant Base de donn\u00e9es MongoDB, PostgreSQL D\u00e9butant Cloud Google Cloud, Azure Notions"},{"location":"ressources/competences-stage-sio/#conseils-pour-valoriser-ces-competences","title":"Conseils pour valoriser ces comp\u00e9tences","text":"<ol> <li>Cr\u00e9ez un portfolio avec des exemples concrets de mini-projets</li> <li>Documentez vos projets en expliquant clairement votre d\u00e9marche</li> <li>Pr\u00e9parez des d\u00e9mos fonctionnelles \u00e0 montrer lors des entretiens</li> <li>Participez \u00e0 des projets open source ou des hackathons IA</li> <li>Cr\u00e9ez un profil GitHub regroupant vos r\u00e9alisations en IA/ML</li> </ol>"},{"location":"ressources/guide-etudiant/","title":"Guide d'utilisation des ressources - Formation Deep Learning","text":""},{"location":"ressources/guide-etudiant/#bienvenue-dans-votre-formation-deep-learning","title":"Bienvenue dans votre formation Deep Learning !","text":"<p>Ce guide vous explique comment acc\u00e9der et utiliser les diff\u00e9rentes ressources du cours.</p>"},{"location":"ressources/guide-etudiant/#1-acces-aux-notebooks-jupyter","title":"1. Acc\u00e8s aux notebooks Jupyter","text":""},{"location":"ressources/guide-etudiant/#quest-ce-quun-notebook-jupyter","title":"Qu'est-ce qu'un notebook Jupyter ?","text":"<p>Un notebook Jupyter est un document interactif qui vous permet d'ex\u00e9cuter du code Python directement dans votre navigateur, tout en incluant du texte explicatif, des images et des visualisations. C'est l'outil id\u00e9al pour apprendre le Deep Learning de fa\u00e7on pratique.</p>"},{"location":"ressources/guide-etudiant/#comment-acceder-aux-notebooks-du-cours","title":"Comment acc\u00e9der aux notebooks du cours","text":"<p>Nous utilisons Google Colab, qui vous permet d'ex\u00e9cuter des notebooks Jupyter dans le cloud, sans aucune installation sur votre ordinateur. Vous avez simplement besoin d'un compte Google.</p> <p>Pour acc\u00e9der \u00e0 chaque notebook : 1. Cliquez sur le badge \"Open in Colab\" ou le lien correspondant au notebook 2. Le notebook s'ouvrira dans Google Colab 3. Si demand\u00e9, connectez-vous avec votre compte Google</p>"},{"location":"ressources/guide-etudiant/#enregistrer-votre-travail","title":"Enregistrer votre travail","text":"<p>Important : Les notebooks s'ouvrent en mode lecture seule. Pour sauvegarder votre travail : 1. Allez dans le menu \"Fichier\" &gt; \"Enregistrer une copie dans Drive\" 2. Une copie personnelle sera cr\u00e9\u00e9e dans votre Google Drive 3. Travaillez d\u00e9sormais sur cette copie</p>"},{"location":"ressources/guide-etudiant/#liste-des-notebooks-disponibles","title":"Liste des notebooks disponibles","text":"Notebook Description Lien direct Hello World du Deep Learning Premier r\u00e9seau de neurones sur MNIST Machine Learning classique Classification avec Random Forest Deep Learning Classification avec r\u00e9seau de neurones Anatomie d'un r\u00e9seau Exploration interactive d'un r\u00e9seau Template du mini-projet Base pour le challenge d'am\u00e9lioration"},{"location":"ressources/guide-etudiant/#2-utilisation-des-notebooks","title":"2. Utilisation des notebooks","text":""},{"location":"ressources/guide-etudiant/#executer-les-cellules","title":"Ex\u00e9cuter les cellules","text":"<ul> <li>Pour ex\u00e9cuter une cellule de code, cliquez sur le bouton \u25b6\ufe0f \u00e0 gauche de la cellule ou appuyez sur <code>Ctrl+Entr\u00e9e</code></li> <li>Ex\u00e9cutez les cellules dans l'ordre, de haut en bas</li> <li>Attendez qu'une cellule ait termin\u00e9 son ex\u00e9cution avant de passer \u00e0 la suivante (le symbole \u25cf devient \u2713)</li> </ul>"},{"location":"ressources/guide-etudiant/#types-de-cellules","title":"Types de cellules","text":"<ul> <li>Cellules de code : Contiennent du code Python ex\u00e9cutable</li> <li>Cellules de texte : Contiennent des explications et des instructions</li> </ul>"},{"location":"ressources/guide-etudiant/#conseils-pratiques","title":"Conseils pratiques","text":"<ul> <li>Red\u00e9marrer le runtime : Si vous rencontrez des erreurs, essayez de red\u00e9marrer le runtime (Menu \"Runtime\" &gt; \"Restart runtime\")</li> <li>RAM limit\u00e9e : Si vous recevez des erreurs de m\u00e9moire, fermez les autres onglets Colab</li> <li>D\u00e9connexion : Google Colab peut se d\u00e9connecter apr\u00e8s inactivit\u00e9, sauvegardez r\u00e9guli\u00e8rement</li> </ul>"},{"location":"ressources/guide-etudiant/#3-documents-a-completer","title":"3. Documents \u00e0 compl\u00e9ter","text":""},{"location":"ressources/guide-etudiant/#telechargement-des-fiches","title":"T\u00e9l\u00e9chargement des fiches","text":"<p>Pour chaque s\u00e9ance, t\u00e9l\u00e9chargez les fiches d'observation et autres documents \u00e0 compl\u00e9ter : 1. Cliquez sur les liens fournis dans la page de la s\u00e9ance 2. Choisissez entre la version PDF (pour impression) ou Word/ODT (pour \u00e9dition \u00e9lectronique)</p>"},{"location":"ressources/guide-etudiant/#soumission-des-travaux","title":"Soumission des travaux","text":"<p>Pour soumettre vos travaux compl\u00e9t\u00e9s : 1. Nommez vos fichiers avec votre nom et le num\u00e9ro de s\u00e9ance (ex: \"Dupont_Seance1_Fiche.docx\") 2. D\u00e9posez-les sur la plateforme de cours en ligne ou envoyez-les par email selon les instructions de votre formateur 3. Pour les notebooks, partagez l'URL de votre copie dans Google Drive ou exportez-les au format .ipynb</p>"},{"location":"ressources/guide-etudiant/#4-resolution-des-problemes-courants","title":"4. R\u00e9solution des probl\u00e8mes courants","text":"Probl\u00e8me Solution Le notebook ne se charge pas V\u00e9rifiez votre connexion internet ou r\u00e9essayez dans quelques minutes Erreur \"CUDA out of memory\" Allez dans \"Runtime\" &gt; \"Change runtime type\" et s\u00e9lectionnez \"None\" pour GPU Modules manquants Ex\u00e9cutez <code>!pip install nom-du-module</code> dans une cellule Acc\u00e8s refus\u00e9 Assurez-vous d'\u00eatre connect\u00e9 \u00e0 votre compte Google"},{"location":"ressources/guide-etudiant/#5-ressources-complementaires","title":"5. Ressources compl\u00e9mentaires","text":"<ul> <li>Documentation TensorFlow</li> <li>Documentation Keras</li> <li>Tutoriels Google Colab</li> </ul>"},{"location":"ressources/instructions-integration/","title":"Guide d'int\u00e9gration FastAPI pour le chatbot p\u00e9dagogique","text":"<p>Ce guide explique comment int\u00e9grer FastAPI dans votre projet de chatbot p\u00e9dagogique sur le Deep Learning pour b\u00e9n\u00e9ficier de meilleures performances et de fonctionnalit\u00e9s plus avanc\u00e9es.</p>"},{"location":"ressources/instructions-integration/#pourquoi-passer-a-fastapi","title":"Pourquoi passer \u00e0 FastAPI ?","text":"<p>FastAPI offre plusieurs avantages pour notre projet :</p> <ol> <li>Performance : FastAPI est l'un des frameworks Python les plus rapides, bas\u00e9 sur Starlette et Pydantic</li> <li>Documentation automatique : G\u00e9n\u00e9ration automatique de documentation interactive (OpenAPI/Swagger)</li> <li>Validation des donn\u00e9es : Validation automatique des requ\u00eates et r\u00e9ponses avec Pydantic</li> <li>Support asynchrone natif : Support de l'asynchrone pour les op\u00e9rations \u00e0 latence \u00e9lev\u00e9e (comme les appels API externes)</li> <li>Typage fort : Utilisation du typage Python pour une meilleure d\u00e9tection d'erreurs</li> </ol>"},{"location":"ressources/instructions-integration/#prerequis","title":"Pr\u00e9requis","text":"<ul> <li>Python 3.7 ou sup\u00e9rieur</li> <li>Acc\u00e8s \u00e0 un terminal pour installer les packages</li> <li>Compte et cl\u00e9 API Mistral AI</li> </ul>"},{"location":"ressources/instructions-integration/#installation-des-dependances","title":"Installation des d\u00e9pendances","text":"<pre><code>pip install fastapi uvicorn mistralai python-dotenv pydantic\n</code></pre> <p>Note: <code>uvicorn</code> est le serveur ASGI recommand\u00e9 pour ex\u00e9cuter FastAPI.</p>"},{"location":"ressources/instructions-integration/#structure-du-projet","title":"Structure du projet","text":"<p>Voici la structure de base recommand\u00e9e pour votre projet :</p> <pre><code>chatbot-pedagogique/\n\u251c\u2500\u2500 .env                    # Variables d'environnement (cl\u00e9s API)\n\u251c\u2500\u2500 main.py                 # Point d'entr\u00e9e principal avec FastAPI\n\u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 conversation.py     # Mod\u00e8les Pydantic pour les requ\u00eates/r\u00e9ponses\n\u2502   \u2514\u2500\u2500 knowledge_base.py   # Mod\u00e8les pour la base de connaissances\n\u251c\u2500\u2500 services/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 mistral_service.py  # Interaction avec l'API Mistral\n\u2502   \u2514\u2500\u2500 knowledge_service.py # Gestion de la base de connaissances\n\u251c\u2500\u2500 static/                 # Fichiers statiques (CSS, JS, images)\n\u2514\u2500\u2500 templates/              # Templates HTML (si utilis\u00e9s)\n</code></pre>"},{"location":"ressources/instructions-integration/#configuration-initiale","title":"Configuration initiale","text":""},{"location":"ressources/instructions-integration/#1-fichier-env","title":"1. Fichier .env","text":"<p>Cr\u00e9ez un fichier <code>.env</code> \u00e0 la racine du projet avec votre cl\u00e9 API :</p> <pre><code>MISTRAL_API_KEY=votre_cl\u00e9_api_ici\n</code></pre>"},{"location":"ressources/instructions-integration/#2-modeles-pydantic-modelsconversationpy","title":"2. Mod\u00e8les Pydantic (models/conversation.py)","text":"<p>D\u00e9finissez les mod\u00e8les de donn\u00e9es pour les requ\u00eates et r\u00e9ponses :</p> <pre><code>from typing import List, Optional\nfrom pydantic import BaseModel\n\nclass Message(BaseModel):\n    role: str  # \"user\" ou \"assistant\"\n    content: str\n\nclass ConversationRequest(BaseModel):\n    messages: List[Message]\n    user_level: Optional[str] = \"beginner\"\n    temperature: Optional[float] = 0.7\n    model: Optional[str] = \"mistral-medium\"\n\nclass ConversationResponse(BaseModel):\n    response: str\n    conversation_id: str\n</code></pre>"},{"location":"ressources/instructions-integration/#3-service-mistral-servicesmistral_servicepy","title":"3. Service Mistral (services/mistral_service.py)","text":"<p>Cr\u00e9ez un service pour interagir avec l'API Mistral :</p> <pre><code>import os\nfrom typing import List, Dict, Any\nfrom mistralai.client import MistralClient\nfrom mistralai.models.chat_completion import ChatMessage\n\nclass MistralService:\n    def __init__(self):\n        self.api_key = os.getenv(\"MISTRAL_API_KEY\")\n        self.client = MistralClient(api_key=self.api_key)\n\n    def generate_response(self, messages: List[Dict[str, str]], \n                         model: str = \"mistral-medium\", \n                         temperature: float = 0.7) -&gt; str:\n        \"\"\"\n        G\u00e9n\u00e8re une r\u00e9ponse via l'API Mistral.\n\n        Args:\n            messages: Liste de messages au format {\"role\": \"...\", \"content\": \"...\"}\n            model: Mod\u00e8le Mistral \u00e0 utiliser\n            temperature: Temp\u00e9rature (cr\u00e9ativit\u00e9) de la g\u00e9n\u00e9ration\n\n        Returns:\n            str: R\u00e9ponse g\u00e9n\u00e9r\u00e9e\n        \"\"\"\n        # Convertir les messages au format attendu par l'API Mistral\n        mistral_messages = [\n            ChatMessage(role=msg[\"role\"], content=msg[\"content\"])\n            for msg in messages\n        ]\n\n        try:\n            # Appel \u00e0 l'API Mistral\n            response = self.client.chat(\n                model=model,\n                messages=mistral_messages,\n                temperature=temperature\n            )\n\n            return response.choices[0].message.content\n        except Exception as e:\n            # Gestion des erreurs\n            print(f\"Erreur lors de l'appel \u00e0 l'API Mistral: {e}\")\n            return f\"D\u00e9sol\u00e9, une erreur s'est produite lors de la g\u00e9n\u00e9ration de la r\u00e9ponse: {str(e)}\"\n</code></pre>"},{"location":"ressources/instructions-integration/#4-application-principale-mainpy","title":"4. Application principale (main.py)","text":"<p>L'application FastAPI principale qui expose les endpoints :</p> <pre><code>import os\nimport uuid\nfrom fastapi import FastAPI, HTTPException, BackgroundTasks\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.staticfiles import StaticFiles\nfrom fastapi.responses import JSONResponse\n\nfrom models.conversation import ConversationRequest, ConversationResponse\nfrom services.mistral_service import MistralService\n\n# Chargement des variables d'environnement\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Initialisation de l'application FastAPI\napp = FastAPI(\n    title=\"Chatbot p\u00e9dagogique API\",\n    description=\"API pour le chatbot p\u00e9dagogique sur le Deep Learning\",\n    version=\"1.0.0\"\n)\n\n# Configuration CORS\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  # En production, sp\u00e9cifiez les domaines autoris\u00e9s\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Initialisation des services\nmistral_service = MistralService()\n\n# Stockage des conversations (dans un vrai projet, utilisez une base de donn\u00e9es)\nconversations = {}\n\n# Routes\n@app.post(\"/api/chat\", response_model=ConversationResponse)\nasync def chat(request: ConversationRequest):\n    \"\"\"\n    Endpoint pour interagir avec le chatbot\n    \"\"\"\n    # G\u00e9n\u00e9rer un ID de conversation s'il n'existe pas\n    conversation_id = str(uuid.uuid4())\n\n    try:\n        # Obtenir la r\u00e9ponse de Mistral AI\n        system_message = {\n            \"role\": \"system\",\n            \"content\": f\"Tu es un assistant p\u00e9dagogique sp\u00e9cialis\u00e9 en Deep Learning pour des \u00e9tudiants de niveau {request.user_level}.\"\n        }\n\n        # Pr\u00e9parer les messages avec le message syst\u00e8me en premier\n        all_messages = [system_message] + [msg.dict() for msg in request.messages]\n\n        # G\u00e9n\u00e9rer la r\u00e9ponse\n        response = mistral_service.generate_response(\n            messages=all_messages,\n            model=request.model,\n            temperature=request.temperature\n        )\n\n        # Sauvegarder la conversation\n        if conversation_id not in conversations:\n            conversations[conversation_id] = all_messages\n        conversations[conversation_id].append({\"role\": \"assistant\", \"content\": response})\n\n        return ConversationResponse(\n            response=response,\n            conversation_id=conversation_id\n        )\n\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/api/health\")\nasync def health_check():\n    \"\"\"\n    Endpoint de v\u00e9rification de la sant\u00e9 de l'API\n    \"\"\"\n    return {\"status\": \"healthy\"}\n\n# Servir les fichiers statiques (si n\u00e9cessaire)\napp.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n\n# Point d'entr\u00e9e\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(\"main:app\", host=\"0.0.0.0\", port=8000, reload=True)\n</code></pre>"},{"location":"ressources/instructions-integration/#utilisation-de-lapi-fastapi","title":"Utilisation de l'API FastAPI","text":""},{"location":"ressources/instructions-integration/#lancement-de-lapplication","title":"Lancement de l'application","text":"<p>Pour d\u00e9marrer l'application :</p> <pre><code>uvicorn main:app --reload\n</code></pre> <p>L'application sera accessible \u00e0 l'adresse <code>http://localhost:8000</code>.</p>"},{"location":"ressources/instructions-integration/#documentation-automatique","title":"Documentation automatique","text":"<p>FastAPI g\u00e9n\u00e8re automatiquement une documentation interactive :</p> <ul> <li>Swagger UI: <code>http://localhost:8000/docs</code></li> <li>ReDoc: <code>http://localhost:8000/redoc</code></li> </ul>"},{"location":"ressources/instructions-integration/#appel-de-lapi-depuis-javascript","title":"Appel de l'API depuis JavaScript","text":"<p>Voici comment appeler votre API depuis le frontend :</p> <pre><code>async function sendMessage(message) {\n    const conversation = [\n        {\n            role: \"user\",\n            content: message\n        }\n    ];\n\n    try {\n        const response = await fetch('http://localhost:8000/api/chat', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json'\n            },\n            body: JSON.stringify({\n                messages: conversation,\n                user_level: \"beginner\", // ou \"intermediate\", \"advanced\"\n                temperature: 0.7\n            })\n        });\n\n        if (!response.ok) {\n            throw new Error('Erreur r\u00e9seau ou serveur');\n        }\n\n        const data = await response.json();\n        return data.response;\n    } catch (error) {\n        console.error('Erreur:', error);\n        return \"D\u00e9sol\u00e9, une erreur s'est produite lors de la communication avec le serveur.\";\n    }\n}\n</code></pre>"},{"location":"ressources/instructions-integration/#fonctionnalites-avancees","title":"Fonctionnalit\u00e9s avanc\u00e9es","text":""},{"location":"ressources/instructions-integration/#1-traitement-asynchrone","title":"1. Traitement asynchrone","text":"<p>FastAPI supporte nativement l'asynchrone, utile pour les op\u00e9rations \u00e0 latence \u00e9lev\u00e9e :</p> <pre><code>@app.post(\"/api/chat/async\")\nasync def chat_async(request: ConversationRequest, background_tasks: BackgroundTasks):\n    # G\u00e9n\u00e9rer un ID pour cette requ\u00eate\n    request_id = str(uuid.uuid4())\n\n    # Traiter la requ\u00eate en arri\u00e8re-plan\n    background_tasks.add_task(process_chat_request, request, request_id)\n\n    return {\"status\": \"processing\", \"request_id\": request_id}\n\nasync def process_chat_request(request: ConversationRequest, request_id: str):\n    # Traitement asynchrone de la requ\u00eate\n    # ...\n</code></pre>"},{"location":"ressources/instructions-integration/#2-dependances-et-injection","title":"2. D\u00e9pendances et injection","text":"<p>FastAPI offre un syst\u00e8me de d\u00e9pendances puissant :</p> <pre><code>async def get_mistral_service():\n    return MistralService()\n\n@app.post(\"/api/chat\")\nasync def chat(\n    request: ConversationRequest, \n    mistral_service: MistralService = Depends(get_mistral_service)\n):\n    # Utiliser le service inject\u00e9\n    # ...\n</code></pre>"},{"location":"ressources/instructions-integration/#3-rate-limiting","title":"3. Rate limiting","text":"<p>Impl\u00e9mentation simple de rate limiting :</p> <pre><code>from fastapi import Request, HTTPException\nimport time\n\n# Dictionnaire pour stocker les compteurs de requ\u00eates\nrequest_counts = {}\n\n@app.middleware(\"http\")\nasync def rate_limit_middleware(request: Request, call_next):\n    client_ip = request.client.host\n\n    # Initialiser ou r\u00e9cup\u00e9rer les donn\u00e9es de l'IP\n    if client_ip not in request_counts:\n        request_counts[client_ip] = {\"count\": 0, \"reset_time\": time.time() + 60}\n\n    # V\u00e9rifier si le compteur doit \u00eatre r\u00e9initialis\u00e9\n    if time.time() &gt; request_counts[client_ip][\"reset_time\"]:\n        request_counts[client_ip] = {\"count\": 0, \"reset_time\": time.time() + 60}\n\n    # V\u00e9rifier la limite\n    if request_counts[client_ip][\"count\"] &gt;= 10:  # Limite de 10 requ\u00eates par minute\n        raise HTTPException(status_code=429, detail=\"Trop de requ\u00eates\")\n\n    # Incr\u00e9menter le compteur\n    request_counts[client_ip][\"count\"] += 1\n\n    # Continuer avec la requ\u00eate\n    response = await call_next(request)\n    return response\n</code></pre>"},{"location":"ressources/instructions-integration/#bonnes-pratiques","title":"Bonnes pratiques","text":"<ol> <li>Utiliser des mod\u00e8les Pydantic pour valider les entr\u00e9es/sorties</li> <li>Organiser le code en modules r\u00e9utilisables (services, mod\u00e8les, etc.)</li> <li>Impl\u00e9menter la gestion d'erreurs pour toutes les op\u00e9rations critiques</li> <li>Documenter les endpoints avec des docstrings d\u00e9taill\u00e9es</li> <li>Utiliser des d\u00e9pendances pour l'injection et la r\u00e9utilisation du code</li> <li>Impl\u00e9menter des tests avec pytest et le client de test FastAPI</li> <li>Utiliser des variables d'environnement pour les configurations sensibles</li> <li>Mettre en cache les r\u00e9ponses fr\u00e9quentes pour optimiser les performances</li> </ol>"},{"location":"ressources/instructions-integration/#comparaison-avec-flask","title":"Comparaison avec Flask","text":"Fonctionnalit\u00e9 FastAPI Flask Performance Tr\u00e8s rapide (bas\u00e9 sur Starlette) Moins performant Documentation Automatique (OpenAPI/Swagger) Manuelle ou extensions tierces Validation Automatique avec Pydantic Manuelle ou Flask-WTF Asynchrone Support natif Pas de support natif Typage Typage fort Pas de typage par d\u00e9faut Extensions \u00c9cosyst\u00e8me en croissance \u00c9cosyst\u00e8me mature Courbe d'apprentissage Moyenne Faible"},{"location":"ressources/instructions-integration/#ressources-supplementaires","title":"Ressources suppl\u00e9mentaires","text":"<ul> <li>Documentation officielle FastAPI</li> <li>Pydantic</li> <li>Uvicorn ASGI Server</li> <li>Mistral AI API Docs</li> </ul>"},{"location":"ressources/instructions-integration/#troubleshooting","title":"Troubleshooting","text":"Probl\u00e8me Solution <code>ModuleNotFoundError</code> V\u00e9rifiez que vous avez install\u00e9 toutes les d\u00e9pendances Erreur CORS Assurez-vous que le middleware CORS est correctement configur\u00e9 Erreur 422 Unprocessable Entity V\u00e9rifiez la structure de votre requ\u00eate selon les mod\u00e8les Pydantic Erreur API Mistral V\u00e9rifiez votre cl\u00e9 API et la disponibilit\u00e9 du service <p>Ce guide vous a fourni une base solide pour int\u00e9grer FastAPI dans votre projet de chatbot p\u00e9dagogique. N'h\u00e9sitez pas \u00e0 explorer les fonctionnalit\u00e9s avanc\u00e9es de FastAPI pour am\u00e9liorer encore votre application.</p>"},{"location":"ressources/json-schemas/","title":"Sch\u00e9mas JSON pour le Chatbot P\u00e9dagogique","text":"<p>Ce document d\u00e9crit les structures JSON utilis\u00e9es pour organiser la base de connaissances du chatbot p\u00e9dagogique sur le Deep Learning. Ces sch\u00e9mas permettent de standardiser les donn\u00e9es et de faciliter leur utilisation par l'application.</p>"},{"location":"ressources/json-schemas/#base-de-connaissances-principale","title":"Base de connaissances principale","text":""},{"location":"ressources/json-schemas/#structure-globale","title":"Structure globale","text":"<p>La base de connaissances est organis\u00e9e en concepts principaux, chacun contenant des sous-concepts. Voici la structure g\u00e9n\u00e9rale:</p> <pre><code>{\n  \"concepts\": [\n    {\n      \"id\": \"string\",\n      \"title\": \"string\",\n      \"description\": \"string\",\n      \"subconcepts\": [\n        {\n          \"id\": \"string\",\n          \"title\": \"string\",\n          \"definition\": \"string\",\n          \"details\": {\n            \"beginner\": \"string\",\n            \"intermediate\": \"string\",\n            \"advanced\": \"string\"\n          },\n          \"examples\": [\"string\"],\n          \"related\": [\"string\"]\n        }\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"ressources/json-schemas/#description-des-champs","title":"Description des champs","text":"Champ Type Description Exemple <code>concepts</code> array Liste des concepts principaux <code>id</code> string Identifiant unique du concept (format: snake_case) \"neural_network\" <code>title</code> string Titre lisible du concept \"R\u00e9seau de neurones\" <code>description</code> string Description g\u00e9n\u00e9rale du concept \"Mod\u00e8le de calcul inspir\u00e9 du cerveau...\" <code>subconcepts</code> array Liste des sous-concepts <code>definition</code> string D\u00e9finition concise du sous-concept \"Un r\u00e9seau de neurones est...\" <code>details</code> object Explications adapt\u00e9es \u00e0 diff\u00e9rents niveaux <code>details.beginner</code> string Explication pour les d\u00e9butants \"Imaginez une s\u00e9rie de filtres...\" <code>details.intermediate</code> string Explication de niveau interm\u00e9diaire \"Techniquement, un CNN utilise...\" <code>details.advanced</code> string Explication avanc\u00e9e avec termes techniques \"L'op\u00e9ration de convolution peut...\" <code>examples</code> array Liste d'exemples concrets [\"ResNet\", \"LeNet-5\"] <code>related</code> array Liste d'IDs de concepts li\u00e9s [\"convolution\", \"pooling\"]"},{"location":"ressources/json-schemas/#exemple-de-concept","title":"Exemple de concept","text":"<p>Voici un exemple complet d'un concept dans la base de connaissances:</p> <pre><code>{\n  \"id\": \"cnn\",\n  \"title\": \"R\u00e9seau de neurones convolutif (CNN)\",\n  \"definition\": \"Type de r\u00e9seau sp\u00e9cialis\u00e9 dans le traitement des donn\u00e9es en grille comme les images, utilisant des op\u00e9rations de convolution pour d\u00e9tecter des motifs spatiaux.\",\n  \"details\": {\n    \"beginner\": \"Les CNN sont des r\u00e9seaux sp\u00e9cialis\u00e9s pour analyser les images. Ils utilisent des filtres qui 'glissent' sur l'image pour d\u00e9tecter des motifs comme les contours, les textures, puis des formes plus complexes.\",\n    \"intermediate\": \"Ces r\u00e9seaux exploitent trois id\u00e9es cl\u00e9s: les connexions locales (chaque neurone voit seulement une petite r\u00e9gion), le partage de param\u00e8tres (les m\u00eames filtres sont appliqu\u00e9s partout), et la mise en commun (pooling) pour r\u00e9duire la dimensionnalit\u00e9 tout en pr\u00e9servant les caract\u00e9ristiques importantes.\",\n    \"advanced\": \"L'op\u00e9ration de convolution peut \u00eatre vue comme un produit de tenseurs avec un noyau partag\u00e9, ce qui r\u00e9duit consid\u00e9rablement le nombre de param\u00e8tres par rapport \u00e0 un r\u00e9seau enti\u00e8rement connect\u00e9. Cette inductive bias de localit\u00e9 et d'invariance \u00e0 la translation est particuli\u00e8rement adapt\u00e9e aux donn\u00e9es visuelles.\"\n  },\n  \"examples\": [\n    \"LeNet-5 (1998): Premier CNN efficace, utilis\u00e9 pour la reconnaissance de chiffres manuscrits\",\n    \"ResNet (2015): Architecture introduisant les connexions r\u00e9siduelles pour entra\u00eener des r\u00e9seaux tr\u00e8s profonds\",\n    \"EfficientNet (2019): Famille de CNN optimis\u00e9s pour le rapport performance/nombre de param\u00e8tres\"\n  ],\n  \"related\": [\"convolution\", \"pooling\", \"image_recognition\", \"feature_map\"]\n}\n</code></pre>"},{"location":"ressources/json-schemas/#schema-pour-les-questions-et-reponses","title":"Sch\u00e9ma pour les questions et r\u00e9ponses","text":"<p>Pour le syst\u00e8me de quiz et d'exercices, un sch\u00e9ma diff\u00e9rent est utilis\u00e9:</p> <pre><code>{\n  \"quizzes\": [\n    {\n      \"id\": \"string\",\n      \"topic\": \"string\",\n      \"difficulty\": \"beginner|intermediate|advanced\",\n      \"questions\": [\n        {\n          \"id\": \"string\",\n          \"text\": \"string\",\n          \"type\": \"mcq|true_false|short_answer\",\n          \"options\": [\"string\"],\n          \"correct_answer\": \"string|number|array\",\n          \"explanation\": \"string\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"ressources/json-schemas/#description-des-champs-du-quiz","title":"Description des champs du quiz","text":"Champ Type Description Exemple <code>quizzes</code> array Liste des quiz disponibles <code>id</code> string Identifiant unique du quiz \"cnn_basics\" <code>topic</code> string Sujet principal du quiz \"R\u00e9seaux convolutifs\" <code>difficulty</code> enum Niveau de difficult\u00e9 \"intermediate\" <code>questions</code> array Liste des questions <code>text</code> string \u00c9nonc\u00e9 de la question \"Quelle est la principale caract\u00e9ristique...\" <code>type</code> enum Type de question \"mcq\" (choix multiple) <code>options</code> array Options pour les QCM [\"Pooling\", \"Convolution\", \"ReLU\"] <code>correct_answer</code> mixed R\u00e9ponse(s) correcte(s) 1 ou [0, 2] <code>explanation</code> string Explication de la r\u00e9ponse \"La convolution est...\""},{"location":"ressources/json-schemas/#schema-pour-lhistorique-des-conversations","title":"Sch\u00e9ma pour l'historique des conversations","text":"<p>Pour g\u00e9rer l'historique des conversations, le chatbot utilise le format suivant:</p> <pre><code>{\n  \"conversations\": [\n    {\n      \"id\": \"string\",\n      \"user_id\": \"string\",\n      \"timestamp_start\": \"string (ISO date)\",\n      \"timestamp_last_activity\": \"string (ISO date)\",\n      \"messages\": [\n        {\n          \"role\": \"system|user|assistant\",\n          \"content\": \"string\",\n          \"timestamp\": \"string (ISO date)\"\n        }\n      ],\n      \"context\": {\n        \"user_level\": \"beginner|intermediate|advanced\",\n        \"topics_covered\": [\"string\"],\n        \"last_quiz_score\": number,\n        \"session_metrics\": {\n          \"questions_asked\": number,\n          \"topics_explored\": number,\n          \"quizzes_completed\": number\n        }\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"ressources/json-schemas/#description-des-champs-de-conversation","title":"Description des champs de conversation","text":"Champ Type Description Exemple <code>conversations</code> array Liste des conversations <code>id</code> string Identifiant unique de la conversation \"conv_123456\" <code>user_id</code> string Identifiant de l'utilisateur \"user_789\" <code>timestamp_start</code> string Date de d\u00e9but de conversation \"2023-06-15T14:23:45Z\" <code>messages</code> array Liste des messages \u00e9chang\u00e9s <code>role</code> enum R\u00f4le de l'exp\u00e9diteur du message \"user\" <code>content</code> string Contenu du message \"Qu'est-ce qu'un CNN?\" <code>context</code> object Informations contextuelles <code>user_level</code> enum Niveau estim\u00e9 de l'utilisateur \"beginner\" <code>topics_covered</code> array Sujets abord\u00e9s dans la conversation [\"cnn\", \"pooling\"] <code>session_metrics</code> object M\u00e9triques de la session"},{"location":"ressources/json-schemas/#utilisation-des-schemas-dans-lapplication","title":"Utilisation des sch\u00e9mas dans l'application","text":""},{"location":"ressources/json-schemas/#chargement-de-la-base-de-connaissances","title":"Chargement de la base de connaissances","text":"<pre><code>import json\n\ndef load_knowledge_base(file_path=\"knowledge_base.json\"):\n    \"\"\"\n    Charge la base de connaissances depuis un fichier JSON.\n\n    Args:\n        file_path (str): Chemin vers le fichier JSON\n\n    Returns:\n        dict: Base de connaissances\n    \"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            knowledge_base = json.load(f)\n        return knowledge_base\n    except Exception as e:\n        print(f\"Erreur lors du chargement de la base de connaissances: {e}\")\n        return {\"concepts\": []}\n</code></pre>"},{"location":"ressources/json-schemas/#recherche-dans-la-base-de-connaissances","title":"Recherche dans la base de connaissances","text":"<pre><code>def find_concept(knowledge_base, concept_id):\n    \"\"\"\n    Trouve un concept ou sous-concept par son ID.\n\n    Args:\n        knowledge_base (dict): Base de connaissances\n        concept_id (str): ID du concept \u00e0 trouver\n\n    Returns:\n        dict: Concept trouv\u00e9 ou None\n    \"\"\"\n    # Recherche dans les concepts principaux\n    for concept in knowledge_base.get(\"concepts\", []):\n        if concept[\"id\"] == concept_id:\n            return concept\n\n        # Recherche dans les sous-concepts\n        for subconcept in concept.get(\"subconcepts\", []):\n            if subconcept[\"id\"] == concept_id:\n                return subconcept\n\n    return None\n</code></pre>"},{"location":"ressources/json-schemas/#enrichissement-du-prompt-avec-la-base-de-connaissances","title":"Enrichissement du prompt avec la base de connaissances","text":"<pre><code>def enrich_prompt_with_knowledge(user_message, knowledge_base, user_level=\"beginner\"):\n    \"\"\"\n    Enrichit le prompt utilisateur avec des informations pertinentes\n    de la base de connaissances.\n\n    Args:\n        user_message (str): Message de l'utilisateur\n        knowledge_base (dict): Base de connaissances\n        user_level (str): Niveau de l'utilisateur\n\n    Returns:\n        str: Prompt enrichi\n    \"\"\"\n    # Rechercher des mots-cl\u00e9s dans le message\n    relevant_concepts = []\n\n    for concept in knowledge_base.get(\"concepts\", []):\n        # V\u00e9rifier si le concept principal est mentionn\u00e9\n        if concept[\"title\"].lower() in user_message.lower():\n            relevant_concepts.append(concept)\n\n        # V\u00e9rifier les sous-concepts\n        for subconcept in concept.get(\"subconcepts\", []):\n            if subconcept[\"title\"].lower() in user_message.lower():\n                relevant_concepts.append(subconcept)\n\n    # Construire le prompt enrichi\n    if not relevant_concepts:\n        return user_message\n\n    enriched_prompt = f\"Question de l'utilisateur: {user_message}\\n\\n\"\n    enriched_prompt += \"Informations pertinentes de la base de connaissances:\\n\\n\"\n\n    for concept in relevant_concepts[:2]:  # Limiter \u00e0 2 concepts pour \u00e9viter un prompt trop long\n        enriched_prompt += f\"Concept: {concept['title']}\\n\"\n\n        if \"definition\" in concept:\n            enriched_prompt += f\"D\u00e9finition: {concept['definition']}\\n\"\n\n        if \"details\" in concept and user_level in concept[\"details\"]:\n            enriched_prompt += f\"Explication ({user_level}): {concept['details'][user_level]}\\n\"\n\n        if \"examples\" in concept and concept[\"examples\"]:\n            examples = concept[\"examples\"][:2]  # Limiter \u00e0 2 exemples\n            enriched_prompt += f\"Exemples: {', '.join(examples)}\\n\"\n\n        enriched_prompt += \"\\n\"\n\n    enriched_prompt += f\"R\u00e9ponds \u00e0 la question de l'utilisateur de mani\u00e8re conversationnelle en utilisant ces informations, adapt\u00e9 au niveau {user_level}.\"\n\n    return enriched_prompt\n</code></pre>"},{"location":"ressources/json-schemas/#validation-des-donnees","title":"Validation des donn\u00e9es","text":"<p>Pour assurer l'int\u00e9grit\u00e9 des donn\u00e9es, un script de validation peut \u00eatre utilis\u00e9:</p> <pre><code>import jsonschema\n\n# D\u00e9finition du sch\u00e9ma pour validation\nknowledge_base_schema = {\n    \"type\": \"object\",\n    \"required\": [\"concepts\"],\n    \"properties\": {\n        \"concepts\": {\n            \"type\": \"array\",\n            \"items\": {\n                \"type\": \"object\",\n                \"required\": [\"id\", \"title\", \"subconcepts\"],\n                \"properties\": {\n                    \"id\": {\"type\": \"string\"},\n                    \"title\": {\"type\": \"string\"},\n                    \"description\": {\"type\": \"string\"},\n                    \"subconcepts\": {\n                        \"type\": \"array\",\n                        \"items\": {\n                            \"type\": \"object\",\n                            \"required\": [\"id\", \"title\", \"definition\"],\n                            \"properties\": {\n                                \"id\": {\"type\": \"string\"},\n                                \"title\": {\"type\": \"string\"},\n                                \"definition\": {\"type\": \"string\"},\n                                \"details\": {\n                                    \"type\": \"object\",\n                                    \"properties\": {\n                                        \"beginner\": {\"type\": \"string\"},\n                                        \"intermediate\": {\"type\": \"string\"},\n                                        \"advanced\": {\"type\": \"string\"}\n                                    }\n                                },\n                                \"examples\": {\n                                    \"type\": \"array\",\n                                    \"items\": {\"type\": \"string\"}\n                                },\n                                \"related\": {\n                                    \"type\": \"array\",\n                                    \"items\": {\"type\": \"string\"}\n                                }\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n\ndef validate_knowledge_base(knowledge_base, schema=knowledge_base_schema):\n    \"\"\"\n    Valide la structure de la base de connaissances.\n\n    Args:\n        knowledge_base (dict): Base de connaissances \u00e0 valider\n        schema (dict): Sch\u00e9ma JSON de validation\n\n    Returns:\n        bool: True si valide, False sinon\n    \"\"\"\n    try:\n        jsonschema.validate(instance=knowledge_base, schema=schema)\n        return True\n    except jsonschema.exceptions.ValidationError as e:\n        print(f\"Erreur de validation: {e}\")\n        return False\n</code></pre>"},{"location":"ressources/json-schemas/#bonnes-pratiques-pour-lextension-de-la-base-de-connaissances","title":"Bonnes pratiques pour l'extension de la base de connaissances","text":"<ol> <li>Maintenir la coh\u00e9rence des IDs en utilisant le format snake_case</li> <li>\u00c9viter les duplications de concepts</li> <li>Cr\u00e9er des liens bidirectionnels entre concepts li\u00e9s</li> <li>Adapter les explications aux diff\u00e9rents niveaux</li> <li>Inclure des exemples concrets pour chaque concept</li> <li>Valider le fichier JSON apr\u00e8s chaque modification</li> <li>Versionner la base de connaissances pour suivre l'\u00e9volution</li> <li>Structurer hi\u00e9rarchiquement les concepts pour une navigation logique</li> <li>Limiter la profondeur de la hi\u00e9rarchie pour faciliter la navigation</li> <li>Documenter les changements dans un journal des modifications</li> </ol> <p>Ces sch\u00e9mas JSON constituent la structure fondamentale de la base de connaissances du chatbot p\u00e9dagogique, assurant une organisation coh\u00e9rente des informations et facilitant leur utilisation par l'application.</p>"},{"location":"ressources/code/api-integration/","title":"Api integration","text":"<p>Int\u00e9gration de l'API Mistral - Premier test BTS SIO  - S\u00e9ance 2: Types de r\u00e9seaux et applications</p> In\u00a0[\u00a0]: Copied! <pre>import requests\nimport json\nimport os\nfrom dotenv import load_dotenv\nfrom flask import Flask, request, jsonify, render_template\n</pre> import requests import json import os from dotenv import load_dotenv from flask import Flask, request, jsonify, render_template In\u00a0[\u00a0]: Copied! <pre># Charger les variables d'environnement\nload_dotenv()\n</pre> # Charger les variables d'environnement load_dotenv() In\u00a0[\u00a0]: Copied! <pre># Configuration de l'API Mistral\nMISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\", \"votre_cl\u00e9_api\")  # \u00c0 remplacer par votre cl\u00e9 API\nMISTRAL_API_URL = \"https://api.mistral.ai/v1/chat/completions\"\n</pre> # Configuration de l'API Mistral MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\", \"votre_cl\u00e9_api\")  # \u00c0 remplacer par votre cl\u00e9 API MISTRAL_API_URL = \"https://api.mistral.ai/v1/chat/completions\" In\u00a0[\u00a0]: Copied! <pre># 1. Fonction simple pour appeler l'API Mistral\ndef mistral_chat_completion(prompt, model=\"mistral-tiny\", max_tokens=1000):\n    \"\"\"\n    Appelle l'API Mistral pour g\u00e9n\u00e9rer une r\u00e9ponse \u00e0 partir d'un prompt.\n    \n    Args:\n        prompt (str): Le message \u00e0 envoyer \u00e0 l'API\n        model (str): Le mod\u00e8le \u00e0 utiliser (mistral-tiny, mistral-small, mistral-medium, etc.)\n        max_tokens (int): Nombre maximum de tokens pour la r\u00e9ponse\n        \n    Returns:\n        dict: La r\u00e9ponse de l'API\n    \"\"\"\n    headers = {\n        \"Authorization\": f\"Bearer {MISTRAL_API_KEY}\",\n        \"Content-Type\": \"application/json\"\n    }\n    \n    data = {\n        \"model\": model,\n        \"messages\": [\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        \"max_tokens\": max_tokens\n    }\n    \n    try:\n        response = requests.post(MISTRAL_API_URL, headers=headers, data=json.dumps(data))\n        response.raise_for_status()  # Lever une exception si la requ\u00eate \u00e9choue\n        return response.json()\n    except requests.exceptions.RequestException as e:\n        print(f\"Erreur lors de l'appel \u00e0 l'API Mistral: {e}\")\n        return {\"error\": str(e)}\n</pre> # 1. Fonction simple pour appeler l'API Mistral def mistral_chat_completion(prompt, model=\"mistral-tiny\", max_tokens=1000):     \"\"\"     Appelle l'API Mistral pour g\u00e9n\u00e9rer une r\u00e9ponse \u00e0 partir d'un prompt.          Args:         prompt (str): Le message \u00e0 envoyer \u00e0 l'API         model (str): Le mod\u00e8le \u00e0 utiliser (mistral-tiny, mistral-small, mistral-medium, etc.)         max_tokens (int): Nombre maximum de tokens pour la r\u00e9ponse              Returns:         dict: La r\u00e9ponse de l'API     \"\"\"     headers = {         \"Authorization\": f\"Bearer {MISTRAL_API_KEY}\",         \"Content-Type\": \"application/json\"     }          data = {         \"model\": model,         \"messages\": [             {\"role\": \"user\", \"content\": prompt}         ],         \"max_tokens\": max_tokens     }          try:         response = requests.post(MISTRAL_API_URL, headers=headers, data=json.dumps(data))         response.raise_for_status()  # Lever une exception si la requ\u00eate \u00e9choue         return response.json()     except requests.exceptions.RequestException as e:         print(f\"Erreur lors de l'appel \u00e0 l'API Mistral: {e}\")         return {\"error\": str(e)} In\u00a0[\u00a0]: Copied! <pre># 2. Test simple de l'API\ndef test_mistral_api():\n    \"\"\"\n    Teste l'API Mistral avec un prompt simple.\n    \"\"\"\n    prompt = \"Explique-moi ce qu'est le Deep Learning en 3 phrases simples.\"\n    \n    print(f\"Envoi du prompt \u00e0 Mistral: '{prompt}'\")\n    response = mistral_chat_completion(prompt)\n    \n    if \"error\" in response:\n        print(f\"Erreur: {response['error']}\")\n        return\n    \n    # Extraire et afficher la r\u00e9ponse\n    try:\n        message_content = response[\"choices\"][0][\"message\"][\"content\"]\n        print(\"\\nR\u00e9ponse de Mistral:\")\n        print(message_content)\n        \n        # Informations suppl\u00e9mentaires sur la r\u00e9ponse\n        if \"usage\" in response:\n            usage = response[\"usage\"]\n            print(\"\\nUtilisation de tokens:\")\n            print(f\"- Prompt: {usage.get('prompt_tokens', 'N/A')} tokens\")\n            print(f\"- R\u00e9ponse: {usage.get('completion_tokens', 'N/A')} tokens\")\n            print(f\"- Total: {usage.get('total_tokens', 'N/A')} tokens\")\n    except (KeyError, IndexError) as e:\n        print(f\"Erreur lors du traitement de la r\u00e9ponse: {e}\")\n        print(\"R\u00e9ponse brute:\", response)\n</pre> # 2. Test simple de l'API def test_mistral_api():     \"\"\"     Teste l'API Mistral avec un prompt simple.     \"\"\"     prompt = \"Explique-moi ce qu'est le Deep Learning en 3 phrases simples.\"          print(f\"Envoi du prompt \u00e0 Mistral: '{prompt}'\")     response = mistral_chat_completion(prompt)          if \"error\" in response:         print(f\"Erreur: {response['error']}\")         return          # Extraire et afficher la r\u00e9ponse     try:         message_content = response[\"choices\"][0][\"message\"][\"content\"]         print(\"\\nR\u00e9ponse de Mistral:\")         print(message_content)                  # Informations suppl\u00e9mentaires sur la r\u00e9ponse         if \"usage\" in response:             usage = response[\"usage\"]             print(\"\\nUtilisation de tokens:\")             print(f\"- Prompt: {usage.get('prompt_tokens', 'N/A')} tokens\")             print(f\"- R\u00e9ponse: {usage.get('completion_tokens', 'N/A')} tokens\")             print(f\"- Total: {usage.get('total_tokens', 'N/A')} tokens\")     except (KeyError, IndexError) as e:         print(f\"Erreur lors du traitement de la r\u00e9ponse: {e}\")         print(\"R\u00e9ponse brute:\", response) In\u00a0[\u00a0]: Copied! <pre># 3. Fonction avanc\u00e9e pour l'explication de concepts de Deep Learning\ndef explain_deep_learning_concept(concept, difficulty=\"d\u00e9butant\"):\n    \"\"\"\n    Demande \u00e0 l'API Mistral d'expliquer un concept de Deep Learning.\n    \n    Args:\n        concept (str): Le concept \u00e0 expliquer\n        difficulty (str): Le niveau de difficult\u00e9 (d\u00e9butant, interm\u00e9diaire, avanc\u00e9)\n        \n    Returns:\n        str: L'explication g\u00e9n\u00e9r\u00e9e\n    \"\"\"\n    # Construire un prompt \u00e9ducatif structur\u00e9\n    prompt = f\"\"\"\n    En tant que tuteur p\u00e9dagogique sp\u00e9cialis\u00e9 en Deep Learning, explique le concept de '{concept}' \n    \u00e0 un \u00e9tudiant de BTS SIO  (niveau {difficulty}).\n    \n    Ton explication doit inclure:\n    1. Une d\u00e9finition simple et claire\n    2. Un exemple concret d'application\n    3. Comment ce concept est utilis\u00e9 dans le d\u00e9veloppement d'applications\n    \n    Utilise un langage accessible mais techniquement pr\u00e9cis.\n    \"\"\"\n    \n    response = mistral_chat_completion(prompt, model=\"mistral-small\")\n    \n    if \"error\" in response:\n        return f\"Erreur: {response['error']}\"\n    \n    try:\n        return response[\"choices\"][0][\"message\"][\"content\"]\n    except (KeyError, IndexError):\n        return \"Erreur lors de la r\u00e9cup\u00e9ration de la r\u00e9ponse.\"\n</pre> # 3. Fonction avanc\u00e9e pour l'explication de concepts de Deep Learning def explain_deep_learning_concept(concept, difficulty=\"d\u00e9butant\"):     \"\"\"     Demande \u00e0 l'API Mistral d'expliquer un concept de Deep Learning.          Args:         concept (str): Le concept \u00e0 expliquer         difficulty (str): Le niveau de difficult\u00e9 (d\u00e9butant, interm\u00e9diaire, avanc\u00e9)              Returns:         str: L'explication g\u00e9n\u00e9r\u00e9e     \"\"\"     # Construire un prompt \u00e9ducatif structur\u00e9     prompt = f\"\"\"     En tant que tuteur p\u00e9dagogique sp\u00e9cialis\u00e9 en Deep Learning, explique le concept de '{concept}'      \u00e0 un \u00e9tudiant de BTS SIO  (niveau {difficulty}).          Ton explication doit inclure:     1. Une d\u00e9finition simple et claire     2. Un exemple concret d'application     3. Comment ce concept est utilis\u00e9 dans le d\u00e9veloppement d'applications          Utilise un langage accessible mais techniquement pr\u00e9cis.     \"\"\"          response = mistral_chat_completion(prompt, model=\"mistral-small\")          if \"error\" in response:         return f\"Erreur: {response['error']}\"          try:         return response[\"choices\"][0][\"message\"][\"content\"]     except (KeyError, IndexError):         return \"Erreur lors de la r\u00e9cup\u00e9ration de la r\u00e9ponse.\" In\u00a0[\u00a0]: Copied! <pre># 4. Cr\u00e9ation d'une petite application Flask pour interagir avec l'API\napp = Flask(__name__)\n</pre> # 4. Cr\u00e9ation d'une petite application Flask pour interagir avec l'API app = Flask(__name__) In\u00a0[\u00a0]: Copied! <pre>@app.route('/')\ndef home():\n    return render_template('index.html')\n</pre> @app.route('/') def home():     return render_template('index.html') In\u00a0[\u00a0]: Copied! <pre>@app.route('/api/explain', methods=['POST'])\ndef api_explain():\n    data = request.json\n    concept = data.get('concept', '')\n    difficulty = data.get('difficulty', 'd\u00e9butant')\n    \n    if not concept:\n        return jsonify({\"error\": \"Concept manquant\"}), 400\n    \n    explanation = explain_deep_learning_concept(concept, difficulty)\n    return jsonify({\"explanation\": explanation})\n</pre> @app.route('/api/explain', methods=['POST']) def api_explain():     data = request.json     concept = data.get('concept', '')     difficulty = data.get('difficulty', 'd\u00e9butant')          if not concept:         return jsonify({\"error\": \"Concept manquant\"}), 400          explanation = explain_deep_learning_concept(concept, difficulty)     return jsonify({\"explanation\": explanation}) In\u00a0[\u00a0]: Copied! <pre># 5. Template HTML simple pour l'interface\ndef create_template_directory():\n    \"\"\"Cr\u00e9e un r\u00e9pertoire templates et un fichier index.html\"\"\"\n    os.makedirs('templates', exist_ok=True)\n    \n    with open('templates/index.html', 'w') as f:\n        f.write(\"\"\"\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"fr\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;Explorateur de concepts Deep Learning&lt;/title&gt;\n    &lt;style&gt;\n        body {\n            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n            line-height: 1.6;\n            max-width: 800px;\n            margin: 0 auto;\n            padding: 20px;\n            background-color: #f5f8fa;\n            color: #333;\n        }\n        h1 {\n            color: #2c3e50;\n            border-bottom: 2px solid #3498db;\n            padding-bottom: 10px;\n        }\n        .container {\n            background-color: white;\n            border-radius: 8px;\n            padding: 20px;\n            box-shadow: 0 2px 10px rgba(0,0,0,0.1);\n        }\n        label {\n            display: block;\n            margin-top: 15px;\n            font-weight: bold;\n            color: #2c3e50;\n        }\n        input, select, button {\n            width: 100%;\n            padding: 10px;\n            margin-top: 5px;\n            border: 1px solid #ddd;\n            border-radius: 4px;\n            box-sizing: border-box;\n        }\n        button {\n            background-color: #3498db;\n            color: white;\n            border: none;\n            padding: 12px;\n            margin-top: 20px;\n            cursor: pointer;\n            font-weight: bold;\n            transition: background-color 0.3s;\n        }\n        button:hover {\n            background-color: #2980b9;\n        }\n        #result {\n            margin-top: 20px;\n            padding: 20px;\n            background-color: #f8f9fa;\n            border-left: 4px solid #3498db;\n            border-radius: 4px;\n            white-space: pre-wrap;\n        }\n        .loading {\n            text-align: center;\n            margin-top: 20px;\n            display: none;\n        }\n        .hint {\n            font-size: 0.8em;\n            color: #7f8c8d;\n            margin-top: 5px;\n        }\n    &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;div class=\"container\"&gt;\n        &lt;h1&gt;Explorateur de concepts Deep Learning&lt;/h1&gt;\n        &lt;p&gt;Utilisez cet outil pour explorer et comprendre les concepts cl\u00e9s du Deep Learning, expliqu\u00e9s par l'IA.&lt;/p&gt;\n        \n        &lt;form id=\"explainForm\"&gt;\n            &lt;label for=\"concept\"&gt;Concept \u00e0 expliquer:&lt;/label&gt;\n            &lt;input type=\"text\" id=\"concept\" required placeholder=\"Ex: r\u00e9seaux de neurones convolutifs, LSTM, dropout...\"&gt;\n            &lt;div class=\"hint\"&gt;Essayez des concepts comme: convolution, pooling, fonction d'activation, r\u00e9tropropagation...&lt;/div&gt;\n            \n            &lt;label for=\"difficulty\"&gt;Niveau de difficult\u00e9:&lt;/label&gt;\n            &lt;select id=\"difficulty\"&gt;\n                &lt;option value=\"d\u00e9butant\"&gt;D\u00e9butant&lt;/option&gt;\n                &lt;option value=\"interm\u00e9diaire\"&gt;Interm\u00e9diaire&lt;/option&gt;\n                &lt;option value=\"avanc\u00e9\"&gt;Avanc\u00e9&lt;/option&gt;\n            &lt;/select&gt;\n            \n            &lt;button type=\"submit\"&gt;Expliquer&lt;/button&gt;\n        &lt;/form&gt;\n        \n        &lt;div class=\"loading\" id=\"loading\"&gt;\n            &lt;p&gt;Chargement de l'explication...&lt;/p&gt;\n        &lt;/div&gt;\n        \n        &lt;div id=\"result\"&gt;&lt;/div&gt;\n    &lt;/div&gt;\n    \n    &lt;script&gt;\n        document.getElementById('explainForm').addEventListener('submit', async function(e) {\n            e.preventDefault();\n            \n            const concept = document.getElementById('concept').value.trim();\n            const difficulty = document.getElementById('difficulty').value;\n            const resultDiv = document.getElementById('result');\n            const loadingDiv = document.getElementById('loading');\n            \n            if (!concept) {\n                resultDiv.innerHTML = \"Veuillez entrer un concept \u00e0 expliquer.\";\n                return;\n            }\n            \n            // Afficher l'indicateur de chargement\n            loadingDiv.style.display = 'block';\n            resultDiv.innerHTML = \"\";\n            \n            try {\n                const response = await fetch('/api/explain', {\n                    method: 'POST',\n                    headers: {\n                        'Content-Type': 'application/json'\n                    },\n                    body: JSON.stringify({ concept, difficulty })\n                });\n                \n                const data = await response.json();\n                \n                if (data.error) {\n                    resultDiv.innerHTML = `&lt;p style=\"color: red\"&gt;Erreur: ${data.error}&lt;/p&gt;`;\n                } else {\n                    resultDiv.innerHTML = data.explanation.replace(/\\\\n/g, '&lt;br&gt;');\n                }\n            } catch (error) {\n                resultDiv.innerHTML = `&lt;p style=\"color: red\"&gt;Erreur: ${error.message}&lt;/p&gt;`;\n            } finally {\n                // Cacher l'indicateur de chargement\n                loadingDiv.style.display = 'none';\n            }\n        });\n    &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n        \"\"\")\n</pre> # 5. Template HTML simple pour l'interface def create_template_directory():     \"\"\"Cr\u00e9e un r\u00e9pertoire templates et un fichier index.html\"\"\"     os.makedirs('templates', exist_ok=True)          with open('templates/index.html', 'w') as f:         f.write(\"\"\"  Explorateur de concepts Deep Learning Explorateur de concepts Deep Learning <p>Utilisez cet outil pour explorer et comprendre les concepts cl\u00e9s du Deep Learning, expliqu\u00e9s par l'IA.</p> Concept \u00e0 expliquer: Essayez des concepts comme: convolution, pooling, fonction d'activation, r\u00e9tropropagation... Niveau de difficult\u00e9: D\u00e9butant Interm\u00e9diaire Avanc\u00e9 Expliquer <p>Chargement de l'explication...</p>          \"\"\") In\u00a0[\u00a0]: Copied! <pre># 6. Fonction principale pour ex\u00e9cuter l'application\ndef main():\n    \"\"\"Fonction principale\"\"\"\n    print(\"=== EXPLORATION DE L'API MISTRAL POUR LE CHATBOT P\u00c9DAGOGIQUE ===\")\n    \n    # Tester si la cl\u00e9 API est configur\u00e9e\n    if MISTRAL_API_KEY == \"votre_cl\u00e9_api\":\n        print(\"\\nERREUR: Vous devez configurer votre cl\u00e9 API Mistral!\")\n        print(\"1. Cr\u00e9ez un fichier .env dans le m\u00eame r\u00e9pertoire que ce script\")\n        print(\"2. Ajoutez la ligne: MISTRAL_API_KEY=votre_cl\u00e9_api_r\u00e9elle\")\n        print(\"3. Relancez le script\")\n        return\n    \n    # Test simple de l'API\n    print(\"\\n1. Test simple de l'API Mistral\")\n    test_mistral_api()\n    \n    # Cr\u00e9ation du r\u00e9pertoire et du fichier template\n    print(\"\\n2. Cr\u00e9ation du template pour l'application web\")\n    create_template_directory()\n    print(\"   Template cr\u00e9\u00e9 dans le r\u00e9pertoire 'templates/'\")\n    \n    # Lancement de l'application Flask\n    print(\"\\n3. D\u00e9marrage de l'application web\")\n    print(\"   URL: http://localhost:5000\")\n    print(\"   Appuyez sur Ctrl+C pour quitter\")\n    app.run(debug=True)\n</pre> # 6. Fonction principale pour ex\u00e9cuter l'application def main():     \"\"\"Fonction principale\"\"\"     print(\"=== EXPLORATION DE L'API MISTRAL POUR LE CHATBOT P\u00c9DAGOGIQUE ===\")          # Tester si la cl\u00e9 API est configur\u00e9e     if MISTRAL_API_KEY == \"votre_cl\u00e9_api\":         print(\"\\nERREUR: Vous devez configurer votre cl\u00e9 API Mistral!\")         print(\"1. Cr\u00e9ez un fichier .env dans le m\u00eame r\u00e9pertoire que ce script\")         print(\"2. Ajoutez la ligne: MISTRAL_API_KEY=votre_cl\u00e9_api_r\u00e9elle\")         print(\"3. Relancez le script\")         return          # Test simple de l'API     print(\"\\n1. Test simple de l'API Mistral\")     test_mistral_api()          # Cr\u00e9ation du r\u00e9pertoire et du fichier template     print(\"\\n2. Cr\u00e9ation du template pour l'application web\")     create_template_directory()     print(\"   Template cr\u00e9\u00e9 dans le r\u00e9pertoire 'templates/'\")          # Lancement de l'application Flask     print(\"\\n3. D\u00e9marrage de l'application web\")     print(\"   URL: http://localhost:5000\")     print(\"   Appuyez sur Ctrl+C pour quitter\")     app.run(debug=True) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    main()\n</pre> if __name__ == \"__main__\":     main()"},{"location":"ressources/notebooks/cnn-classification/","title":"Cnn classification","text":"In\u00a0[\u00a0]: Copied! <pre>{\n  \"cells\": [\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"# CNN pour la classification d'images - MNIST\\n\",\n        \"\\n\",\n        \"## BTS SIO  - S\u00e9ance 2: Types de r\u00e9seaux de neurones\\n\",\n        \"\\n\",\n        \"Ce notebook vous guidera \u00e0 travers l'impl\u00e9mentation et l'utilisation d'un r\u00e9seau de neurones convolutif (CNN) pour la classification d'images, en utilisant le c\u00e9l\u00e8bre dataset MNIST des chiffres manuscrits.\\n\",\n        \"\\n\",\n        \"### Objectifs d'apprentissage:\\n\",\n        \"- Comprendre l'architecture d'un r\u00e9seau convolutif (CNN)\\n\",\n        \"- Impl\u00e9menter un CNN avec TensorFlow/Keras\\n\",\n        \"- Visualiser les filtres et feature maps\\n\",\n        \"- Analyser les performances du mod\u00e8le\\n\",\n        \"\\n\",\n        \"### Pr\u00e9requis:\\n\",\n        \"- Connaissances de base en Python\\n\",\n        \"- Avoir suivi la s\u00e9ance 1 d'introduction au Deep Learning\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 1. Configuration de l'environnement\\n\",\n        \"\\n\",\n        \"Commen\u00e7ons par importer les biblioth\u00e8ques n\u00e9cessaires.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"import numpy as np\\n\",\n        \"import matplotlib.pyplot as plt\\n\",\n        \"import tensorflow as tf\\n\",\n        \"from tensorflow.keras.models import Sequential\\n\",\n        \"from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\\n\",\n        \"from tensorflow.keras.utils import to_categorical\\n\",\n        \"from tensorflow.keras.datasets import mnist\\n\",\n        \"import time\\n\",\n        \"import seaborn as sns\\n\",\n        \"from sklearn.metrics import confusion_matrix\\n\",\n        \"\\n\",\n        \"# Configuration pour reproductibilit\u00e9\\n\",\n        \"np.random.seed(42)\\n\",\n        \"tf.random.set_seed(42)\\n\",\n        \"\\n\",\n        \"# V\u00e9rifier la version de TensorFlow\\n\",\n        \"print(f\\\"TensorFlow version: {tf.__version__}\\\")\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 2. Chargement et pr\u00e9paration du dataset MNIST\\n\",\n        \"\\n\",\n        \"Le dataset MNIST contient 70,000 images de chiffres manuscrits de taille 28x28 pixels.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"print(\\\"Chargement des donn\u00e9es MNIST...\\\")\\n\",\n        \"(X_train, y_train), (X_test, y_test) = mnist.load_data()\\n\",\n        \"\\n\",\n        \"# Afficher les dimensions des donn\u00e9es\\n\",\n        \"print(f\\\"Dimensions de X_train: {X_train.shape}\\\")\\n\",\n        \"print(f\\\"Dimensions de y_train: {y_train.shape}\\\")\\n\",\n        \"print(f\\\"Dimensions de X_test: {X_test.shape}\\\")\\n\",\n        \"print(f\\\"Dimensions de y_test: {y_test.shape}\\\")\\n\",\n        \"print(f\\\"Nombre de classes: {len(np.unique(y_train))}\\\")\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### Pr\u00e9paration des donn\u00e9es pour le CNN\\n\",\n        \"\\n\",\n        \"Pour utiliser nos images avec un CNN, nous devons :\\n\",\n        \"1. Ajouter une dimension pour le canal (les images sont en niveaux de gris, donc 1 seul canal)\\n\",\n        \"2. Normaliser les valeurs de pixels entre 0 et 1\\n\",\n        \"3. Convertir les \u00e9tiquettes en format one-hot encoding\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# Redimensionnement et normalisation\\n\",\n        \"X_train = X_train.reshape(-1, 28, 28, 1) / 255.0\\n\",\n        \"X_test = X_test.reshape(-1, 28, 28, 1) / 255.0\\n\",\n        \"\\n\",\n        \"# Conversion des \u00e9tiquettes en cat\u00e9gories one-hot\\n\",\n        \"y_train_onehot = to_categorical(y_train, 10)\\n\",\n        \"y_test_onehot = to_categorical(y_test, 10)\\n\",\n        \"\\n\",\n        \"print(f\\\"Nouvelle forme de X_train: {X_train.shape}\\\")\\n\",\n        \"print(f\\\"Nouvelle forme de y_train_onehot: {y_train_onehot.shape}\\\")\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### Visualisation de quelques exemples\\n\",\n        \"\\n\",\n        \"Regardons \u00e0 quoi ressemblent nos donn\u00e9es.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"plt.figure(figsize=(10, 5))\\n\",\n        \"for i in range(10):\\n\",\n        \"    plt.subplot(2, 5, i+1)\\n\",\n        \"    plt.imshow(X_train[i].reshape(28, 28), cmap='gray')\\n\",\n        \"    plt.title(f\\\"Chiffre: {y_train[i]}\\\")\\n\",\n        \"    plt.axis('off')\\n\",\n        \"plt.tight_layout()\\n\",\n        \"plt.show()\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 3. Cr\u00e9ation d'un mod\u00e8le CNN\\n\",\n        \"\\n\",\n        \"Un CNN est un type de r\u00e9seau de neurones sp\u00e9cialis\u00e9 pour traiter des donn\u00e9es ayant une structure en grille, comme les images. Les principales couches sont :\\n\",\n        \"\\n\",\n        \"1. **Couches de convolution (Conv2D)** : D\u00e9tectent des caract\u00e9ristiques locales (lignes, formes...)\\n\",\n        \"2. **Couches de pooling (MaxPooling2D)** : R\u00e9duisent la dimension des donn\u00e9es\\n\",\n        \"3. **Couches denses (Dense)** : Effectuent la classification finale\\n\",\n        \"\\n\",\n        \"Nous allons cr\u00e9er un CNN simple avec 2 couches de convolution pour classifier les chiffres MNIST.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# Cr\u00e9er un mod\u00e8le CNN\\n\",\n        \"model = Sequential([\\n\",\n        \"    # Premi\u00e8re couche de convolution\\n\",\n        \"    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), name='conv1'),\\n\",\n        \"    MaxPooling2D((2, 2), name='pool1'),\\n\",\n        \"    \\n\",\n        \"    # Deuxi\u00e8me couche de convolution\\n\",\n        \"    Conv2D(64, (3, 3), activation='relu', name='conv2'),\\n\",\n        \"    MaxPooling2D((2, 2), name='pool2'),\\n\",\n        \"    \\n\",\n        \"    # Aplatissement pour passer aux couches denses\\n\",\n        \"    Flatten(name='flatten'),\\n\",\n        \"    \\n\",\n        \"    # Couches denses (fully connected)\\n\",\n        \"    Dense(128, activation='relu', name='dense1'),\\n\",\n        \"    Dropout(0.5, name='dropout1'),  # \u00c9vite le surapprentissage\\n\",\n        \"    Dense(10, activation='softmax', name='output')  # 10 classes (chiffres 0-9)\\n\",\n        \"])\\n\",\n        \"\\n\",\n        \"# Compiler le mod\u00e8le\\n\",\n        \"model.compile(\\n\",\n        \"    optimizer='adam',\\n\",\n        \"    loss='categorical_crossentropy',\\n\",\n        \"    metrics=['accuracy']\\n\",\n        \")\\n\",\n        \"\\n\",\n        \"# Afficher le r\u00e9sum\u00e9 de l'architecture\\n\",\n        \"model.summary()\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 4. Entra\u00eenement du mod\u00e8le\\n\",\n        \"\\n\",\n        \"Entra\u00eenons maintenant notre CNN sur les donn\u00e9es MNIST.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# Entra\u00eenement du mod\u00e8le\\n\",\n        \"start_time = time.time()\\n\",\n        \"\\n\",\n        \"history = model.fit(\\n\",\n        \"    X_train, \\n\",\n        \"    y_train_onehot, \\n\",\n        \"    batch_size=128, \\n\",\n        \"    epochs=5,  # Nombre r\u00e9duit d'\u00e9poques pour la d\u00e9monstration\\n\",\n        \"    validation_split=0.2,  # 20% des donn\u00e9es d'entra\u00eenement pour la validation\\n\",\n        \"    verbose=1\\n\",\n        \")\\n\",\n        \"\\n\",\n        \"training_time = time.time() - start_time\\n\",\n        \"print(f\\\"\\\\nTemps d'entra\u00eenement: {training_time:.2f} secondes\\\")\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### Visualisation de l'\u00e9volution de l'entra\u00eenement\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"plt.figure(figsize=(12, 4))\\n\",\n        \"\\n\",\n        \"# Graphique de pr\u00e9cision\\n\",\n        \"plt.subplot(1, 2, 1)\\n\",\n        \"plt.plot(history.history['accuracy'], label='Entra\u00eenement')\\n\",\n        \"plt.plot(history.history['val_accuracy'], label='Validation')\\n\",\n        \"plt.title('\u00c9volution de la pr\u00e9cision')\\n\",\n        \"plt.xlabel('\u00c9poque')\\n\",\n        \"plt.ylabel('Pr\u00e9cision')\\n\",\n        \"plt.legend()\\n\",\n        \"plt.grid(True, linestyle='--', alpha=0.6)\\n\",\n        \"\\n\",\n        \"# Graphique de perte\\n\",\n        \"plt.subplot(1, 2, 2)\\n\",\n        \"plt.plot(history.history['loss'], label='Entra\u00eenement')\\n\",\n        \"plt.plot(history.history['val_loss'], label='Validation')\\n\",\n        \"plt.title('\u00c9volution de la perte')\\n\",\n        \"plt.xlabel('\u00c9poque')\\n\",\n        \"plt.ylabel('Perte')\\n\",\n        \"plt.legend()\\n\",\n        \"plt.grid(True, linestyle='--', alpha=0.6)\\n\",\n        \"\\n\",\n        \"plt.tight_layout()\\n\",\n        \"plt.show()\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 5. \u00c9valuation du mod\u00e8le\\n\",\n        \"\\n\",\n        \"\u00c9valuons notre mod\u00e8le sur l'ensemble de test.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# \u00c9valuation sur l'ensemble de test\\n\",\n        \"test_loss, test_acc = model.evaluate(X_test, y_test_onehot, verbose=1)\\n\",\n        \"print(f\\\"Pr\u00e9cision sur l'ensemble de test: {test_acc*100:.2f}%\\\")\\n\",\n        \"\\n\",\n        \"# Pr\u00e9dictions\\n\",\n        \"y_pred = model.predict(X_test)\\n\",\n        \"y_pred_classes = np.argmax(y_pred, axis=1)\\n\",\n        \"\\n\",\n        \"# Matrice de confusion\\n\",\n        \"conf_mat = confusion_matrix(y_test, y_pred_classes)\\n\",\n        \"plt.figure(figsize=(10, 8))\\n\",\n        \"sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues')\\n\",\n        \"plt.xlabel('Pr\u00e9dit')\\n\",\n        \"plt.ylabel('R\u00e9el')\\n\",\n        \"plt.title('Matrice de confusion')\\n\",\n        \"plt.show()\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### Visualisation des exemples mal classifi\u00e9s\\n\",\n        \"\\n\",\n        \"Explorons quelques exemples que notre mod\u00e8le a mal classifi\u00e9s.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# Identifier les erreurs\\n\",\n        \"misclassified_indices = np.where(y_pred_classes != y_test)[0]\\n\",\n        \"misclassified_count = len(misclassified_indices)\\n\",\n        \"print(f\\\"Nombre total d'erreurs: {misclassified_count} sur {len(y_test)} images de test\\\")\\n\",\n        \"\\n\",\n        \"# Afficher quelques exemples mal classifi\u00e9s\\n\",\n        \"num_examples = min(10, misclassified_count)\\n\",\n        \"plt.figure(figsize=(15, 6))\\n\",\n        \"\\n\",\n        \"for i, idx in enumerate(misclassified_indices[:num_examples]):\\n\",\n        \"    plt.subplot(2, 5, i+1)\\n\",\n        \"    plt.imshow(X_test[idx].reshape(28, 28), cmap='gray')\\n\",\n        \"    plt.title(f\\\"R\u00e9el: {y_test[idx]}\\\\nPr\u00e9dit: {y_pred_classes[idx]}\\\")\\n\",\n        \"    plt.axis('off')\\n\",\n        \"    \\n\",\n        \"plt.tight_layout()\\n\",\n        \"plt.show()\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### \ud83e\udde0 R\u00e9flexion sur les erreurs\\n\",\n        \"\\n\",\n        \"**Question**: En observant les exemples mal classifi\u00e9s, quelles pourraient \u00eatre les raisons de ces erreurs? Notez vos observations et hypoth\u00e8ses ci-dessous.\\n\",\n        \"\\n\",\n        \"**Points \u00e0 consid\u00e9rer:**\\n\",\n        \"- Certains chiffres sont-ils plus souvent confondus que d'autres?\\n\",\n        \"- Quelles caract\u00e9ristiques visuelles communes peuvent expliquer les erreurs?\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"*\u00c9crivez vos observations ici...*\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 6. Visualisation des filtres et feature maps\\n\",\n        \"\\n\",\n        \"Une des grandes forces des CNNs est leur interpr\u00e9tabilit\u00e9 visuelle. Explorons ce que le r\u00e9seau \\\"voit\\\" r\u00e9ellement.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# Fonction pour visualiser les filtres de convolution\\n\",\n        \"def visualize_filters(model, layer_name, num_filters=8):\\n\",\n        \"    \\\"\\\"\\\"Visualise les filtres d'une couche de convolution\\\"\\\"\\\"\\n\",\n        \"    \\n\",\n        \"    # R\u00e9cup\u00e9rer les poids du filtre de la couche sp\u00e9cifi\u00e9e\\n\",\n        \"    filters, biases = model.get_layer(layer_name).get_weights()\\n\",\n        \"    \\n\",\n        \"    # Normaliser les filtres pour une meilleure visualisation\\n\",\n        \"    f_min, f_max = filters.min(), filters.max()\\n\",\n        \"    filters = (filters - f_min) / (f_max - f_min)\\n\",\n        \"    \\n\",\n        \"    # Afficher les premiers filtres\\n\",\n        \"    plt.figure(figsize=(12, 4))\\n\",\n        \"    for i in range(num_filters):\\n\",\n        \"        plt.subplot(2, 4, i+1)\\n\",\n        \"        # Pour la premi\u00e8re couche de convolution, les filtres sont 3D (hauteur, largeur, canaux)\\n\",\n        \"        # Nous affichons le filtre pour le premier canal (0)\\n\",\n        \"        plt.imshow(filters[:, :, 0, i], cmap='viridis')\\n\",\n        \"        plt.title(f'Filtre {i+1}')\\n\",\n        \"        plt.axis('off')\\n\",\n        \"    plt.suptitle(f'Filtres de la couche {layer_name}')\\n\",\n        \"    plt.tight_layout()\\n\",\n        \"    plt.show()\\n\",\n        \"\\n\",\n        \"# Visualiser les filtres de la premi\u00e8re couche de convolution\\n\",\n        \"visualize_filters(model, 'conv1')\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### Visualisation des feature maps (cartes d'activation)\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"def visualize_feature_maps(model, image, layer_name, num_features=8):\\n\",\n        \"    \\\"\\\"\\\"Visualise les feature maps (activations) d'une couche pour une image donn\u00e9e\\\"\\\"\\\"\\n\",\n        \"    \\n\",\n        \"    # Cr\u00e9er un mod\u00e8le qui renvoie les activations de la couche sp\u00e9cifi\u00e9e\\n\",\n        \"    layer_model = tf.keras.Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\\n\",\n        \"    \\n\",\n        \"    # Obtenir les activations pour une image\\n\",\n        \"    feature_maps = layer_model.predict(image.reshape(1, 28, 28, 1))\\n\",\n        \"    \\n\",\n        \"    # Afficher les premi\u00e8res cartes d'activation\\n\",\n        \"    plt.figure(figsize=(12, 4))\\n\",\n        \"    for i in range(min(num_features, feature_maps.shape[3])):\\n\",\n        \"        plt.subplot(2, 4, i+1)\\n\",\n        \"        plt.imshow(feature_maps[0, :, :, i], cmap='viridis')\\n\",\n        \"        plt.title(f'Feature {i+1}')\\n\",\n        \"        plt.axis('off')\\n\",\n        \"    plt.suptitle(f'Feature Maps de la couche {layer_name}')\\n\",\n        \"    plt.tight_layout()\\n\",\n        \"    plt.show()\\n\",\n        \"\\n\",\n        \"# Choisir une image de test\\n\",\n        \"sample_idx = 12  # Vous pouvez essayer avec diff\u00e9rents indices\\n\",\n        \"sample_image = X_test[sample_idx]\\n\",\n        \"\\n\",\n        \"# Afficher l'image originale\\n\",\n        \"plt.figure(figsize=(3, 3))\\n\",\n        \"plt.imshow(sample_image.reshape(28, 28), cmap='gray')\\n\",\n        \"plt.title(f\\\"Chiffre: {y_test[sample_idx]}\\\")\\n\",\n        \"plt.axis('off')\\n\",\n        \"plt.show()\\n\",\n        \"\\n\",\n        \"# Visualiser les feature maps pour chaque couche de convolution\\n\",\n        \"print(\\\"Feature maps de la premi\u00e8re couche de convolution:\\\")\\n\",\n        \"visualize_feature_maps(model, sample_image, 'conv1')\\n\",\n        \"\\n\",\n        \"print(\\\"Feature maps de la deuxi\u00e8me couche de convolution:\\\")\\n\",\n        \"visualize_feature_maps(model, sample_image, 'conv2')\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### \ud83d\udca1 Interpr\u00e9tation des feature maps\\n\",\n        \"\\n\",\n        \"Les feature maps nous montrent ce que \\\"voit\\\" chaque filtre de convolution :\\n\",\n        \"\\n\",\n        \"- **Premi\u00e8re couche** : D\u00e9tecte principalement des caract\u00e9ristiques de base comme les bords et les contours\\n\",\n        \"- **Deuxi\u00e8me couche** : Combine ces caract\u00e9ristiques de base pour d\u00e9tecter des formes plus complexes\\n\",\n        \"\\n\",\n        \"Cette hi\u00e9rarchie de repr\u00e9sentations est ce qui rend les CNNs si puissants pour la vision par ordinateur.\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 7. Test avec des images bruit\u00e9es\\n\",\n        \"\\n\",\n        \"Testons la robustesse de notre mod\u00e8le face \u00e0 des perturbations.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# Fonction pour ajouter du bruit aux images\\n\",\n        \"def add_noise(images, noise_level=0.2):\\n\",\n        \"    \\\"\\\"\\\"Ajoute du bruit gaussien aux images\\\"\\\"\\\"\\n\",\n        \"    noisy_images = images.copy()\\n\",\n        \"    noise = np.random.normal(0, noise_level, images.shape)\\n\",\n        \"    noisy_images = noisy_images + noise\\n\",\n        \"    # Assurer que les valeurs restent entre 0 et 1\\n\",\n        \"    return np.clip(noisy_images, 0, 1)\\n\",\n        \"\\n\",\n        \"# Cr\u00e9er des versions bruit\u00e9es de quelques images de test\\n\",\n        \"num_test_images = 5\\n\",\n        \"test_samples = X_test[:num_test_images]\\n\",\n        \"noisy_samples = add_noise(test_samples, noise_level=0.3)\\n\",\n        \"\\n\",\n        \"# Visualiser les images originales et bruit\u00e9es\\n\",\n        \"plt.figure(figsize=(12, 4))\\n\",\n        \"for i in range(num_test_images):\\n\",\n        \"    # Image originale\\n\",\n        \"    plt.subplot(2, num_test_images, i+1)\\n\",\n        \"    plt.imshow(test_samples[i].reshape(28, 28), cmap='gray')\\n\",\n        \"    plt.title(f\\\"Original: {y_test[i]}\\\")\\n\",\n        \"    plt.axis('off')\\n\",\n        \"    \\n\",\n        \"    # Image bruit\u00e9e\\n\",\n        \"    plt.subplot(2, num_test_images, i+num_test_images+1)\\n\",\n        \"    plt.imshow(noisy_samples[i].reshape(28, 28), cmap='gray')\\n\",\n        \"    plt.axis('off')\\n\",\n        \"    \\n\",\n        \"plt.tight_layout()\\n\",\n        \"plt.show()\\n\",\n        \"\\n\",\n        \"# Pr\u00e9dire sur les images bruit\u00e9es\\n\",\n        \"noisy_predictions = model.predict(noisy_samples)\\n\",\n        \"noisy_pred_classes = np.argmax(noisy_predictions, axis=1)\\n\",\n        \"\\n\",\n        \"# Afficher les r\u00e9sultats\\n\",\n        \"print(\\\"R\u00e9sultats des pr\u00e9dictions sur les images bruit\u00e9es:\\\")\\n\",\n        \"for i in range(num_test_images):\\n\",\n        \"    status = \\\"\u2713\\\" if noisy_pred_classes[i] == y_test[i] else \\\"\u2717\\\"\\n\",\n        \"    print(f\\\"Image {i+1} - R\u00e9el: {y_test[i]}, Pr\u00e9dit: {noisy_pred_classes[i]} {status}\\\")\\n\",\n        \"\\n\",\n        \"# Calculer la pr\u00e9cision sur les images bruit\u00e9es\\n\",\n        \"accuracy_on_noisy = np.mean(noisy_pred_classes == y_test[:num_test_images]) * 100\\n\",\n        \"print(f\\\"\\\\nPr\u00e9cision sur les images bruit\u00e9es: {accuracy_on_noisy:.1f}%\\\")\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 8. Exercice : Am\u00e9lioration du mod\u00e8le\\n\",\n        \"\\n\",\n        \"\u00c0 vous de jouer ! Essayez de modifier l'architecture du mod\u00e8le pour am\u00e9liorer ses performances. Voici quelques suggestions :\\n\",\n        \"\\n\",\n        \"1. Ajouter plus de couches de convolution\\n\",\n        \"2. Modifier le nombre de filtres\\n\",\n        \"3. Changer la taille des filtres\\n\",\n        \"4. Ajuster les param\u00e8tres d'entra\u00eenement (epochs, batch_size)\\n\",\n        \"\\n\",\n        \"Copiez le code de cr\u00e9ation du mod\u00e8le ci-dessous et modifiez-le :\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# VOTRE CODE ICI\\n\",\n        \"# Cr\u00e9ez votre propre mod\u00e8le am\u00e9lior\u00e9\\n\",\n        \"\\n\",\n        \"improved_model = Sequential([\\n\",\n        \"    # Modifiez l'architecture ici\\n\",\n        \"    \\n\",\n        \"])\\n\",\n        \"\\n\",\n        \"# Compiler le mod\u00e8le\\n\",\n        \"improved_model.compile(\\n\",\n        \"    optimizer='adam',\\n\",\n        \"    loss='categorical_crossentropy',\\n\",\n        \"    metrics=['accuracy']\\n\",\n        \")\\n\",\n        \"\\n\",\n        \"# Afficher le r\u00e9sum\u00e9\\n\",\n        \"improved_model.summary()\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# Entra\u00eenez votre mod\u00e8le am\u00e9lior\u00e9\\n\",\n        \"# history = improved_model.fit(...)\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 9. Conclusion\\n\",\n        \"\\n\",\n        \"Dans ce notebook, vous avez :\\n\",\n        \"- Cr\u00e9\u00e9 et entra\u00een\u00e9 un r\u00e9seau de neurones convolutif (CNN) pour la classification d'images\\n\",\n        \"- Visualis\u00e9 les filtres et les feature maps pour comprendre ce que \\\"voit\\\" le r\u00e9seau\\n\",\n        \"- \u00c9valu\u00e9 les performances du mod\u00e8le et sa robustesse face au bruit\\n\",\n        \"\\n\",\n        \"Les CNN sont la base de nombreuses applications modernes de vision par ordinateur comme la reconnaissance faciale, la d\u00e9tection d'objets, et bien d'autres.\"\n      ]\n    }\n  ],\n  \"metadata\": {\n    \"kernelspec\": {\n      \"display_name\": \"Python 3\",\n      \"language\": \"python\",\n      \"name\": \"python3\"\n    },\n    \"language_info\": {\n      \"codemirror_mode\": {\n        \"name\": \"ipython\",\n        \"version\": 3\n      },\n      \"file_extension\": \".py\",\n      \"mimetype\": \"text/x-python\",\n      \"name\": \"python\",\n      \"nbconvert_exporter\": \"python\",\n      \"pygments_lexer\": \"ipython3\",\n      \"version\": \"3.8.5\"\n    }\n  },\n  \"nbformat\": 4,\n  \"nbformat_minor\": 4\n}\n</pre> {   \"cells\": [     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"# CNN pour la classification d'images - MNIST\\n\",         \"\\n\",         \"## BTS SIO  - S\u00e9ance 2: Types de r\u00e9seaux de neurones\\n\",         \"\\n\",         \"Ce notebook vous guidera \u00e0 travers l'impl\u00e9mentation et l'utilisation d'un r\u00e9seau de neurones convolutif (CNN) pour la classification d'images, en utilisant le c\u00e9l\u00e8bre dataset MNIST des chiffres manuscrits.\\n\",         \"\\n\",         \"### Objectifs d'apprentissage:\\n\",         \"- Comprendre l'architecture d'un r\u00e9seau convolutif (CNN)\\n\",         \"- Impl\u00e9menter un CNN avec TensorFlow/Keras\\n\",         \"- Visualiser les filtres et feature maps\\n\",         \"- Analyser les performances du mod\u00e8le\\n\",         \"\\n\",         \"### Pr\u00e9requis:\\n\",         \"- Connaissances de base en Python\\n\",         \"- Avoir suivi la s\u00e9ance 1 d'introduction au Deep Learning\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 1. Configuration de l'environnement\\n\",         \"\\n\",         \"Commen\u00e7ons par importer les biblioth\u00e8ques n\u00e9cessaires.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"import numpy as np\\n\",         \"import matplotlib.pyplot as plt\\n\",         \"import tensorflow as tf\\n\",         \"from tensorflow.keras.models import Sequential\\n\",         \"from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\\n\",         \"from tensorflow.keras.utils import to_categorical\\n\",         \"from tensorflow.keras.datasets import mnist\\n\",         \"import time\\n\",         \"import seaborn as sns\\n\",         \"from sklearn.metrics import confusion_matrix\\n\",         \"\\n\",         \"# Configuration pour reproductibilit\u00e9\\n\",         \"np.random.seed(42)\\n\",         \"tf.random.set_seed(42)\\n\",         \"\\n\",         \"# V\u00e9rifier la version de TensorFlow\\n\",         \"print(f\\\"TensorFlow version: {tf.__version__}\\\")\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 2. Chargement et pr\u00e9paration du dataset MNIST\\n\",         \"\\n\",         \"Le dataset MNIST contient 70,000 images de chiffres manuscrits de taille 28x28 pixels.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"print(\\\"Chargement des donn\u00e9es MNIST...\\\")\\n\",         \"(X_train, y_train), (X_test, y_test) = mnist.load_data()\\n\",         \"\\n\",         \"# Afficher les dimensions des donn\u00e9es\\n\",         \"print(f\\\"Dimensions de X_train: {X_train.shape}\\\")\\n\",         \"print(f\\\"Dimensions de y_train: {y_train.shape}\\\")\\n\",         \"print(f\\\"Dimensions de X_test: {X_test.shape}\\\")\\n\",         \"print(f\\\"Dimensions de y_test: {y_test.shape}\\\")\\n\",         \"print(f\\\"Nombre de classes: {len(np.unique(y_train))}\\\")\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### Pr\u00e9paration des donn\u00e9es pour le CNN\\n\",         \"\\n\",         \"Pour utiliser nos images avec un CNN, nous devons :\\n\",         \"1. Ajouter une dimension pour le canal (les images sont en niveaux de gris, donc 1 seul canal)\\n\",         \"2. Normaliser les valeurs de pixels entre 0 et 1\\n\",         \"3. Convertir les \u00e9tiquettes en format one-hot encoding\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# Redimensionnement et normalisation\\n\",         \"X_train = X_train.reshape(-1, 28, 28, 1) / 255.0\\n\",         \"X_test = X_test.reshape(-1, 28, 28, 1) / 255.0\\n\",         \"\\n\",         \"# Conversion des \u00e9tiquettes en cat\u00e9gories one-hot\\n\",         \"y_train_onehot = to_categorical(y_train, 10)\\n\",         \"y_test_onehot = to_categorical(y_test, 10)\\n\",         \"\\n\",         \"print(f\\\"Nouvelle forme de X_train: {X_train.shape}\\\")\\n\",         \"print(f\\\"Nouvelle forme de y_train_onehot: {y_train_onehot.shape}\\\")\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### Visualisation de quelques exemples\\n\",         \"\\n\",         \"Regardons \u00e0 quoi ressemblent nos donn\u00e9es.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"plt.figure(figsize=(10, 5))\\n\",         \"for i in range(10):\\n\",         \"    plt.subplot(2, 5, i+1)\\n\",         \"    plt.imshow(X_train[i].reshape(28, 28), cmap='gray')\\n\",         \"    plt.title(f\\\"Chiffre: {y_train[i]}\\\")\\n\",         \"    plt.axis('off')\\n\",         \"plt.tight_layout()\\n\",         \"plt.show()\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 3. Cr\u00e9ation d'un mod\u00e8le CNN\\n\",         \"\\n\",         \"Un CNN est un type de r\u00e9seau de neurones sp\u00e9cialis\u00e9 pour traiter des donn\u00e9es ayant une structure en grille, comme les images. Les principales couches sont :\\n\",         \"\\n\",         \"1. **Couches de convolution (Conv2D)** : D\u00e9tectent des caract\u00e9ristiques locales (lignes, formes...)\\n\",         \"2. **Couches de pooling (MaxPooling2D)** : R\u00e9duisent la dimension des donn\u00e9es\\n\",         \"3. **Couches denses (Dense)** : Effectuent la classification finale\\n\",         \"\\n\",         \"Nous allons cr\u00e9er un CNN simple avec 2 couches de convolution pour classifier les chiffres MNIST.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# Cr\u00e9er un mod\u00e8le CNN\\n\",         \"model = Sequential([\\n\",         \"    # Premi\u00e8re couche de convolution\\n\",         \"    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), name='conv1'),\\n\",         \"    MaxPooling2D((2, 2), name='pool1'),\\n\",         \"    \\n\",         \"    # Deuxi\u00e8me couche de convolution\\n\",         \"    Conv2D(64, (3, 3), activation='relu', name='conv2'),\\n\",         \"    MaxPooling2D((2, 2), name='pool2'),\\n\",         \"    \\n\",         \"    # Aplatissement pour passer aux couches denses\\n\",         \"    Flatten(name='flatten'),\\n\",         \"    \\n\",         \"    # Couches denses (fully connected)\\n\",         \"    Dense(128, activation='relu', name='dense1'),\\n\",         \"    Dropout(0.5, name='dropout1'),  # \u00c9vite le surapprentissage\\n\",         \"    Dense(10, activation='softmax', name='output')  # 10 classes (chiffres 0-9)\\n\",         \"])\\n\",         \"\\n\",         \"# Compiler le mod\u00e8le\\n\",         \"model.compile(\\n\",         \"    optimizer='adam',\\n\",         \"    loss='categorical_crossentropy',\\n\",         \"    metrics=['accuracy']\\n\",         \")\\n\",         \"\\n\",         \"# Afficher le r\u00e9sum\u00e9 de l'architecture\\n\",         \"model.summary()\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 4. Entra\u00eenement du mod\u00e8le\\n\",         \"\\n\",         \"Entra\u00eenons maintenant notre CNN sur les donn\u00e9es MNIST.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# Entra\u00eenement du mod\u00e8le\\n\",         \"start_time = time.time()\\n\",         \"\\n\",         \"history = model.fit(\\n\",         \"    X_train, \\n\",         \"    y_train_onehot, \\n\",         \"    batch_size=128, \\n\",         \"    epochs=5,  # Nombre r\u00e9duit d'\u00e9poques pour la d\u00e9monstration\\n\",         \"    validation_split=0.2,  # 20% des donn\u00e9es d'entra\u00eenement pour la validation\\n\",         \"    verbose=1\\n\",         \")\\n\",         \"\\n\",         \"training_time = time.time() - start_time\\n\",         \"print(f\\\"\\\\nTemps d'entra\u00eenement: {training_time:.2f} secondes\\\")\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### Visualisation de l'\u00e9volution de l'entra\u00eenement\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"plt.figure(figsize=(12, 4))\\n\",         \"\\n\",         \"# Graphique de pr\u00e9cision\\n\",         \"plt.subplot(1, 2, 1)\\n\",         \"plt.plot(history.history['accuracy'], label='Entra\u00eenement')\\n\",         \"plt.plot(history.history['val_accuracy'], label='Validation')\\n\",         \"plt.title('\u00c9volution de la pr\u00e9cision')\\n\",         \"plt.xlabel('\u00c9poque')\\n\",         \"plt.ylabel('Pr\u00e9cision')\\n\",         \"plt.legend()\\n\",         \"plt.grid(True, linestyle='--', alpha=0.6)\\n\",         \"\\n\",         \"# Graphique de perte\\n\",         \"plt.subplot(1, 2, 2)\\n\",         \"plt.plot(history.history['loss'], label='Entra\u00eenement')\\n\",         \"plt.plot(history.history['val_loss'], label='Validation')\\n\",         \"plt.title('\u00c9volution de la perte')\\n\",         \"plt.xlabel('\u00c9poque')\\n\",         \"plt.ylabel('Perte')\\n\",         \"plt.legend()\\n\",         \"plt.grid(True, linestyle='--', alpha=0.6)\\n\",         \"\\n\",         \"plt.tight_layout()\\n\",         \"plt.show()\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 5. \u00c9valuation du mod\u00e8le\\n\",         \"\\n\",         \"\u00c9valuons notre mod\u00e8le sur l'ensemble de test.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# \u00c9valuation sur l'ensemble de test\\n\",         \"test_loss, test_acc = model.evaluate(X_test, y_test_onehot, verbose=1)\\n\",         \"print(f\\\"Pr\u00e9cision sur l'ensemble de test: {test_acc*100:.2f}%\\\")\\n\",         \"\\n\",         \"# Pr\u00e9dictions\\n\",         \"y_pred = model.predict(X_test)\\n\",         \"y_pred_classes = np.argmax(y_pred, axis=1)\\n\",         \"\\n\",         \"# Matrice de confusion\\n\",         \"conf_mat = confusion_matrix(y_test, y_pred_classes)\\n\",         \"plt.figure(figsize=(10, 8))\\n\",         \"sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues')\\n\",         \"plt.xlabel('Pr\u00e9dit')\\n\",         \"plt.ylabel('R\u00e9el')\\n\",         \"plt.title('Matrice de confusion')\\n\",         \"plt.show()\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### Visualisation des exemples mal classifi\u00e9s\\n\",         \"\\n\",         \"Explorons quelques exemples que notre mod\u00e8le a mal classifi\u00e9s.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# Identifier les erreurs\\n\",         \"misclassified_indices = np.where(y_pred_classes != y_test)[0]\\n\",         \"misclassified_count = len(misclassified_indices)\\n\",         \"print(f\\\"Nombre total d'erreurs: {misclassified_count} sur {len(y_test)} images de test\\\")\\n\",         \"\\n\",         \"# Afficher quelques exemples mal classifi\u00e9s\\n\",         \"num_examples = min(10, misclassified_count)\\n\",         \"plt.figure(figsize=(15, 6))\\n\",         \"\\n\",         \"for i, idx in enumerate(misclassified_indices[:num_examples]):\\n\",         \"    plt.subplot(2, 5, i+1)\\n\",         \"    plt.imshow(X_test[idx].reshape(28, 28), cmap='gray')\\n\",         \"    plt.title(f\\\"R\u00e9el: {y_test[idx]}\\\\nPr\u00e9dit: {y_pred_classes[idx]}\\\")\\n\",         \"    plt.axis('off')\\n\",         \"    \\n\",         \"plt.tight_layout()\\n\",         \"plt.show()\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### \ud83e\udde0 R\u00e9flexion sur les erreurs\\n\",         \"\\n\",         \"**Question**: En observant les exemples mal classifi\u00e9s, quelles pourraient \u00eatre les raisons de ces erreurs? Notez vos observations et hypoth\u00e8ses ci-dessous.\\n\",         \"\\n\",         \"**Points \u00e0 consid\u00e9rer:**\\n\",         \"- Certains chiffres sont-ils plus souvent confondus que d'autres?\\n\",         \"- Quelles caract\u00e9ristiques visuelles communes peuvent expliquer les erreurs?\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"*\u00c9crivez vos observations ici...*\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 6. Visualisation des filtres et feature maps\\n\",         \"\\n\",         \"Une des grandes forces des CNNs est leur interpr\u00e9tabilit\u00e9 visuelle. Explorons ce que le r\u00e9seau \\\"voit\\\" r\u00e9ellement.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# Fonction pour visualiser les filtres de convolution\\n\",         \"def visualize_filters(model, layer_name, num_filters=8):\\n\",         \"    \\\"\\\"\\\"Visualise les filtres d'une couche de convolution\\\"\\\"\\\"\\n\",         \"    \\n\",         \"    # R\u00e9cup\u00e9rer les poids du filtre de la couche sp\u00e9cifi\u00e9e\\n\",         \"    filters, biases = model.get_layer(layer_name).get_weights()\\n\",         \"    \\n\",         \"    # Normaliser les filtres pour une meilleure visualisation\\n\",         \"    f_min, f_max = filters.min(), filters.max()\\n\",         \"    filters = (filters - f_min) / (f_max - f_min)\\n\",         \"    \\n\",         \"    # Afficher les premiers filtres\\n\",         \"    plt.figure(figsize=(12, 4))\\n\",         \"    for i in range(num_filters):\\n\",         \"        plt.subplot(2, 4, i+1)\\n\",         \"        # Pour la premi\u00e8re couche de convolution, les filtres sont 3D (hauteur, largeur, canaux)\\n\",         \"        # Nous affichons le filtre pour le premier canal (0)\\n\",         \"        plt.imshow(filters[:, :, 0, i], cmap='viridis')\\n\",         \"        plt.title(f'Filtre {i+1}')\\n\",         \"        plt.axis('off')\\n\",         \"    plt.suptitle(f'Filtres de la couche {layer_name}')\\n\",         \"    plt.tight_layout()\\n\",         \"    plt.show()\\n\",         \"\\n\",         \"# Visualiser les filtres de la premi\u00e8re couche de convolution\\n\",         \"visualize_filters(model, 'conv1')\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### Visualisation des feature maps (cartes d'activation)\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"def visualize_feature_maps(model, image, layer_name, num_features=8):\\n\",         \"    \\\"\\\"\\\"Visualise les feature maps (activations) d'une couche pour une image donn\u00e9e\\\"\\\"\\\"\\n\",         \"    \\n\",         \"    # Cr\u00e9er un mod\u00e8le qui renvoie les activations de la couche sp\u00e9cifi\u00e9e\\n\",         \"    layer_model = tf.keras.Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\\n\",         \"    \\n\",         \"    # Obtenir les activations pour une image\\n\",         \"    feature_maps = layer_model.predict(image.reshape(1, 28, 28, 1))\\n\",         \"    \\n\",         \"    # Afficher les premi\u00e8res cartes d'activation\\n\",         \"    plt.figure(figsize=(12, 4))\\n\",         \"    for i in range(min(num_features, feature_maps.shape[3])):\\n\",         \"        plt.subplot(2, 4, i+1)\\n\",         \"        plt.imshow(feature_maps[0, :, :, i], cmap='viridis')\\n\",         \"        plt.title(f'Feature {i+1}')\\n\",         \"        plt.axis('off')\\n\",         \"    plt.suptitle(f'Feature Maps de la couche {layer_name}')\\n\",         \"    plt.tight_layout()\\n\",         \"    plt.show()\\n\",         \"\\n\",         \"# Choisir une image de test\\n\",         \"sample_idx = 12  # Vous pouvez essayer avec diff\u00e9rents indices\\n\",         \"sample_image = X_test[sample_idx]\\n\",         \"\\n\",         \"# Afficher l'image originale\\n\",         \"plt.figure(figsize=(3, 3))\\n\",         \"plt.imshow(sample_image.reshape(28, 28), cmap='gray')\\n\",         \"plt.title(f\\\"Chiffre: {y_test[sample_idx]}\\\")\\n\",         \"plt.axis('off')\\n\",         \"plt.show()\\n\",         \"\\n\",         \"# Visualiser les feature maps pour chaque couche de convolution\\n\",         \"print(\\\"Feature maps de la premi\u00e8re couche de convolution:\\\")\\n\",         \"visualize_feature_maps(model, sample_image, 'conv1')\\n\",         \"\\n\",         \"print(\\\"Feature maps de la deuxi\u00e8me couche de convolution:\\\")\\n\",         \"visualize_feature_maps(model, sample_image, 'conv2')\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### \ud83d\udca1 Interpr\u00e9tation des feature maps\\n\",         \"\\n\",         \"Les feature maps nous montrent ce que \\\"voit\\\" chaque filtre de convolution :\\n\",         \"\\n\",         \"- **Premi\u00e8re couche** : D\u00e9tecte principalement des caract\u00e9ristiques de base comme les bords et les contours\\n\",         \"- **Deuxi\u00e8me couche** : Combine ces caract\u00e9ristiques de base pour d\u00e9tecter des formes plus complexes\\n\",         \"\\n\",         \"Cette hi\u00e9rarchie de repr\u00e9sentations est ce qui rend les CNNs si puissants pour la vision par ordinateur.\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 7. Test avec des images bruit\u00e9es\\n\",         \"\\n\",         \"Testons la robustesse de notre mod\u00e8le face \u00e0 des perturbations.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# Fonction pour ajouter du bruit aux images\\n\",         \"def add_noise(images, noise_level=0.2):\\n\",         \"    \\\"\\\"\\\"Ajoute du bruit gaussien aux images\\\"\\\"\\\"\\n\",         \"    noisy_images = images.copy()\\n\",         \"    noise = np.random.normal(0, noise_level, images.shape)\\n\",         \"    noisy_images = noisy_images + noise\\n\",         \"    # Assurer que les valeurs restent entre 0 et 1\\n\",         \"    return np.clip(noisy_images, 0, 1)\\n\",         \"\\n\",         \"# Cr\u00e9er des versions bruit\u00e9es de quelques images de test\\n\",         \"num_test_images = 5\\n\",         \"test_samples = X_test[:num_test_images]\\n\",         \"noisy_samples = add_noise(test_samples, noise_level=0.3)\\n\",         \"\\n\",         \"# Visualiser les images originales et bruit\u00e9es\\n\",         \"plt.figure(figsize=(12, 4))\\n\",         \"for i in range(num_test_images):\\n\",         \"    # Image originale\\n\",         \"    plt.subplot(2, num_test_images, i+1)\\n\",         \"    plt.imshow(test_samples[i].reshape(28, 28), cmap='gray')\\n\",         \"    plt.title(f\\\"Original: {y_test[i]}\\\")\\n\",         \"    plt.axis('off')\\n\",         \"    \\n\",         \"    # Image bruit\u00e9e\\n\",         \"    plt.subplot(2, num_test_images, i+num_test_images+1)\\n\",         \"    plt.imshow(noisy_samples[i].reshape(28, 28), cmap='gray')\\n\",         \"    plt.axis('off')\\n\",         \"    \\n\",         \"plt.tight_layout()\\n\",         \"plt.show()\\n\",         \"\\n\",         \"# Pr\u00e9dire sur les images bruit\u00e9es\\n\",         \"noisy_predictions = model.predict(noisy_samples)\\n\",         \"noisy_pred_classes = np.argmax(noisy_predictions, axis=1)\\n\",         \"\\n\",         \"# Afficher les r\u00e9sultats\\n\",         \"print(\\\"R\u00e9sultats des pr\u00e9dictions sur les images bruit\u00e9es:\\\")\\n\",         \"for i in range(num_test_images):\\n\",         \"    status = \\\"\u2713\\\" if noisy_pred_classes[i] == y_test[i] else \\\"\u2717\\\"\\n\",         \"    print(f\\\"Image {i+1} - R\u00e9el: {y_test[i]}, Pr\u00e9dit: {noisy_pred_classes[i]} {status}\\\")\\n\",         \"\\n\",         \"# Calculer la pr\u00e9cision sur les images bruit\u00e9es\\n\",         \"accuracy_on_noisy = np.mean(noisy_pred_classes == y_test[:num_test_images]) * 100\\n\",         \"print(f\\\"\\\\nPr\u00e9cision sur les images bruit\u00e9es: {accuracy_on_noisy:.1f}%\\\")\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 8. Exercice : Am\u00e9lioration du mod\u00e8le\\n\",         \"\\n\",         \"\u00c0 vous de jouer ! Essayez de modifier l'architecture du mod\u00e8le pour am\u00e9liorer ses performances. Voici quelques suggestions :\\n\",         \"\\n\",         \"1. Ajouter plus de couches de convolution\\n\",         \"2. Modifier le nombre de filtres\\n\",         \"3. Changer la taille des filtres\\n\",         \"4. Ajuster les param\u00e8tres d'entra\u00eenement (epochs, batch_size)\\n\",         \"\\n\",         \"Copiez le code de cr\u00e9ation du mod\u00e8le ci-dessous et modifiez-le :\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# VOTRE CODE ICI\\n\",         \"# Cr\u00e9ez votre propre mod\u00e8le am\u00e9lior\u00e9\\n\",         \"\\n\",         \"improved_model = Sequential([\\n\",         \"    # Modifiez l'architecture ici\\n\",         \"    \\n\",         \"])\\n\",         \"\\n\",         \"# Compiler le mod\u00e8le\\n\",         \"improved_model.compile(\\n\",         \"    optimizer='adam',\\n\",         \"    loss='categorical_crossentropy',\\n\",         \"    metrics=['accuracy']\\n\",         \")\\n\",         \"\\n\",         \"# Afficher le r\u00e9sum\u00e9\\n\",         \"improved_model.summary()\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# Entra\u00eenez votre mod\u00e8le am\u00e9lior\u00e9\\n\",         \"# history = improved_model.fit(...)\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 9. Conclusion\\n\",         \"\\n\",         \"Dans ce notebook, vous avez :\\n\",         \"- Cr\u00e9\u00e9 et entra\u00een\u00e9 un r\u00e9seau de neurones convolutif (CNN) pour la classification d'images\\n\",         \"- Visualis\u00e9 les filtres et les feature maps pour comprendre ce que \\\"voit\\\" le r\u00e9seau\\n\",         \"- \u00c9valu\u00e9 les performances du mod\u00e8le et sa robustesse face au bruit\\n\",         \"\\n\",         \"Les CNN sont la base de nombreuses applications modernes de vision par ordinateur comme la reconnaissance faciale, la d\u00e9tection d'objets, et bien d'autres.\"       ]     }   ],   \"metadata\": {     \"kernelspec\": {       \"display_name\": \"Python 3\",       \"language\": \"python\",       \"name\": \"python3\"     },     \"language_info\": {       \"codemirror_mode\": {         \"name\": \"ipython\",         \"version\": 3       },       \"file_extension\": \".py\",       \"mimetype\": \"text/x-python\",       \"name\": \"python\",       \"nbconvert_exporter\": \"python\",       \"pygments_lexer\": \"ipython3\",       \"version\": \"3.8.5\"     }   },   \"nbformat\": 4,   \"nbformat_minor\": 4 }"},{"location":"ressources/notebooks/hello-world-dl/","title":"Hello world dl","text":"In\u00a0[2]: Copied! <pre>{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# \ud83d\ude80 Hello World du Deep Learning\\n\",\n    \"\\n\",\n    \"## Reconnaissance de chiffres manuscrits avec TensorFlow et Keras\\n\",\n    \"\\n\",\n    \"### Objectifs de ce notebook\\n\",\n    \"\\n\",\n    \"- Charger et pr\u00e9parer un jeu de donn\u00e9es de chiffres manuscrits\\n\",\n    \"- Cr\u00e9er un r\u00e9seau de neurones simple\\n\",\n    \"- Entra\u00eener le mod\u00e8le\\n\",\n    \"- Visualiser les r\u00e9sultats\\n\",\n    \"- Tester le mod\u00e8le avec vos propres dessins\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"source\": [\n    \"# Importation des biblioth\u00e8ques n\u00e9cessaires\\n\",\n    \"import numpy as np\\n\",\n    \"import matplotlib.pyplot as plt\\n\",\n    \"import tensorflow as tf\\n\",\n    \"from tensorflow import keras\\n\",\n    \"from tensorflow.keras import layers\\n\",\n    \"\\n\",\n    \"# V\u00e9rification de la version de TensorFlow\\n\",\n    \"print(f\\\"TensorFlow version: {tf.__version__}\\\")\\n\",\n    \"print(f\\\"Keras version: {keras.__version__}\\\")\\n\",\n    \"\\n\",\n    \"# V\u00e9rification du GPU (si disponible)\\n\",\n    \"print(\\\"GPU disponible :\\\", tf.test.is_gpu_available())\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"source\": [\n    \"# Chargement du dataset MNIST\\n\",\n    \"(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\\n\",\n    \"\\n\",\n    \"# Pr\u00e9traitement des donn\u00e9es\\n\",\n    \"X_train = X_train.reshape((60000, 28, 28, 1)) / 255.0\\n\",\n    \"X_test = X_test.reshape((10000, 28, 28, 1)) / 255.0\\n\",\n    \"\\n\",\n    \"# Conversion des labels en cat\u00e9gories\\n\",\n    \"y_train = keras.utils.to_categorical(y_train)\\n\",\n    \"y_test = keras.utils.to_categorical(y_test)\\n\",\n    \"\\n\",\n    \"# Affichage de quelques exemples\\n\",\n    \"plt.figure(figsize=(10, 2))\\n\",\n    \"for i in range(10):\\n\",\n    \"    plt.subplot(1, 10, i+1)\\n\",\n    \"    plt.imshow(X_train[i].reshape(28, 28), cmap='gray')\\n\",\n    \"    plt.axis('off')\\n\",\n    \"plt.suptitle(\\\"Exemples de chiffres manuscrits\\\")\\n\",\n    \"plt.show()\\n\",\n    \"\\n\",\n    \"print(f\\\"Nombre d'exemples d'entra\u00eenement : {X_train.shape[0]}\\\")\\n\",\n    \"print(f\\\"Nombre d'exemples de test : {X_test.shape[0]}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"source\": [\n    \"# Cr\u00e9ation du mod\u00e8le de r\u00e9seau de neurones\\n\",\n    \"model = keras.Sequential([\\n\",\n    \"    # Couche de convolution\\n\",\n    \"    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\\n\",\n    \"    layers.MaxPooling2D((2, 2)),\\n\",\n    \"    \\n\",\n    \"    # Couche de convolution suppl\u00e9mentaire\\n\",\n    \"    layers.Conv2D(64, (3, 3), activation='relu'),\\n\",\n    \"    layers.MaxPooling2D((2, 2)),\\n\",\n    \"    \\n\",\n    \"    # Aplatissement\\n\",\n    \"    layers.Flatten(),\\n\",\n    \"    \\n\",\n    \"    # Couche dense\\n\",\n    \"    layers.Dense(64, activation='relu'),\\n\",\n    \"    \\n\",\n    \"    # Couche de sortie\\n\",\n    \"    layers.Dense(10, activation='softmax')\\n\",\n    \"])\\n\",\n    \"\\n\",\n    \"# Compilation du mod\u00e8le\\n\",\n    \"model.compile(\\n\",\n    \"    optimizer='adam',\\n\",\n    \"    loss='categorical_crossentropy',\\n\",\n    \"    metrics=['accuracy']\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"# Affichage du r\u00e9sum\u00e9 du mod\u00e8le\\n\",\n    \"model.summary()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\\n\",\n   \"execution_count\": null,\\n\",\n   \"metadata\": {},\\n\",\n   \"source\": [\n    \"# Entra\u00eenement du mod\u00e8le\\n\",\n    \"# Note : Nombre d'\u00e9poques r\u00e9duit pour la d\u00e9monstration\\n\",\n    \"history = model.fit(\\n\",\n    \"    X_train, y_train,\\n\",\n    \"    epochs=5,\\n\",\n    \"    batch_size=64,\\n\",\n    \"    validation_split=0.2,\\n\",\n    \"    verbose=1\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"# \u00c9valuation du mod\u00e8le\\n\",\n    \"test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\\n\",\n    \"print(f\\\"\\\\nPr\u00e9cision sur l'ensemble de test : {test_accuracy*100:.2f}%\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\\n\",\n   \"execution_count\": null,\\n\",\n   \"metadata\": {},\\n\",\n   \"source\": [\n    \"# Visualisation de la pr\u00e9cision et de la perte\\n\",\n    \"plt.figure(figsize=(12, 4))\\n\",\n    \"\\n\",\n    \"# Pr\u00e9cision\\n\",\n    \"plt.subplot(1, 2, 1)\\n\",\n    \"plt.plot(history.history['accuracy'], label='Pr\u00e9cision entra\u00eenement')\\n\",\n    \"plt.plot(history.history['val_accuracy'], label='Pr\u00e9cision validation')\\n\",\n    \"plt.title('Pr\u00e9cision du mod\u00e8le')\\n\",\n    \"plt.xlabel('\u00c9poque')\\n\",\n    \"plt.ylabel('Pr\u00e9cision')\\n\",\n    \"plt.legend()\\n\",\n    \"\\n\",\n    \"# Perte\\n\",\n    \"plt.subplot(1, 2, 2)\\n\",\n    \"plt.plot(history.history['loss'], label='Perte entra\u00eenement')\\n\",\n    \"plt.plot(history.history['val_loss'], label='Perte validation')\\n\",\n    \"plt.title('Perte du mod\u00e8le')\\n\",\n    \"plt.xlabel('\u00c9poque')\\n\",\n    \"plt.ylabel('Perte')\\n\",\n    \"plt.legend()\\n\",\n    \"\\n\",\n    \"plt.tight_layout()\\n\",\n    \"plt.show()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\\n\",\n   \"execution_count\": null,\\n\",\n   \"metadata\": {},\\n\",\n   \"source\": [\n    \"# Pr\u00e9dictions et visualisation\\n\",\n    \"# Pr\u00e9dire sur quelques images de test\\n\",\n    \"predictions = model.predict(X_test[:10])\\n\",\n    \"\\n\",\n    \"plt.figure(figsize=(15, 6))\\n\",\n    \"for i in range(10):\\n\",\n    \"    plt.subplot(2, 10, i+1)\\n\",\n    \"    plt.imshow(X_test[i].reshape(28, 28), cmap='gray')\\n\",\n    \"    plt.axis('off')\\n\",\n    \"    \\n\",\n    \"    plt.subplot(2, 10, i+11)\\n\",\n    \"    plt.bar(range(10), predictions[i])\\n\",\n    \"    plt.title(f\\\"Pr\u00e9diction: {np.argmax(predictions[i])}\\\")\\n\",\n    \"    plt.xticks(range(10))\\n\",\n    \"    plt.ylim(0, 1)\\n\",\n    \"\\n\",\n    \"plt.suptitle(\\\"Pr\u00e9dictions du mod\u00e8le\\\")\\n\",\n    \"plt.tight_layout()\\n\",\n    \"plt.show()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\\n\",\n   \"metadata\": {},\\n\",\n   \"source\": [\n    \"## \ud83e\udd14 Questions de r\u00e9flexion\\n\",\n    \"\\n\",\n    \"1. Que se passe-t-il si vous augmentez le nombre d'\u00e9poques ?\\n\",\n    \"2. Comment changeriez-vous l'architecture du r\u00e9seau pour am\u00e9liorer les performances ?\\n\",\n    \"3. Quelles diff\u00e9rences observez-vous entre la pr\u00e9cision d'entra\u00eenement et de validation ?\\n\",\n    \"\\n\",\n    \"## \ud83d\ude80 D\u00e9fis\\n\",\n    \"\\n\",\n    \"- Essayez de modifier le nombre de neurones dans les couches denses\\n\",\n    \"- Changez la fonction d'activation dans certaines couches\\n\",\n    \"- Ajoutez une couche de dropout pour r\u00e9duire le surapprentissage\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"name\": \"python\",\n   \"version\": \"3.8.0\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 2\n}\n</pre> {  \"cells\": [   {    \"cell_type\": \"markdown\",    \"metadata\": {},    \"source\": [     \"# \ud83d\ude80 Hello World du Deep Learning\\n\",     \"\\n\",     \"## Reconnaissance de chiffres manuscrits avec TensorFlow et Keras\\n\",     \"\\n\",     \"### Objectifs de ce notebook\\n\",     \"\\n\",     \"- Charger et pr\u00e9parer un jeu de donn\u00e9es de chiffres manuscrits\\n\",     \"- Cr\u00e9er un r\u00e9seau de neurones simple\\n\",     \"- Entra\u00eener le mod\u00e8le\\n\",     \"- Visualiser les r\u00e9sultats\\n\",     \"- Tester le mod\u00e8le avec vos propres dessins\"    ]   },   {    \"cell_type\": \"code\",    \"execution_count\": null,    \"metadata\": {},    \"source\": [     \"# Importation des biblioth\u00e8ques n\u00e9cessaires\\n\",     \"import numpy as np\\n\",     \"import matplotlib.pyplot as plt\\n\",     \"import tensorflow as tf\\n\",     \"from tensorflow import keras\\n\",     \"from tensorflow.keras import layers\\n\",     \"\\n\",     \"# V\u00e9rification de la version de TensorFlow\\n\",     \"print(f\\\"TensorFlow version: {tf.__version__}\\\")\\n\",     \"print(f\\\"Keras version: {keras.__version__}\\\")\\n\",     \"\\n\",     \"# V\u00e9rification du GPU (si disponible)\\n\",     \"print(\\\"GPU disponible :\\\", tf.test.is_gpu_available())\"    ]   },   {    \"cell_type\": \"code\",    \"execution_count\": null,    \"metadata\": {},    \"source\": [     \"# Chargement du dataset MNIST\\n\",     \"(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\\n\",     \"\\n\",     \"# Pr\u00e9traitement des donn\u00e9es\\n\",     \"X_train = X_train.reshape((60000, 28, 28, 1)) / 255.0\\n\",     \"X_test = X_test.reshape((10000, 28, 28, 1)) / 255.0\\n\",     \"\\n\",     \"# Conversion des labels en cat\u00e9gories\\n\",     \"y_train = keras.utils.to_categorical(y_train)\\n\",     \"y_test = keras.utils.to_categorical(y_test)\\n\",     \"\\n\",     \"# Affichage de quelques exemples\\n\",     \"plt.figure(figsize=(10, 2))\\n\",     \"for i in range(10):\\n\",     \"    plt.subplot(1, 10, i+1)\\n\",     \"    plt.imshow(X_train[i].reshape(28, 28), cmap='gray')\\n\",     \"    plt.axis('off')\\n\",     \"plt.suptitle(\\\"Exemples de chiffres manuscrits\\\")\\n\",     \"plt.show()\\n\",     \"\\n\",     \"print(f\\\"Nombre d'exemples d'entra\u00eenement : {X_train.shape[0]}\\\")\\n\",     \"print(f\\\"Nombre d'exemples de test : {X_test.shape[0]}\\\")\"    ]   },   {    \"cell_type\": \"code\",    \"execution_count\": null,    \"metadata\": {},    \"source\": [     \"# Cr\u00e9ation du mod\u00e8le de r\u00e9seau de neurones\\n\",     \"model = keras.Sequential([\\n\",     \"    # Couche de convolution\\n\",     \"    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\\n\",     \"    layers.MaxPooling2D((2, 2)),\\n\",     \"    \\n\",     \"    # Couche de convolution suppl\u00e9mentaire\\n\",     \"    layers.Conv2D(64, (3, 3), activation='relu'),\\n\",     \"    layers.MaxPooling2D((2, 2)),\\n\",     \"    \\n\",     \"    # Aplatissement\\n\",     \"    layers.Flatten(),\\n\",     \"    \\n\",     \"    # Couche dense\\n\",     \"    layers.Dense(64, activation='relu'),\\n\",     \"    \\n\",     \"    # Couche de sortie\\n\",     \"    layers.Dense(10, activation='softmax')\\n\",     \"])\\n\",     \"\\n\",     \"# Compilation du mod\u00e8le\\n\",     \"model.compile(\\n\",     \"    optimizer='adam',\\n\",     \"    loss='categorical_crossentropy',\\n\",     \"    metrics=['accuracy']\\n\",     \")\\n\",     \"\\n\",     \"# Affichage du r\u00e9sum\u00e9 du mod\u00e8le\\n\",     \"model.summary()\"    ]   },   {    \"cell_type\": \"code\",\\n\",    \"execution_count\": null,\\n\",    \"metadata\": {},\\n\",    \"source\": [     \"# Entra\u00eenement du mod\u00e8le\\n\",     \"# Note : Nombre d'\u00e9poques r\u00e9duit pour la d\u00e9monstration\\n\",     \"history = model.fit(\\n\",     \"    X_train, y_train,\\n\",     \"    epochs=5,\\n\",     \"    batch_size=64,\\n\",     \"    validation_split=0.2,\\n\",     \"    verbose=1\\n\",     \")\\n\",     \"\\n\",     \"# \u00c9valuation du mod\u00e8le\\n\",     \"test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\\n\",     \"print(f\\\"\\\\nPr\u00e9cision sur l'ensemble de test : {test_accuracy*100:.2f}%\\\")\"    ]   },   {    \"cell_type\": \"code\",\\n\",    \"execution_count\": null,\\n\",    \"metadata\": {},\\n\",    \"source\": [     \"# Visualisation de la pr\u00e9cision et de la perte\\n\",     \"plt.figure(figsize=(12, 4))\\n\",     \"\\n\",     \"# Pr\u00e9cision\\n\",     \"plt.subplot(1, 2, 1)\\n\",     \"plt.plot(history.history['accuracy'], label='Pr\u00e9cision entra\u00eenement')\\n\",     \"plt.plot(history.history['val_accuracy'], label='Pr\u00e9cision validation')\\n\",     \"plt.title('Pr\u00e9cision du mod\u00e8le')\\n\",     \"plt.xlabel('\u00c9poque')\\n\",     \"plt.ylabel('Pr\u00e9cision')\\n\",     \"plt.legend()\\n\",     \"\\n\",     \"# Perte\\n\",     \"plt.subplot(1, 2, 2)\\n\",     \"plt.plot(history.history['loss'], label='Perte entra\u00eenement')\\n\",     \"plt.plot(history.history['val_loss'], label='Perte validation')\\n\",     \"plt.title('Perte du mod\u00e8le')\\n\",     \"plt.xlabel('\u00c9poque')\\n\",     \"plt.ylabel('Perte')\\n\",     \"plt.legend()\\n\",     \"\\n\",     \"plt.tight_layout()\\n\",     \"plt.show()\"    ]   },   {    \"cell_type\": \"code\",\\n\",    \"execution_count\": null,\\n\",    \"metadata\": {},\\n\",    \"source\": [     \"# Pr\u00e9dictions et visualisation\\n\",     \"# Pr\u00e9dire sur quelques images de test\\n\",     \"predictions = model.predict(X_test[:10])\\n\",     \"\\n\",     \"plt.figure(figsize=(15, 6))\\n\",     \"for i in range(10):\\n\",     \"    plt.subplot(2, 10, i+1)\\n\",     \"    plt.imshow(X_test[i].reshape(28, 28), cmap='gray')\\n\",     \"    plt.axis('off')\\n\",     \"    \\n\",     \"    plt.subplot(2, 10, i+11)\\n\",     \"    plt.bar(range(10), predictions[i])\\n\",     \"    plt.title(f\\\"Pr\u00e9diction: {np.argmax(predictions[i])}\\\")\\n\",     \"    plt.xticks(range(10))\\n\",     \"    plt.ylim(0, 1)\\n\",     \"\\n\",     \"plt.suptitle(\\\"Pr\u00e9dictions du mod\u00e8le\\\")\\n\",     \"plt.tight_layout()\\n\",     \"plt.show()\"    ]   },   {    \"cell_type\": \"markdown\",\\n\",    \"metadata\": {},\\n\",    \"source\": [     \"## \ud83e\udd14 Questions de r\u00e9flexion\\n\",     \"\\n\",     \"1. Que se passe-t-il si vous augmentez le nombre d'\u00e9poques ?\\n\",     \"2. Comment changeriez-vous l'architecture du r\u00e9seau pour am\u00e9liorer les performances ?\\n\",     \"3. Quelles diff\u00e9rences observez-vous entre la pr\u00e9cision d'entra\u00eenement et de validation ?\\n\",     \"\\n\",     \"## \ud83d\ude80 D\u00e9fis\\n\",     \"\\n\",     \"- Essayez de modifier le nombre de neurones dans les couches denses\\n\",     \"- Changez la fonction d'activation dans certaines couches\\n\",     \"- Ajoutez une couche de dropout pour r\u00e9duire le surapprentissage\"    ]   }  ],  \"metadata\": {   \"kernelspec\": {    \"display_name\": \"Python 3\",    \"language\": \"python\",    \"name\": \"python3\"   },   \"language_info\": {    \"name\": \"python\",    \"version\": \"3.8.0\"   }  },  \"nbformat\": 4,  \"nbformat_minor\": 2 } <pre>\n  Cell In[2], line 106\n    \"cell_type\": \"code\",\\n\",\n                         ^\nSyntaxError: unexpected character after line continuation character\n</pre>"},{"location":"ressources/notebooks/hello-world-dl/","title":"\ud83d\ude80 Hello World du Deep Learning","text":""},{"location":"ressources/notebooks/hello-world-dl/#reconnaissance-de-chiffres-manuscrits-avec-tensorflow-et-keras","title":"Reconnaissance de chiffres manuscrits avec TensorFlow et Keras","text":""},{"location":"ressources/notebooks/hello-world-dl/#objectifs-de-ce-notebook","title":"Objectifs de ce notebook","text":"<ul> <li>Charger et pr\u00e9parer un jeu de donn\u00e9es de chiffres manuscrits</li> <li>Cr\u00e9er un r\u00e9seau de neurones simple</li> <li>Entra\u00eener le mod\u00e8le</li> <li>Visualiser les r\u00e9sultats</li> <li>Tester le mod\u00e8le avec vos propres dessins</li> </ul> <p>```python</p>"},{"location":"ressources/notebooks/hello-world-dl/#importation-des-bibliotheques-necessaires","title":"Importation des biblioth\u00e8ques n\u00e9cessaires","text":"<p>import numpy as np import matplotlib.pyplot as plt import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers</p>"},{"location":"ressources/notebooks/hello-world-dl/#verification-de-la-version-de-tensorflow","title":"V\u00e9rification de la version de TensorFlow","text":"<p>print(f\"TensorFlow version: {tf.version}\") print(f\"Keras version: {keras.version}\")</p>"},{"location":"ressources/notebooks/hello-world-dl/#verification-du-gpu-si-disponible","title":"V\u00e9rification du GPU (si disponible)","text":"<p>print(\"GPU disponible :\", tf.test.is_gpu_available())</p>"},{"location":"ressources/notebooks/model-improvement/","title":"Model improvement","text":"In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! <pre>{\n  \"cells\": [\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"# Challenge d'am\u00e9lioration de mod\u00e8le CNN\\n\",\n        \"\\n\",\n        \"## BTS SIO  - S\u00e9ance 2: Types de r\u00e9seaux et applications\\n\",\n        \"\\n\",\n        \"Ce notebook vous guidera \u00e0 travers un challenge d'am\u00e9lioration d'un mod\u00e8le CNN pour la classification d'images de v\u00eatements (Fashion MNIST). Vous partirez d'un mod\u00e8le de base volontairement sous-optimal et explorerez diff\u00e9rentes strat\u00e9gies pour am\u00e9liorer ses performances.\\n\",\n        \"\\n\",\n        \"### Objectifs d'apprentissage:\\n\",\n        \"- Diagnostiquer les faiblesses d'un mod\u00e8le de Deep Learning\\n\",\n        \"- Exp\u00e9rimenter avec diff\u00e9rentes architectures et hyperparam\u00e8tres\\n\",\n        \"- Appliquer des techniques d'optimisation (dropout, batch normalization, etc.)\\n\",\n        \"- Mesurer et comparer quantitativement les am\u00e9liorations\\n\",\n        \"- Documenter m\u00e9thodiquement les modifications et leurs impacts\\n\",\n        \"\\n\",\n        \"### Pr\u00e9requis:\\n\",\n        \"- Connaissances de base en TensorFlow/Keras\\n\",\n        \"- Compr\u00e9hension des principes des r\u00e9seaux CNN\\n\",\n        \"- Avoir suivi la premi\u00e8re partie du TP sur les CNN\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 1. Configuration de l'environnement\\n\",\n        \"\\n\",\n        \"Commen\u00e7ons par importer les biblioth\u00e8ques n\u00e9cessaires.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"import tensorflow as tf\\n\",\n        \"from tensorflow.keras.models import Sequential, load_model\\n\",\n        \"from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation\\n\",\n        \"from tensorflow.keras.optimizers import Adam, RMSprop, SGD\\n\",\n        \"from tensorflow.keras.preprocessing.image import ImageDataGenerator\\n\",\n        \"from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\\n\",\n        \"from tensorflow.keras.datasets import fashion_mnist\\n\",\n        \"import numpy as np\\n\",\n        \"import matplotlib.pyplot as plt\\n\",\n        \"import pandas as pd\\n\",\n        \"import time\\n\",\n        \"import os\\n\",\n        \"import seaborn as sns\\n\",\n        \"from sklearn.metrics import confusion_matrix\\n\",\n        \"\\n\",\n        \"# Configuration pour reproductibilit\u00e9\\n\",\n        \"np.random.seed(42)\\n\",\n        \"tf.random.set_seed(42)\\n\",\n        \"\\n\",\n        \"# V\u00e9rifier la version de TensorFlow\\n\",\n        \"print(f\\\"TensorFlow version: {tf.__version__}\\\")\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 2. Chargement du dataset Fashion MNIST\\n\",\n        \"\\n\",\n        \"Fashion MNIST est un dataset similaire au MNIST original, mais avec des images de v\u00eatements au lieu de chiffres. C'est un excellent dataset pour tester des mod\u00e8les de vision par ordinateur.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"print(\\\"Chargement du dataset Fashion MNIST...\\\")\\n\",\n        \"(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\\n\",\n        \"\\n\",\n        \"# Normalisation et reshape pour correspondre au format attendu par le CNN\\n\",\n        \"x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\\n\",\n        \"x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\\n\",\n        \"\\n\",\n        \"# Noms des classes pour l'affichage\\n\",\n        \"class_names = ['T-shirt/top', 'Pantalon', 'Pull', 'Robe', 'Manteau',\\n\",\n        \"               'Sandale', 'Chemise', 'Basket', 'Sac', 'Bottine']\\n\",\n        \"\\n\",\n        \"print(f\\\"Forme des donn\u00e9es d'entra\u00eenement: {x_train.shape}\\\")\\n\",\n        \"print(f\\\"Forme des donn\u00e9es de test: {x_test.shape}\\\")\\n\",\n        \"print(f\\\"Nombre de classes: {len(class_names)}\\\")\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### Visualisation de quelques exemples\\n\",\n        \"\\n\",\n        \"Examinons \u00e0 quoi ressemblent les images de notre dataset.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"plt.figure(figsize=(10, 10))\\n\",\n        \"for i in range(25):\\n\",\n        \"    plt.subplot(5, 5, i+1)\\n\",\n        \"    plt.xticks([])\\n\",\n        \"    plt.yticks([])\\n\",\n        \"    plt.grid(False)\\n\",\n        \"    plt.imshow(x_train[i].reshape(28, 28), cmap='gray')\\n\",\n        \"    plt.xlabel(class_names[y_train[i]])\\n\",\n        \"plt.tight_layout()\\n\",\n        \"plt.show()\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 3. Tableau de bord des r\u00e9sultats\\n\",\n        \"\\n\",\n        \"Cr\u00e9ons une classe pour suivre et comparer les performances des diff\u00e9rents mod\u00e8les que nous allons tester.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"class ModelImprovementDashboard:\\n\",\n        \"    \\\"\\\"\\\"Classe pour suivre et afficher les r\u00e9sultats des diff\u00e9rentes am\u00e9liorations\\\"\\\"\\\"\\n\",\n        \"    \\n\",\n        \"    def __init__(self):\\n\",\n        \"        self.results = []\\n\",\n        \"    \\n\",\n        \"    def add_result(self, model_name, metrics, notes=\\\"\\\"):\\n\",\n        \"        \\\"\\\"\\\"Ajoute un r\u00e9sultat au tableau de bord\\\"\\\"\\\"\\n\",\n        \"        result = {\\n\",\n        \"            'model_name': model_name,\\n\",\n        \"            'accuracy': metrics['test_accuracy'],\\n\",\n        \"            'loss': metrics['test_loss'],\\n\",\n        \"            'training_time': metrics['training_time'],\\n\",\n        \"            'epochs': metrics['epochs_completed'],\\n\",\n        \"            'notes': notes\\n\",\n        \"        }\\n\",\n        \"        self.results.append(result)\\n\",\n        \"    \\n\",\n        \"    def show_results(self):\\n\",\n        \"        \\\"\\\"\\\"Affiche un tableau comparatif des r\u00e9sultats\\\"\\\"\\\"\\n\",\n        \"        if not self.results:\\n\",\n        \"            print(\\\"Aucun r\u00e9sultat \u00e0 afficher.\\\")\\n\",\n        \"            return\\n\",\n        \"        \\n\",\n        \"        # Cr\u00e9er un DataFrame\\n\",\n        \"        df = pd.DataFrame(self.results)\\n\",\n        \"        \\n\",\n        \"        # Trier par pr\u00e9cision (descendant)\\n\",\n        \"        df = df.sort_values(by='accuracy', ascending=False)\\n\",\n        \"        \\n\",\n        \"        # Formater les colonnes\\n\",\n        \"        df['accuracy'] = df['accuracy'].apply(lambda x: f\\\"{x:.2f}%\\\")\\n\",\n        \"        df['loss'] = df['loss'].apply(lambda x: f\\\"{x:.4f}\\\")\\n\",\n        \"        df['training_time'] = df['training_time'].apply(lambda x: f\\\"{x:.2f}s\\\")\\n\",\n        \"        \\n\",\n        \"        print(\\\"\\\\n=== TABLEAU COMPARATIF DES MOD\u00c8LES ===\\\")\\n\",\n        \"        print(df)\\n\",\n        \"        \\n\",\n        \"        return df\\n\",\n        \"    \\n\",\n        \"    def plot_comparison(self):\\n\",\n        \"        \\\"\\\"\\\"Visualise la comparaison des mod\u00e8les\\\"\\\"\\\"\\n\",\n        \"        if not self.results:\\n\",\n        \"            print(\\\"Aucun r\u00e9sultat \u00e0 afficher.\\\")\\n\",\n        \"            return\\n\",\n        \"        \\n\",\n        \"        # Pr\u00e9parer les donn\u00e9es\\n\",\n        \"        models = [r['model_name'] for r in self.results]\\n\",\n        \"        accuracies = [float(r['accuracy'].strip('%')) for r in self.results]\\n\",\n        \"        times = [float(r['training_time'].strip('s')) for r in self.results]\\n\",\n        \"        \\n\",\n        \"        # Cr\u00e9er le graphique\\n\",\n        \"        plt.figure(figsize=(12, 6))\\n\",\n        \"        \\n\",\n        \"        # Graphique de pr\u00e9cision\\n\",\n        \"        plt.subplot(1, 2, 1)\\n\",\n        \"        bars = plt.bar(models, accuracies, color='skyblue')\\n\",\n        \"        plt.title('Comparaison des pr\u00e9cisions')\\n\",\n        \"        plt.xlabel('Mod\u00e8le')\\n\",\n        \"        plt.ylabel('Pr\u00e9cision (%)')\\n\",\n        \"        plt.xticks(rotation=45, ha='right')\\n\",\n        \"        \\n\",\n        \"        # Ajouter les valeurs sur les barres\\n\",\n        \"        for bar in bars:\\n\",\n        \"            height = bar.get_height()\\n\",\n        \"            plt.text(bar.get_x() + bar.get_width()/2., height,\\n\",\n        \"                     f'{height:.2f}%',\\n\",\n        \"                     ha='center', va='bottom')\\n\",\n        \"        \\n\",\n        \"        # Graphique de temps d'entra\u00eenement\\n\",\n        \"        plt.subplot(1, 2, 2)\\n\",\n        \"        bars = plt.bar(models, times, color='salmon')\\n\",\n        \"        plt.title('Comparaison des temps d\\\\'entra\u00eenement')\\n\",\n        \"        plt.xlabel('Mod\u00e8le')\\n\",\n        \"        plt.ylabel('Temps (secondes)')\\n\",\n        \"        plt.xticks(rotation=45, ha='right')\\n\",\n        \"        \\n\",\n        \"        # Ajouter les valeurs sur les barres\\n\",\n        \"        for bar in bars:\\n\",\n        \"            height = bar.get_height()\\n\",\n        \"            plt.text(bar.get_x() + bar.get_width()/2., height,\\n\",\n        \"                     f'{height:.2f}s',\\n\",\n        \"                     ha='center', va='bottom')\\n\",\n        \"        \\n\",\n        \"        plt.tight_layout()\\n\",\n        \"        plt.show()\\n\",\n        \"\\n\",\n        \"# Initialiser le tableau de bord\\n\",\n        \"dashboard = ModelImprovementDashboard()\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 4. Fonctions d'\u00e9valuation de mod\u00e8le\\n\",\n        \"\\n\",\n        \"D\u00e9finissons des fonctions pour entra\u00eener, \u00e9valuer et visualiser les mod\u00e8les de mani\u00e8re coh\u00e9rente.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"def evaluate_model(model, x_train, y_train, x_test, y_test, epochs=5, batch_size=128, data_augmentation=False):\\n\",\n        \"    \\\"\\\"\\\"Entra\u00eene et \u00e9value un mod\u00e8le, retourne les m\u00e9triques de performance\\\"\\\"\\\"\\n\",\n        \"    \\n\",\n        \"    # Configuration pour l'augmentation de donn\u00e9es (si activ\u00e9e)\\n\",\n        \"    if data_augmentation:\\n\",\n        \"        train_datagen = ImageDataGenerator(\\n\",\n        \"            rotation_range=10,\\n\",\n        \"            width_shift_range=0.1,\\n\",\n        \"            height_shift_range=0.1,\\n\",\n        \"            zoom_range=0.1,\\n\",\n        \"        )\\n\",\n        \"        train_generator = train_datagen.flow(x_train, y_train, batch_size=batch_size)\\n\",\n        \"    \\n\",\n        \"    # Callbacks pour am\u00e9liorer l'entra\u00eenement\\n\",\n        \"    callbacks = []\\n\",\n        \"    if epochs &gt; 5:\\n\",\n        \"        callbacks = [\\n\",\n        \"            EarlyStopping(patience=5, restore_best_weights=True),\\n\",\n        \"            ReduceLROnPlateau(factor=0.2, patience=3, min_lr=0.0001)\\n\",\n        \"        ]\\n\",\n        \"    \\n\",\n        \"    # Mesure du temps d'entra\u00eenement\\n\",\n        \"    start_time = time.time()\\n\",\n        \"    \\n\",\n        \"    # Entra\u00eenement du mod\u00e8le\\n\",\n        \"    if data_augmentation:\\n\",\n        \"        history = model.fit(\\n\",\n        \"            train_generator,\\n\",\n        \"            epochs=epochs,\\n\",\n        \"            steps_per_epoch=len(x_train) // batch_size,\\n\",\n        \"            validation_data=(x_test, y_test),\\n\",\n        \"            callbacks=callbacks,\\n\",\n        \"            verbose=1\\n\",\n        \"        )\\n\",\n        \"    else:\\n\",\n        \"        history = model.fit(\\n\",\n        \"            x_train, y_train,\\n\",\n        \"            batch_size=batch_size,\\n\",\n        \"            epochs=epochs,\\n\",\n        \"            validation_data=(x_test, y_test),\\n\",\n        \"            callbacks=callbacks,\\n\",\n        \"            verbose=1\\n\",\n        \"        )\\n\",\n        \"    \\n\",\n        \"    training_time = time.time() - start_time\\n\",\n        \"    \\n\",\n        \"    # \u00c9valuation du mod\u00e8le\\n\",\n        \"    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\\n\",\n        \"    \\n\",\n        \"    # Pr\u00e9parer les m\u00e9triques\\n\",\n        \"    metrics = {\\n\",\n        \"        'test_accuracy': test_acc * 100,\\n\",\n        \"        'test_loss': test_loss,\\n\",\n        \"        'training_time': training_time,\\n\",\n        \"        'epochs_completed': len(history.history['loss']),\\n\",\n        \"        'history': history\\n\",\n        \"    }\\n\",\n        \"    \\n\",\n        \"    return metrics\\n\",\n        \"\\n\",\n        \"def plot_training_history(history):\\n\",\n        \"    \\\"\\\"\\\"Visualise l'historique d'entra\u00eenement\\\"\\\"\\\"\\n\",\n        \"    plt.figure(figsize=(12, 5))\\n\",\n        \"    \\n\",\n        \"    # Graphique de pr\u00e9cision\\n\",\n        \"    plt.subplot(1, 2, 1)\\n\",\n        \"    plt.plot(history.history['accuracy'], label='Entra\u00eenement')\\n\",\n        \"    plt.plot(history.history['val_accuracy'], label='Validation')\\n\",\n        \"    plt.title('\u00c9volution de la pr\u00e9cision')\\n\",\n        \"    plt.xlabel('\u00c9poque')\\n\",\n        \"    plt.ylabel('Pr\u00e9cision')\\n\",\n        \"    plt.legend()\\n\",\n        \"    plt.grid(True, linestyle='--', alpha=0.6)\\n\",\n        \"    \\n\",\n        \"    # Graphique de perte\\n\",\n        \"    plt.subplot(1, 2, 2)\\n\",\n        \"    plt.plot(history.history['loss'], label='Entra\u00eenement')\\n\",\n        \"    plt.plot(history.history['val_loss'], label='Validation')\\n\",\n        \"    plt.title('\u00c9volution de la perte')\\n\",\n        \"    plt.xlabel('\u00c9poque')\\n\",\n        \"    plt.ylabel('Perte')\\n\",\n        \"    plt.legend()\\n\",\n        \"    plt.grid(True, linestyle='--', alpha=0.6)\\n\",\n        \"    \\n\",\n        \"    plt.tight_layout()\\n\",\n        \"    plt.show()\\n\",\n        \"\\n\",\n        \"def plot_confusion_matrix(model, x_test, y_test):\\n\",\n        \"    \\\"\\\"\\\"Visualise la matrice de confusion du mod\u00e8le\\\"\\\"\\\"\\n\",\n        \"    # Pr\u00e9dictions\\n\",\n        \"    y_pred = model.predict(x_test)\\n\",\n        \"    y_pred_classes = np.argmax(y_pred, axis=1)\\n\",\n        \"    \\n\",\n        \"    # Calculer la matrice de confusion\\n\",\n        \"    conf_mat = confusion_matrix(y_test, y_pred_classes)\\n\",\n        \"    \\n\",\n        \"    # Visualisation\\n\",\n        \"    plt.figure(figsize=(10, 8))\\n\",\n        \"    sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\\n\",\n        \"                xticklabels=class_names,\\n\",\n        \"                yticklabels=class_names)\\n\",\n        \"    plt.xlabel('Pr\u00e9dit')\\n\",\n        \"    plt.ylabel('R\u00e9el')\\n\",\n        \"    plt.title('Matrice de confusion')\\n\",\n        \"    plt.xticks(rotation=45, ha='right')\\n\",\n        \"    plt.tight_layout()\\n\",\n        \"    plt.show()\\n\",\n        \"\\n\",\n        \"def show_misclassified_examples(model, x_test, y_test, n=10):\\n\",\n        \"    \\\"\\\"\\\"Affiche des exemples d'images mal classifi\u00e9es\\\"\\\"\\\"\\n\",\n        \"    predictions = model.predict(x_test)\\n\",\n        \"    predicted_classes = np.argmax(predictions, axis=1)\\n\",\n        \"    \\n\",\n        \"    # Trouver les erreurs\\n\",\n        \"    errors = (predicted_classes != y_test)\\n\",\n        \"    error_indices = np.where(errors)[0]\\n\",\n        \"    \\n\",\n        \"    if len(error_indices) == 0:\\n\",\n        \"        print(\\\"Aucune erreur trouv\u00e9e!\\\")\\n\",\n        \"        return\\n\",\n        \"    \\n\",\n        \"    # S\u00e9lectionner un \u00e9chantillon d'erreurs\\n\",\n        \"    sample_size = min(n, len(error_indices))\\n\",\n        \"    sample_indices = np.random.choice(error_indices, size=sample_size, replace=False)\\n\",\n        \"    \\n\",\n        \"    # Afficher les exemples\\n\",\n        \"    plt.figure(figsize=(15, 3*sample_size//5 + 3))\\n\",\n        \"    for i, idx in enumerate(sample_indices):\\n\",\n        \"        plt.subplot(sample_size//5 + 1, 5, i+1)\\n\",\n        \"        plt.imshow(x_test[idx].reshape(28, 28), cmap='gray')\\n\",\n        \"        plt.title(f\\\"R\u00e9el: {class_names[y_test[idx]]}\\\\nPr\u00e9dit: {class_names[predicted_classes[idx]]}\\\")\\n\",\n        \"        plt.axis('off')\\n\",\n        \"    plt.tight_layout()\\n\",\n        \"    plt.show()\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 5. Mod\u00e8le de base (sous-optimal)\\n\",\n        \"\\n\",\n        \"Commen\u00e7ons par cr\u00e9er et \u00e9valuer un mod\u00e8le CNN de base, volontairement sous-optimal, qui servira de point de r\u00e9f\u00e9rence pour nos am\u00e9liorations.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"def create_baseline_model():\\n\",\n        \"    \\\"\\\"\\\"Cr\u00e9e un mod\u00e8le CNN de base volontairement sous-performant\\\"\\\"\\\"\\n\",\n        \"    model = Sequential([\\n\",\n        \"        Conv2D(8, (3, 3), activation='relu', input_shape=(28, 28, 1)),\\n\",\n        \"        MaxPooling2D((2, 2)),\\n\",\n        \"        Flatten(),\\n\",\n        \"        Dense(16, activation='relu'),\\n\",\n        \"        Dense(10, activation='softmax')\\n\",\n        \"    ])\\n\",\n        \"    \\n\",\n        \"    model.compile(\\n\",\n        \"        optimizer=Adam(learning_rate=0.01),  # Learning rate trop \u00e9lev\u00e9\\n\",\n        \"        loss='sparse_categorical_crossentropy',\\n\",\n        \"        metrics=['accuracy']\\n\",\n        \"    )\\n\",\n        \"    \\n\",\n        \"    return model\\n\",\n        \"\\n\",\n        \"# Cr\u00e9er et afficher le mod\u00e8le de base\\n\",\n        \"baseline_model = create_baseline_model()\\n\",\n        \"baseline_model.summary()\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### Entra\u00eenement et \u00e9valuation du mod\u00e8le de base\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"print(\\\"\\\\n--- Mod\u00e8le de base ---\\\")\\n\",\n        \"baseline_metrics = evaluate_model(baseline_model, x_train, y_train, x_test, y_test, epochs=5)\\n\",\n        \"print(f\\\"Pr\u00e9cision du mod\u00e8le de base: {baseline_metrics['test_accuracy']:.2f}%\\\")\\n\",\n        \"print(f\\\"Temps d'entra\u00eenement: {baseline_metrics['training_time']:.2f} secondes\\\")\\n\",\n        \"\\n\",\n        \"# Visualiser l'historique d'entra\u00eenement\\n\",\n        \"plot_training_history(baseline_metrics['history'])\\n\",\n        \"\\n\",\n        \"# Ajouter au tableau de bord\\n\",\n        \"dashboard.add_result(\\\"Mod\u00e8le de base\\\", baseline_metrics, \\n\",\n        \"                     \\\"CNN simple, peu de filtres, learning rate \u00e9lev\u00e9\\\")\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### Analyse des erreurs du mod\u00e8le de base\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# Afficher la matrice de confusion\\n\",\n        \"plot_confusion_matrix(baseline_model, x_test, y_test)\\n\",\n        \"\\n\",\n        \"# Afficher des exemples d'erreurs\\n\",\n        \"print(\\\"\\\\nExemples d'erreurs de classification du mod\u00e8le de base:\\\")\\n\",\n        \"show_misclassified_examples(baseline_model, x_test, y_test)\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### \ud83d\udd0d Diagnostic du mod\u00e8le de base\\n\",\n        \"\\n\",\n        \"Avant de passer aux am\u00e9liorations, analysons les probl\u00e8mes du mod\u00e8le de base :\\n\",\n        \"\\n\",\n        \"1. **Architecture trop simple** : \\n\",\n        \"   - Seulement 8 filtres dans la couche de convolution\\n\",\n        \"   - Une seule couche de convolution\\n\",\n        \"   - Seulement 16 neurones dans la couche dense\\n\",\n        \"   \\n\",\n        \"2. **Optimisation probl\u00e9matique** :\\n\",\n        \"   - Taux d'apprentissage trop \u00e9lev\u00e9 (0.01)\\n\",\n        \"   - Pas de r\u00e9gularisation (dropout, etc.)\\n\",\n        \"   - Nombre d'\u00e9poques potentiellement insuffisant\\n\",\n        \"   \\n\",\n        \"3. **Pr\u00e9traitement minimal** :\\n\",\n        \"   - Pas d'augmentation de donn\u00e9es\\n\",\n        \"   - Pas de normalisation batch\\n\",\n        \"\\n\",\n        \"Ces observations nous guideront dans nos tentatives d'am\u00e9lioration.\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 6. Premi\u00e8re am\u00e9lioration : Architecture plus profonde\\n\",\n        \"\\n\",\n        \"Pour notre premi\u00e8re am\u00e9lioration, nous allons :\\n\",\n        \"- Augmenter le nombre de filtres\\n\",\n        \"- Ajouter une couche de convolution suppl\u00e9mentaire\\n\",\n        \"- Augmenter le nombre de neurones dans la couche dense\\n\",\n        \"- R\u00e9duire le taux d'apprentissage\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"def create_improved_model_1():\\n\",\n        \"    \\\"\\\"\\\"Premier exemple d'am\u00e9lioration: architecture plus profonde\\\"\\\"\\\"\\n\",\n        \"    model = Sequential([\\n\",\n        \"        Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\\n\",\n        \"        MaxPooling2D((2, 2)),\\n\",\n        \"        Conv2D(64, (3, 3), activation='relu'),\\n\",\n        \"        MaxPooling2D((2, 2)),\\n\",\n        \"        Flatten(),\\n\",\n        \"        Dense(128, activation='relu'),\\n\",\n        \"        Dense(10, activation='softmax')\\n\",\n        \"    ])\\n\",\n        \"    \\n\",\n        \"    model.compile(\\n\",\n        \"        optimizer=Adam(learning_rate=0.001),  # Taux d'apprentissage r\u00e9duit\\n\",\n        \"        loss='sparse_categorical_crossentropy',\\n\",\n        \"        metrics=['accuracy']\\n\",\n        \"    )\\n\",\n        \"    \\n\",\n        \"    return model\\n\",\n        \"\\n\",\n        \"# Cr\u00e9er et afficher le mod\u00e8le am\u00e9lior\u00e9 1\\n\",\n        \"improved_model_1 = create_improved_model_1()\\n\",\n        \"improved_model_1.summary()\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### Entra\u00eenement et \u00e9valuation du mod\u00e8le am\u00e9lior\u00e9 1\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"print(\\\"\\\\n--- Mod\u00e8le am\u00e9lior\u00e9 1 ---\\\")\\n\",\n        \"improved_metrics_1 = evaluate_model(improved_model_1, x_train, y_train, x_test, y_test, epochs=10)\\n\",\n        \"print(f\\\"Pr\u00e9cision du mod\u00e8le am\u00e9lior\u00e9 1: {improved_metrics_1['test_accuracy']:.2f}%\\\")\\n\",\n        \"print(f\\\"Temps d'entra\u00eenement: {improved_metrics_1['training_time']:.2f} secondes\\\")\\n\",\n        \"\\n\",\n        \"# Visualiser l'historique d'entra\u00eenement\\n\",\n        \"plot_training_history(improved_metrics_1['history'])\\n\",\n        \"\\n\",\n        \"# Ajouter au tableau de bord\\n\",\n        \"dashboard.add_result(\\\"Mod\u00e8le am\u00e9lior\u00e9 1\\\", improved_metrics_1, \\n\",\n        \"                    \\\"Plus de filtres, couche suppl\u00e9mentaire, learning rate plus bas\\\")\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### Analyse des r\u00e9sultats du mod\u00e8le am\u00e9lior\u00e9 1\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# Visualiser les r\u00e9sultats de l'am\u00e9lioration\\n\",\n        \"print(\\\"Comparaison des mod\u00e8les jusqu'\u00e0 pr\u00e9sent:\\\")\\n\",\n        \"dashboard.show_results()\\n\",\n        \"\\n\",\n        \"# Voir les nouvelles erreurs\\n\",\n        \"print(\\\"\\\\nExemples d'erreurs apr\u00e8s la premi\u00e8re am\u00e9lioration:\\\")\\n\",\n        \"show_misclassified_examples(improved_model_1, x_test, y_test)\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 7. Deuxi\u00e8me am\u00e9lioration : R\u00e9gularisation et augmentation de donn\u00e9es\\n\",\n        \"\\n\",\n        \"Pour notre deuxi\u00e8me am\u00e9lioration, nous allons :\\n\",\n        \"- Ajouter du dropout pour \u00e9viter le surapprentissage\\n\",\n        \"- Int\u00e9grer la normalisation par batch (batch normalization)\\n\",\n        \"- Utiliser l'augmentation de donn\u00e9es pour am\u00e9liorer la g\u00e9n\u00e9ralisation\\n\",\n        \"\\n\",\n        \"### Architecture du mod\u00e8le am\u00e9lior\u00e9 2\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"def create_improved_model_2():\\n\",\n        \"    \\\"\\\"\\\"Deuxi\u00e8me exemple d'am\u00e9lioration: ajout de dropout et batch normalization\\\"\\\"\\\"\\n\",\n        \"    model = Sequential([\\n\",\n        \"        # Premi\u00e8re couche de convolution avec batch normalization\\n\",\n        \"        Conv2D(32, (3, 3), padding='same', input_shape=(28, 28, 1)),\\n\",\n        \"        BatchNormalization(),\\n\",\n        \"        Activation('relu'),\\n\",\n        \"        MaxPooling2D((2, 2)),\\n\",\n        \"        \\n\",\n        \"        # Deuxi\u00e8me couche de convolution avec batch normalization\\n\",\n        \"        Conv2D(64, (3, 3), padding='same'),\\n\",\n        \"        BatchNormalization(),\\n\",\n        \"        Activation('relu'),\\n\",\n        \"        MaxPooling2D((2, 2)),\\n\",\n        \"        \\n\",\n        \"        # Aplatissement\\n\",\n        \"        Flatten(),\\n\",\n        \"        \\n\",\n        \"        # Couche dense avec batch normalization et dropout\\n\",\n        \"        Dense(128),\\n\",\n        \"        BatchNormalization(),\\n\",\n        \"        Activation('relu'),\\n\",\n        \"        Dropout(0.5),  # 50% de dropout pour la r\u00e9gularisation\\n\",\n        \"        \\n\",\n        \"        # Couche de sortie\\n\",\n        \"        Dense(10, activation='softmax')\\n\",\n        \"    ])\\n\",\n        \"    \\n\",\n        \"    model.compile(\\n\",\n        \"        optimizer=Adam(learning_rate=0.001),\\n\",\n        \"        loss='sparse_categorical_crossentropy',\\n\",\n        \"        metrics=['accuracy']\\n\",\n        \"    )\\n\",\n        \"    \\n\",\n        \"    return model\\n\",\n        \"\\n\",\n        \"# Cr\u00e9er et afficher le mod\u00e8le am\u00e9lior\u00e9 2\\n\",\n        \"improved_model_2 = create_improved_model_2()\\n\",\n        \"improved_model_2.summary()\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### Entra\u00eenement avec augmentation de donn\u00e9es\\n\",\n        \"\\n\",\n        \"Pour cette am\u00e9lioration, nous allons \u00e9galement utiliser l'augmentation de donn\u00e9es qui permet de g\u00e9n\u00e9rer artificiellement plus d'exemples d'entra\u00eenement en appliquant des transformations aux images existantes. Cela am\u00e9liore la robustesse du mod\u00e8le face aux variations qu'il pourrait rencontrer en conditions r\u00e9elles.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"print(\\\"\\\\n--- Mod\u00e8le am\u00e9lior\u00e9 2 (avec augmentation de donn\u00e9es) ---\\\")\\n\",\n        \"improved_metrics_2 = evaluate_model(improved_model_2, x_train, y_train, x_test, y_test, \\n\",\n        \"                                   epochs=15, data_augmentation=True)\\n\",\n        \"print(f\\\"Pr\u00e9cision du mod\u00e8le am\u00e9lior\u00e9 2: {improved_metrics_2['test_accuracy']:.2f}%\\\")\\n\",\n        \"print(f\\\"Temps d'entra\u00eenement: {improved_metrics_2['training_time']:.2f} secondes\\\")\\n\",\n        \"\\n\",\n        \"# Visualiser l'historique d'entra\u00eenement\\n\",\n        \"plot_training_history(improved_metrics_2['history'])\\n\",\n        \"\\n\",\n        \"# Ajouter au tableau de bord\\n\",\n        \"dashboard.add_result(\\\"Mod\u00e8le am\u00e9lior\u00e9 2\\\", improved_metrics_2, \\n\",\n        \"                    \\\"Dropout, BatchNorm, augmentation de donn\u00e9es\\\")\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"### Analyse des r\u00e9sultats du mod\u00e8le am\u00e9lior\u00e9 2\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n        \"# Visualiser la matrice de confusion\\n\",\n        \"plot_confusion_matrix(improved_model_2, x_test, y_test)\\n\",\n        \"\\n\",\n        \"# Afficher des exemples d'erreurs\\n\",\n        \"print(\\\"\\\\nExemples d'erreurs apr\u00e8s la deuxi\u00e8me am\u00e9lioration:\\\")\\n\",\n        \"show_misclassified_examples(improved_model_2, x_test, y_test)\\n\",\n        \"\\n\",\n        \"# Comparer tous les mod\u00e8les\\n\",\n        \"dashboard.show_results()\\n\",\n        \"dashboard.plot_comparison()\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"## 8. Cr\u00e9ation de votre propre mod\u00e8le am\u00e9lior\u00e9\\n\",\n        \"\\n\",\n        \"C'est maintenant \u00e0 vous de concevoir votre propre am\u00e9lioration! Vous pouvez explorer diff\u00e9rentes architectures, techniques d'optimisation, ou combinaisons d'approches.\\n\",\n        \"\\n\",\n        \"Voici quelques pistes d'am\u00e9lioration possibles:\\n\",\n        \"- Essayer diff\u00e9rentes architectures (plus/moins de couches, filtres, etc.)\\n\",\n        \"- Exp\u00e9rimenter avec d'autres optimiseurs (RMSprop, SGD avec momentum, etc.)\\n\",\n        \"- Tester diff\u00e9rentes techniques de r\u00e9gularisation\\n\",\n        \"- Modifier les param\u00e8tres d'augmentation de donn\u00e9es\\n\",\n        \"- Utiliser des connexions r\u00e9siduelles (comme dans les architectures ResNet)\\n\",\n        \"- Combiner les meilleures pratiques des mod\u00e8les pr\u00e9c\u00e9dents\"\n      ]\n    },\n.\\n\",\n        \"        \\n\",\n        \"        # Couche de sortie\\n\",\n        \"        Dense(10, activation='softmax')\\n\",\n        \"    ])\\n\",\n        \"    \\n\",\n        \"    # Compilation\\n\",\n        \"    model.compile(\\n\",\n        \"        optimizer='adam',  # Modifiez selon vos pr\u00e9f\u00e9rences\\n\",\n        \"        loss='sparse_categorical_crossentropy',\\n\",\n        \"        metrics=['accuracy']\\n\",\n        \"    )\\n\",\n        \"    \\n\",\n        \"    return model\\n\",\n        \"\\n\",\n        \"# Si vous \u00eates pr\u00eat \u00e0 tester votre mod\u00e8le, d\u00e9commentez les lignes suivantes\\n\",\n        \"#your_model = create_your_improved_model()\\n\",\n        \"#your_model.summary()\"\n      ]\n    },\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"execution_count\": null,\n      \"metadata\": {},\n      \"outputs\": [],\n      \"source\": [\n   \n</pre> {   \"cells\": [     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"# Challenge d'am\u00e9lioration de mod\u00e8le CNN\\n\",         \"\\n\",         \"## BTS SIO  - S\u00e9ance 2: Types de r\u00e9seaux et applications\\n\",         \"\\n\",         \"Ce notebook vous guidera \u00e0 travers un challenge d'am\u00e9lioration d'un mod\u00e8le CNN pour la classification d'images de v\u00eatements (Fashion MNIST). Vous partirez d'un mod\u00e8le de base volontairement sous-optimal et explorerez diff\u00e9rentes strat\u00e9gies pour am\u00e9liorer ses performances.\\n\",         \"\\n\",         \"### Objectifs d'apprentissage:\\n\",         \"- Diagnostiquer les faiblesses d'un mod\u00e8le de Deep Learning\\n\",         \"- Exp\u00e9rimenter avec diff\u00e9rentes architectures et hyperparam\u00e8tres\\n\",         \"- Appliquer des techniques d'optimisation (dropout, batch normalization, etc.)\\n\",         \"- Mesurer et comparer quantitativement les am\u00e9liorations\\n\",         \"- Documenter m\u00e9thodiquement les modifications et leurs impacts\\n\",         \"\\n\",         \"### Pr\u00e9requis:\\n\",         \"- Connaissances de base en TensorFlow/Keras\\n\",         \"- Compr\u00e9hension des principes des r\u00e9seaux CNN\\n\",         \"- Avoir suivi la premi\u00e8re partie du TP sur les CNN\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 1. Configuration de l'environnement\\n\",         \"\\n\",         \"Commen\u00e7ons par importer les biblioth\u00e8ques n\u00e9cessaires.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"import tensorflow as tf\\n\",         \"from tensorflow.keras.models import Sequential, load_model\\n\",         \"from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation\\n\",         \"from tensorflow.keras.optimizers import Adam, RMSprop, SGD\\n\",         \"from tensorflow.keras.preprocessing.image import ImageDataGenerator\\n\",         \"from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\\n\",         \"from tensorflow.keras.datasets import fashion_mnist\\n\",         \"import numpy as np\\n\",         \"import matplotlib.pyplot as plt\\n\",         \"import pandas as pd\\n\",         \"import time\\n\",         \"import os\\n\",         \"import seaborn as sns\\n\",         \"from sklearn.metrics import confusion_matrix\\n\",         \"\\n\",         \"# Configuration pour reproductibilit\u00e9\\n\",         \"np.random.seed(42)\\n\",         \"tf.random.set_seed(42)\\n\",         \"\\n\",         \"# V\u00e9rifier la version de TensorFlow\\n\",         \"print(f\\\"TensorFlow version: {tf.__version__}\\\")\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 2. Chargement du dataset Fashion MNIST\\n\",         \"\\n\",         \"Fashion MNIST est un dataset similaire au MNIST original, mais avec des images de v\u00eatements au lieu de chiffres. C'est un excellent dataset pour tester des mod\u00e8les de vision par ordinateur.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"print(\\\"Chargement du dataset Fashion MNIST...\\\")\\n\",         \"(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\\n\",         \"\\n\",         \"# Normalisation et reshape pour correspondre au format attendu par le CNN\\n\",         \"x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\\n\",         \"x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\\n\",         \"\\n\",         \"# Noms des classes pour l'affichage\\n\",         \"class_names = ['T-shirt/top', 'Pantalon', 'Pull', 'Robe', 'Manteau',\\n\",         \"               'Sandale', 'Chemise', 'Basket', 'Sac', 'Bottine']\\n\",         \"\\n\",         \"print(f\\\"Forme des donn\u00e9es d'entra\u00eenement: {x_train.shape}\\\")\\n\",         \"print(f\\\"Forme des donn\u00e9es de test: {x_test.shape}\\\")\\n\",         \"print(f\\\"Nombre de classes: {len(class_names)}\\\")\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### Visualisation de quelques exemples\\n\",         \"\\n\",         \"Examinons \u00e0 quoi ressemblent les images de notre dataset.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"plt.figure(figsize=(10, 10))\\n\",         \"for i in range(25):\\n\",         \"    plt.subplot(5, 5, i+1)\\n\",         \"    plt.xticks([])\\n\",         \"    plt.yticks([])\\n\",         \"    plt.grid(False)\\n\",         \"    plt.imshow(x_train[i].reshape(28, 28), cmap='gray')\\n\",         \"    plt.xlabel(class_names[y_train[i]])\\n\",         \"plt.tight_layout()\\n\",         \"plt.show()\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 3. Tableau de bord des r\u00e9sultats\\n\",         \"\\n\",         \"Cr\u00e9ons une classe pour suivre et comparer les performances des diff\u00e9rents mod\u00e8les que nous allons tester.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"class ModelImprovementDashboard:\\n\",         \"    \\\"\\\"\\\"Classe pour suivre et afficher les r\u00e9sultats des diff\u00e9rentes am\u00e9liorations\\\"\\\"\\\"\\n\",         \"    \\n\",         \"    def __init__(self):\\n\",         \"        self.results = []\\n\",         \"    \\n\",         \"    def add_result(self, model_name, metrics, notes=\\\"\\\"):\\n\",         \"        \\\"\\\"\\\"Ajoute un r\u00e9sultat au tableau de bord\\\"\\\"\\\"\\n\",         \"        result = {\\n\",         \"            'model_name': model_name,\\n\",         \"            'accuracy': metrics['test_accuracy'],\\n\",         \"            'loss': metrics['test_loss'],\\n\",         \"            'training_time': metrics['training_time'],\\n\",         \"            'epochs': metrics['epochs_completed'],\\n\",         \"            'notes': notes\\n\",         \"        }\\n\",         \"        self.results.append(result)\\n\",         \"    \\n\",         \"    def show_results(self):\\n\",         \"        \\\"\\\"\\\"Affiche un tableau comparatif des r\u00e9sultats\\\"\\\"\\\"\\n\",         \"        if not self.results:\\n\",         \"            print(\\\"Aucun r\u00e9sultat \u00e0 afficher.\\\")\\n\",         \"            return\\n\",         \"        \\n\",         \"        # Cr\u00e9er un DataFrame\\n\",         \"        df = pd.DataFrame(self.results)\\n\",         \"        \\n\",         \"        # Trier par pr\u00e9cision (descendant)\\n\",         \"        df = df.sort_values(by='accuracy', ascending=False)\\n\",         \"        \\n\",         \"        # Formater les colonnes\\n\",         \"        df['accuracy'] = df['accuracy'].apply(lambda x: f\\\"{x:.2f}%\\\")\\n\",         \"        df['loss'] = df['loss'].apply(lambda x: f\\\"{x:.4f}\\\")\\n\",         \"        df['training_time'] = df['training_time'].apply(lambda x: f\\\"{x:.2f}s\\\")\\n\",         \"        \\n\",         \"        print(\\\"\\\\n=== TABLEAU COMPARATIF DES MOD\u00c8LES ===\\\")\\n\",         \"        print(df)\\n\",         \"        \\n\",         \"        return df\\n\",         \"    \\n\",         \"    def plot_comparison(self):\\n\",         \"        \\\"\\\"\\\"Visualise la comparaison des mod\u00e8les\\\"\\\"\\\"\\n\",         \"        if not self.results:\\n\",         \"            print(\\\"Aucun r\u00e9sultat \u00e0 afficher.\\\")\\n\",         \"            return\\n\",         \"        \\n\",         \"        # Pr\u00e9parer les donn\u00e9es\\n\",         \"        models = [r['model_name'] for r in self.results]\\n\",         \"        accuracies = [float(r['accuracy'].strip('%')) for r in self.results]\\n\",         \"        times = [float(r['training_time'].strip('s')) for r in self.results]\\n\",         \"        \\n\",         \"        # Cr\u00e9er le graphique\\n\",         \"        plt.figure(figsize=(12, 6))\\n\",         \"        \\n\",         \"        # Graphique de pr\u00e9cision\\n\",         \"        plt.subplot(1, 2, 1)\\n\",         \"        bars = plt.bar(models, accuracies, color='skyblue')\\n\",         \"        plt.title('Comparaison des pr\u00e9cisions')\\n\",         \"        plt.xlabel('Mod\u00e8le')\\n\",         \"        plt.ylabel('Pr\u00e9cision (%)')\\n\",         \"        plt.xticks(rotation=45, ha='right')\\n\",         \"        \\n\",         \"        # Ajouter les valeurs sur les barres\\n\",         \"        for bar in bars:\\n\",         \"            height = bar.get_height()\\n\",         \"            plt.text(bar.get_x() + bar.get_width()/2., height,\\n\",         \"                     f'{height:.2f}%',\\n\",         \"                     ha='center', va='bottom')\\n\",         \"        \\n\",         \"        # Graphique de temps d'entra\u00eenement\\n\",         \"        plt.subplot(1, 2, 2)\\n\",         \"        bars = plt.bar(models, times, color='salmon')\\n\",         \"        plt.title('Comparaison des temps d\\\\'entra\u00eenement')\\n\",         \"        plt.xlabel('Mod\u00e8le')\\n\",         \"        plt.ylabel('Temps (secondes)')\\n\",         \"        plt.xticks(rotation=45, ha='right')\\n\",         \"        \\n\",         \"        # Ajouter les valeurs sur les barres\\n\",         \"        for bar in bars:\\n\",         \"            height = bar.get_height()\\n\",         \"            plt.text(bar.get_x() + bar.get_width()/2., height,\\n\",         \"                     f'{height:.2f}s',\\n\",         \"                     ha='center', va='bottom')\\n\",         \"        \\n\",         \"        plt.tight_layout()\\n\",         \"        plt.show()\\n\",         \"\\n\",         \"# Initialiser le tableau de bord\\n\",         \"dashboard = ModelImprovementDashboard()\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 4. Fonctions d'\u00e9valuation de mod\u00e8le\\n\",         \"\\n\",         \"D\u00e9finissons des fonctions pour entra\u00eener, \u00e9valuer et visualiser les mod\u00e8les de mani\u00e8re coh\u00e9rente.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"def evaluate_model(model, x_train, y_train, x_test, y_test, epochs=5, batch_size=128, data_augmentation=False):\\n\",         \"    \\\"\\\"\\\"Entra\u00eene et \u00e9value un mod\u00e8le, retourne les m\u00e9triques de performance\\\"\\\"\\\"\\n\",         \"    \\n\",         \"    # Configuration pour l'augmentation de donn\u00e9es (si activ\u00e9e)\\n\",         \"    if data_augmentation:\\n\",         \"        train_datagen = ImageDataGenerator(\\n\",         \"            rotation_range=10,\\n\",         \"            width_shift_range=0.1,\\n\",         \"            height_shift_range=0.1,\\n\",         \"            zoom_range=0.1,\\n\",         \"        )\\n\",         \"        train_generator = train_datagen.flow(x_train, y_train, batch_size=batch_size)\\n\",         \"    \\n\",         \"    # Callbacks pour am\u00e9liorer l'entra\u00eenement\\n\",         \"    callbacks = []\\n\",         \"    if epochs &gt; 5:\\n\",         \"        callbacks = [\\n\",         \"            EarlyStopping(patience=5, restore_best_weights=True),\\n\",         \"            ReduceLROnPlateau(factor=0.2, patience=3, min_lr=0.0001)\\n\",         \"        ]\\n\",         \"    \\n\",         \"    # Mesure du temps d'entra\u00eenement\\n\",         \"    start_time = time.time()\\n\",         \"    \\n\",         \"    # Entra\u00eenement du mod\u00e8le\\n\",         \"    if data_augmentation:\\n\",         \"        history = model.fit(\\n\",         \"            train_generator,\\n\",         \"            epochs=epochs,\\n\",         \"            steps_per_epoch=len(x_train) // batch_size,\\n\",         \"            validation_data=(x_test, y_test),\\n\",         \"            callbacks=callbacks,\\n\",         \"            verbose=1\\n\",         \"        )\\n\",         \"    else:\\n\",         \"        history = model.fit(\\n\",         \"            x_train, y_train,\\n\",         \"            batch_size=batch_size,\\n\",         \"            epochs=epochs,\\n\",         \"            validation_data=(x_test, y_test),\\n\",         \"            callbacks=callbacks,\\n\",         \"            verbose=1\\n\",         \"        )\\n\",         \"    \\n\",         \"    training_time = time.time() - start_time\\n\",         \"    \\n\",         \"    # \u00c9valuation du mod\u00e8le\\n\",         \"    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\\n\",         \"    \\n\",         \"    # Pr\u00e9parer les m\u00e9triques\\n\",         \"    metrics = {\\n\",         \"        'test_accuracy': test_acc * 100,\\n\",         \"        'test_loss': test_loss,\\n\",         \"        'training_time': training_time,\\n\",         \"        'epochs_completed': len(history.history['loss']),\\n\",         \"        'history': history\\n\",         \"    }\\n\",         \"    \\n\",         \"    return metrics\\n\",         \"\\n\",         \"def plot_training_history(history):\\n\",         \"    \\\"\\\"\\\"Visualise l'historique d'entra\u00eenement\\\"\\\"\\\"\\n\",         \"    plt.figure(figsize=(12, 5))\\n\",         \"    \\n\",         \"    # Graphique de pr\u00e9cision\\n\",         \"    plt.subplot(1, 2, 1)\\n\",         \"    plt.plot(history.history['accuracy'], label='Entra\u00eenement')\\n\",         \"    plt.plot(history.history['val_accuracy'], label='Validation')\\n\",         \"    plt.title('\u00c9volution de la pr\u00e9cision')\\n\",         \"    plt.xlabel('\u00c9poque')\\n\",         \"    plt.ylabel('Pr\u00e9cision')\\n\",         \"    plt.legend()\\n\",         \"    plt.grid(True, linestyle='--', alpha=0.6)\\n\",         \"    \\n\",         \"    # Graphique de perte\\n\",         \"    plt.subplot(1, 2, 2)\\n\",         \"    plt.plot(history.history['loss'], label='Entra\u00eenement')\\n\",         \"    plt.plot(history.history['val_loss'], label='Validation')\\n\",         \"    plt.title('\u00c9volution de la perte')\\n\",         \"    plt.xlabel('\u00c9poque')\\n\",         \"    plt.ylabel('Perte')\\n\",         \"    plt.legend()\\n\",         \"    plt.grid(True, linestyle='--', alpha=0.6)\\n\",         \"    \\n\",         \"    plt.tight_layout()\\n\",         \"    plt.show()\\n\",         \"\\n\",         \"def plot_confusion_matrix(model, x_test, y_test):\\n\",         \"    \\\"\\\"\\\"Visualise la matrice de confusion du mod\u00e8le\\\"\\\"\\\"\\n\",         \"    # Pr\u00e9dictions\\n\",         \"    y_pred = model.predict(x_test)\\n\",         \"    y_pred_classes = np.argmax(y_pred, axis=1)\\n\",         \"    \\n\",         \"    # Calculer la matrice de confusion\\n\",         \"    conf_mat = confusion_matrix(y_test, y_pred_classes)\\n\",         \"    \\n\",         \"    # Visualisation\\n\",         \"    plt.figure(figsize=(10, 8))\\n\",         \"    sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\\n\",         \"                xticklabels=class_names,\\n\",         \"                yticklabels=class_names)\\n\",         \"    plt.xlabel('Pr\u00e9dit')\\n\",         \"    plt.ylabel('R\u00e9el')\\n\",         \"    plt.title('Matrice de confusion')\\n\",         \"    plt.xticks(rotation=45, ha='right')\\n\",         \"    plt.tight_layout()\\n\",         \"    plt.show()\\n\",         \"\\n\",         \"def show_misclassified_examples(model, x_test, y_test, n=10):\\n\",         \"    \\\"\\\"\\\"Affiche des exemples d'images mal classifi\u00e9es\\\"\\\"\\\"\\n\",         \"    predictions = model.predict(x_test)\\n\",         \"    predicted_classes = np.argmax(predictions, axis=1)\\n\",         \"    \\n\",         \"    # Trouver les erreurs\\n\",         \"    errors = (predicted_classes != y_test)\\n\",         \"    error_indices = np.where(errors)[0]\\n\",         \"    \\n\",         \"    if len(error_indices) == 0:\\n\",         \"        print(\\\"Aucune erreur trouv\u00e9e!\\\")\\n\",         \"        return\\n\",         \"    \\n\",         \"    # S\u00e9lectionner un \u00e9chantillon d'erreurs\\n\",         \"    sample_size = min(n, len(error_indices))\\n\",         \"    sample_indices = np.random.choice(error_indices, size=sample_size, replace=False)\\n\",         \"    \\n\",         \"    # Afficher les exemples\\n\",         \"    plt.figure(figsize=(15, 3*sample_size//5 + 3))\\n\",         \"    for i, idx in enumerate(sample_indices):\\n\",         \"        plt.subplot(sample_size//5 + 1, 5, i+1)\\n\",         \"        plt.imshow(x_test[idx].reshape(28, 28), cmap='gray')\\n\",         \"        plt.title(f\\\"R\u00e9el: {class_names[y_test[idx]]}\\\\nPr\u00e9dit: {class_names[predicted_classes[idx]]}\\\")\\n\",         \"        plt.axis('off')\\n\",         \"    plt.tight_layout()\\n\",         \"    plt.show()\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 5. Mod\u00e8le de base (sous-optimal)\\n\",         \"\\n\",         \"Commen\u00e7ons par cr\u00e9er et \u00e9valuer un mod\u00e8le CNN de base, volontairement sous-optimal, qui servira de point de r\u00e9f\u00e9rence pour nos am\u00e9liorations.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"def create_baseline_model():\\n\",         \"    \\\"\\\"\\\"Cr\u00e9e un mod\u00e8le CNN de base volontairement sous-performant\\\"\\\"\\\"\\n\",         \"    model = Sequential([\\n\",         \"        Conv2D(8, (3, 3), activation='relu', input_shape=(28, 28, 1)),\\n\",         \"        MaxPooling2D((2, 2)),\\n\",         \"        Flatten(),\\n\",         \"        Dense(16, activation='relu'),\\n\",         \"        Dense(10, activation='softmax')\\n\",         \"    ])\\n\",         \"    \\n\",         \"    model.compile(\\n\",         \"        optimizer=Adam(learning_rate=0.01),  # Learning rate trop \u00e9lev\u00e9\\n\",         \"        loss='sparse_categorical_crossentropy',\\n\",         \"        metrics=['accuracy']\\n\",         \"    )\\n\",         \"    \\n\",         \"    return model\\n\",         \"\\n\",         \"# Cr\u00e9er et afficher le mod\u00e8le de base\\n\",         \"baseline_model = create_baseline_model()\\n\",         \"baseline_model.summary()\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### Entra\u00eenement et \u00e9valuation du mod\u00e8le de base\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"print(\\\"\\\\n--- Mod\u00e8le de base ---\\\")\\n\",         \"baseline_metrics = evaluate_model(baseline_model, x_train, y_train, x_test, y_test, epochs=5)\\n\",         \"print(f\\\"Pr\u00e9cision du mod\u00e8le de base: {baseline_metrics['test_accuracy']:.2f}%\\\")\\n\",         \"print(f\\\"Temps d'entra\u00eenement: {baseline_metrics['training_time']:.2f} secondes\\\")\\n\",         \"\\n\",         \"# Visualiser l'historique d'entra\u00eenement\\n\",         \"plot_training_history(baseline_metrics['history'])\\n\",         \"\\n\",         \"# Ajouter au tableau de bord\\n\",         \"dashboard.add_result(\\\"Mod\u00e8le de base\\\", baseline_metrics, \\n\",         \"                     \\\"CNN simple, peu de filtres, learning rate \u00e9lev\u00e9\\\")\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### Analyse des erreurs du mod\u00e8le de base\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# Afficher la matrice de confusion\\n\",         \"plot_confusion_matrix(baseline_model, x_test, y_test)\\n\",         \"\\n\",         \"# Afficher des exemples d'erreurs\\n\",         \"print(\\\"\\\\nExemples d'erreurs de classification du mod\u00e8le de base:\\\")\\n\",         \"show_misclassified_examples(baseline_model, x_test, y_test)\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### \ud83d\udd0d Diagnostic du mod\u00e8le de base\\n\",         \"\\n\",         \"Avant de passer aux am\u00e9liorations, analysons les probl\u00e8mes du mod\u00e8le de base :\\n\",         \"\\n\",         \"1. **Architecture trop simple** : \\n\",         \"   - Seulement 8 filtres dans la couche de convolution\\n\",         \"   - Une seule couche de convolution\\n\",         \"   - Seulement 16 neurones dans la couche dense\\n\",         \"   \\n\",         \"2. **Optimisation probl\u00e9matique** :\\n\",         \"   - Taux d'apprentissage trop \u00e9lev\u00e9 (0.01)\\n\",         \"   - Pas de r\u00e9gularisation (dropout, etc.)\\n\",         \"   - Nombre d'\u00e9poques potentiellement insuffisant\\n\",         \"   \\n\",         \"3. **Pr\u00e9traitement minimal** :\\n\",         \"   - Pas d'augmentation de donn\u00e9es\\n\",         \"   - Pas de normalisation batch\\n\",         \"\\n\",         \"Ces observations nous guideront dans nos tentatives d'am\u00e9lioration.\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 6. Premi\u00e8re am\u00e9lioration : Architecture plus profonde\\n\",         \"\\n\",         \"Pour notre premi\u00e8re am\u00e9lioration, nous allons :\\n\",         \"- Augmenter le nombre de filtres\\n\",         \"- Ajouter une couche de convolution suppl\u00e9mentaire\\n\",         \"- Augmenter le nombre de neurones dans la couche dense\\n\",         \"- R\u00e9duire le taux d'apprentissage\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"def create_improved_model_1():\\n\",         \"    \\\"\\\"\\\"Premier exemple d'am\u00e9lioration: architecture plus profonde\\\"\\\"\\\"\\n\",         \"    model = Sequential([\\n\",         \"        Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\\n\",         \"        MaxPooling2D((2, 2)),\\n\",         \"        Conv2D(64, (3, 3), activation='relu'),\\n\",         \"        MaxPooling2D((2, 2)),\\n\",         \"        Flatten(),\\n\",         \"        Dense(128, activation='relu'),\\n\",         \"        Dense(10, activation='softmax')\\n\",         \"    ])\\n\",         \"    \\n\",         \"    model.compile(\\n\",         \"        optimizer=Adam(learning_rate=0.001),  # Taux d'apprentissage r\u00e9duit\\n\",         \"        loss='sparse_categorical_crossentropy',\\n\",         \"        metrics=['accuracy']\\n\",         \"    )\\n\",         \"    \\n\",         \"    return model\\n\",         \"\\n\",         \"# Cr\u00e9er et afficher le mod\u00e8le am\u00e9lior\u00e9 1\\n\",         \"improved_model_1 = create_improved_model_1()\\n\",         \"improved_model_1.summary()\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### Entra\u00eenement et \u00e9valuation du mod\u00e8le am\u00e9lior\u00e9 1\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"print(\\\"\\\\n--- Mod\u00e8le am\u00e9lior\u00e9 1 ---\\\")\\n\",         \"improved_metrics_1 = evaluate_model(improved_model_1, x_train, y_train, x_test, y_test, epochs=10)\\n\",         \"print(f\\\"Pr\u00e9cision du mod\u00e8le am\u00e9lior\u00e9 1: {improved_metrics_1['test_accuracy']:.2f}%\\\")\\n\",         \"print(f\\\"Temps d'entra\u00eenement: {improved_metrics_1['training_time']:.2f} secondes\\\")\\n\",         \"\\n\",         \"# Visualiser l'historique d'entra\u00eenement\\n\",         \"plot_training_history(improved_metrics_1['history'])\\n\",         \"\\n\",         \"# Ajouter au tableau de bord\\n\",         \"dashboard.add_result(\\\"Mod\u00e8le am\u00e9lior\u00e9 1\\\", improved_metrics_1, \\n\",         \"                    \\\"Plus de filtres, couche suppl\u00e9mentaire, learning rate plus bas\\\")\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### Analyse des r\u00e9sultats du mod\u00e8le am\u00e9lior\u00e9 1\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# Visualiser les r\u00e9sultats de l'am\u00e9lioration\\n\",         \"print(\\\"Comparaison des mod\u00e8les jusqu'\u00e0 pr\u00e9sent:\\\")\\n\",         \"dashboard.show_results()\\n\",         \"\\n\",         \"# Voir les nouvelles erreurs\\n\",         \"print(\\\"\\\\nExemples d'erreurs apr\u00e8s la premi\u00e8re am\u00e9lioration:\\\")\\n\",         \"show_misclassified_examples(improved_model_1, x_test, y_test)\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 7. Deuxi\u00e8me am\u00e9lioration : R\u00e9gularisation et augmentation de donn\u00e9es\\n\",         \"\\n\",         \"Pour notre deuxi\u00e8me am\u00e9lioration, nous allons :\\n\",         \"- Ajouter du dropout pour \u00e9viter le surapprentissage\\n\",         \"- Int\u00e9grer la normalisation par batch (batch normalization)\\n\",         \"- Utiliser l'augmentation de donn\u00e9es pour am\u00e9liorer la g\u00e9n\u00e9ralisation\\n\",         \"\\n\",         \"### Architecture du mod\u00e8le am\u00e9lior\u00e9 2\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"def create_improved_model_2():\\n\",         \"    \\\"\\\"\\\"Deuxi\u00e8me exemple d'am\u00e9lioration: ajout de dropout et batch normalization\\\"\\\"\\\"\\n\",         \"    model = Sequential([\\n\",         \"        # Premi\u00e8re couche de convolution avec batch normalization\\n\",         \"        Conv2D(32, (3, 3), padding='same', input_shape=(28, 28, 1)),\\n\",         \"        BatchNormalization(),\\n\",         \"        Activation('relu'),\\n\",         \"        MaxPooling2D((2, 2)),\\n\",         \"        \\n\",         \"        # Deuxi\u00e8me couche de convolution avec batch normalization\\n\",         \"        Conv2D(64, (3, 3), padding='same'),\\n\",         \"        BatchNormalization(),\\n\",         \"        Activation('relu'),\\n\",         \"        MaxPooling2D((2, 2)),\\n\",         \"        \\n\",         \"        # Aplatissement\\n\",         \"        Flatten(),\\n\",         \"        \\n\",         \"        # Couche dense avec batch normalization et dropout\\n\",         \"        Dense(128),\\n\",         \"        BatchNormalization(),\\n\",         \"        Activation('relu'),\\n\",         \"        Dropout(0.5),  # 50% de dropout pour la r\u00e9gularisation\\n\",         \"        \\n\",         \"        # Couche de sortie\\n\",         \"        Dense(10, activation='softmax')\\n\",         \"    ])\\n\",         \"    \\n\",         \"    model.compile(\\n\",         \"        optimizer=Adam(learning_rate=0.001),\\n\",         \"        loss='sparse_categorical_crossentropy',\\n\",         \"        metrics=['accuracy']\\n\",         \"    )\\n\",         \"    \\n\",         \"    return model\\n\",         \"\\n\",         \"# Cr\u00e9er et afficher le mod\u00e8le am\u00e9lior\u00e9 2\\n\",         \"improved_model_2 = create_improved_model_2()\\n\",         \"improved_model_2.summary()\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### Entra\u00eenement avec augmentation de donn\u00e9es\\n\",         \"\\n\",         \"Pour cette am\u00e9lioration, nous allons \u00e9galement utiliser l'augmentation de donn\u00e9es qui permet de g\u00e9n\u00e9rer artificiellement plus d'exemples d'entra\u00eenement en appliquant des transformations aux images existantes. Cela am\u00e9liore la robustesse du mod\u00e8le face aux variations qu'il pourrait rencontrer en conditions r\u00e9elles.\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"print(\\\"\\\\n--- Mod\u00e8le am\u00e9lior\u00e9 2 (avec augmentation de donn\u00e9es) ---\\\")\\n\",         \"improved_metrics_2 = evaluate_model(improved_model_2, x_train, y_train, x_test, y_test, \\n\",         \"                                   epochs=15, data_augmentation=True)\\n\",         \"print(f\\\"Pr\u00e9cision du mod\u00e8le am\u00e9lior\u00e9 2: {improved_metrics_2['test_accuracy']:.2f}%\\\")\\n\",         \"print(f\\\"Temps d'entra\u00eenement: {improved_metrics_2['training_time']:.2f} secondes\\\")\\n\",         \"\\n\",         \"# Visualiser l'historique d'entra\u00eenement\\n\",         \"plot_training_history(improved_metrics_2['history'])\\n\",         \"\\n\",         \"# Ajouter au tableau de bord\\n\",         \"dashboard.add_result(\\\"Mod\u00e8le am\u00e9lior\u00e9 2\\\", improved_metrics_2, \\n\",         \"                    \\\"Dropout, BatchNorm, augmentation de donn\u00e9es\\\")\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"### Analyse des r\u00e9sultats du mod\u00e8le am\u00e9lior\u00e9 2\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": [         \"# Visualiser la matrice de confusion\\n\",         \"plot_confusion_matrix(improved_model_2, x_test, y_test)\\n\",         \"\\n\",         \"# Afficher des exemples d'erreurs\\n\",         \"print(\\\"\\\\nExemples d'erreurs apr\u00e8s la deuxi\u00e8me am\u00e9lioration:\\\")\\n\",         \"show_misclassified_examples(improved_model_2, x_test, y_test)\\n\",         \"\\n\",         \"# Comparer tous les mod\u00e8les\\n\",         \"dashboard.show_results()\\n\",         \"dashboard.plot_comparison()\"       ]     },     {       \"cell_type\": \"markdown\",       \"metadata\": {},       \"source\": [         \"## 8. Cr\u00e9ation de votre propre mod\u00e8le am\u00e9lior\u00e9\\n\",         \"\\n\",         \"C'est maintenant \u00e0 vous de concevoir votre propre am\u00e9lioration! Vous pouvez explorer diff\u00e9rentes architectures, techniques d'optimisation, ou combinaisons d'approches.\\n\",         \"\\n\",         \"Voici quelques pistes d'am\u00e9lioration possibles:\\n\",         \"- Essayer diff\u00e9rentes architectures (plus/moins de couches, filtres, etc.)\\n\",         \"- Exp\u00e9rimenter avec d'autres optimiseurs (RMSprop, SGD avec momentum, etc.)\\n\",         \"- Tester diff\u00e9rentes techniques de r\u00e9gularisation\\n\",         \"- Modifier les param\u00e8tres d'augmentation de donn\u00e9es\\n\",         \"- Utiliser des connexions r\u00e9siduelles (comme dans les architectures ResNet)\\n\",         \"- Combiner les meilleures pratiques des mod\u00e8les pr\u00e9c\u00e9dents\"       ]     }, .\\n\",         \"        \\n\",         \"        # Couche de sortie\\n\",         \"        Dense(10, activation='softmax')\\n\",         \"    ])\\n\",         \"    \\n\",         \"    # Compilation\\n\",         \"    model.compile(\\n\",         \"        optimizer='adam',  # Modifiez selon vos pr\u00e9f\u00e9rences\\n\",         \"        loss='sparse_categorical_crossentropy',\\n\",         \"        metrics=['accuracy']\\n\",         \"    )\\n\",         \"    \\n\",         \"    return model\\n\",         \"\\n\",         \"# Si vous \u00eates pr\u00eat \u00e0 tester votre mod\u00e8le, d\u00e9commentez les lignes suivantes\\n\",         \"#your_model = create_your_improved_model()\\n\",         \"#your_model.summary()\"       ]     },\"       ]     },     {       \"cell_type\": \"code\",       \"execution_count\": null,       \"metadata\": {},       \"outputs\": [],       \"source\": ["}]}