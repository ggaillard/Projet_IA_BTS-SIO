{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "  \"cells\": [\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"# CNN pour la classification d'images - MNIST\\n\",\n",
    "        \"\\n\",\n",
    "        \"## BTS SIO  - Séance 2: Types de réseaux de neurones\\n\",\n",
    "        \"\\n\",\n",
    "        \"Ce notebook vous guidera à travers l'implémentation et l'utilisation d'un réseau de neurones convolutif (CNN) pour la classification d'images, en utilisant le célèbre dataset MNIST des chiffres manuscrits.\\n\",\n",
    "        \"\\n\",\n",
    "        \"### Objectifs d'apprentissage:\\n\",\n",
    "        \"- Comprendre l'architecture d'un réseau convolutif (CNN)\\n\",\n",
    "        \"- Implémenter un CNN avec TensorFlow/Keras\\n\",\n",
    "        \"- Visualiser les filtres et feature maps\\n\",\n",
    "        \"- Analyser les performances du modèle\\n\",\n",
    "        \"\\n\",\n",
    "        \"### Prérequis:\\n\",\n",
    "        \"- Connaissances de base en Python\\n\",\n",
    "        \"- Avoir suivi la séance 1 d'introduction au Deep Learning\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 1. Configuration de l'environnement\\n\",\n",
    "        \"\\n\",\n",
    "        \"Commençons par importer les bibliothèques nécessaires.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"import numpy as np\\n\",\n",
    "        \"import matplotlib.pyplot as plt\\n\",\n",
    "        \"import tensorflow as tf\\n\",\n",
    "        \"from tensorflow.keras.models import Sequential\\n\",\n",
    "        \"from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\\n\",\n",
    "        \"from tensorflow.keras.utils import to_categorical\\n\",\n",
    "        \"from tensorflow.keras.datasets import mnist\\n\",\n",
    "        \"import time\\n\",\n",
    "        \"import seaborn as sns\\n\",\n",
    "        \"from sklearn.metrics import confusion_matrix\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Configuration pour reproductibilité\\n\",\n",
    "        \"np.random.seed(42)\\n\",\n",
    "        \"tf.random.set_seed(42)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Vérifier la version de TensorFlow\\n\",\n",
    "        \"print(f\\\"TensorFlow version: {tf.__version__}\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 2. Chargement et préparation du dataset MNIST\\n\",\n",
    "        \"\\n\",\n",
    "        \"Le dataset MNIST contient 70,000 images de chiffres manuscrits de taille 28x28 pixels.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"print(\\\"Chargement des données MNIST...\\\")\\n\",\n",
    "        \"(X_train, y_train), (X_test, y_test) = mnist.load_data()\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Afficher les dimensions des données\\n\",\n",
    "        \"print(f\\\"Dimensions de X_train: {X_train.shape}\\\")\\n\",\n",
    "        \"print(f\\\"Dimensions de y_train: {y_train.shape}\\\")\\n\",\n",
    "        \"print(f\\\"Dimensions de X_test: {X_test.shape}\\\")\\n\",\n",
    "        \"print(f\\\"Dimensions de y_test: {y_test.shape}\\\")\\n\",\n",
    "        \"print(f\\\"Nombre de classes: {len(np.unique(y_train))}\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### Préparation des données pour le CNN\\n\",\n",
    "        \"\\n\",\n",
    "        \"Pour utiliser nos images avec un CNN, nous devons :\\n\",\n",
    "        \"1. Ajouter une dimension pour le canal (les images sont en niveaux de gris, donc 1 seul canal)\\n\",\n",
    "        \"2. Normaliser les valeurs de pixels entre 0 et 1\\n\",\n",
    "        \"3. Convertir les étiquettes en format one-hot encoding\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Redimensionnement et normalisation\\n\",\n",
    "        \"X_train = X_train.reshape(-1, 28, 28, 1) / 255.0\\n\",\n",
    "        \"X_test = X_test.reshape(-1, 28, 28, 1) / 255.0\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Conversion des étiquettes en catégories one-hot\\n\",\n",
    "        \"y_train_onehot = to_categorical(y_train, 10)\\n\",\n",
    "        \"y_test_onehot = to_categorical(y_test, 10)\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(f\\\"Nouvelle forme de X_train: {X_train.shape}\\\")\\n\",\n",
    "        \"print(f\\\"Nouvelle forme de y_train_onehot: {y_train_onehot.shape}\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### Visualisation de quelques exemples\\n\",\n",
    "        \"\\n\",\n",
    "        \"Regardons à quoi ressemblent nos données.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"plt.figure(figsize=(10, 5))\\n\",\n",
    "        \"for i in range(10):\\n\",\n",
    "        \"    plt.subplot(2, 5, i+1)\\n\",\n",
    "        \"    plt.imshow(X_train[i].reshape(28, 28), cmap='gray')\\n\",\n",
    "        \"    plt.title(f\\\"Chiffre: {y_train[i]}\\\")\\n\",\n",
    "        \"    plt.axis('off')\\n\",\n",
    "        \"plt.tight_layout()\\n\",\n",
    "        \"plt.show()\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 3. Création d'un modèle CNN\\n\",\n",
    "        \"\\n\",\n",
    "        \"Un CNN est un type de réseau de neurones spécialisé pour traiter des données ayant une structure en grille, comme les images. Les principales couches sont :\\n\",\n",
    "        \"\\n\",\n",
    "        \"1. **Couches de convolution (Conv2D)** : Détectent des caractéristiques locales (lignes, formes...)\\n\",\n",
    "        \"2. **Couches de pooling (MaxPooling2D)** : Réduisent la dimension des données\\n\",\n",
    "        \"3. **Couches denses (Dense)** : Effectuent la classification finale\\n\",\n",
    "        \"\\n\",\n",
    "        \"Nous allons créer un CNN simple avec 2 couches de convolution pour classifier les chiffres MNIST.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Créer un modèle CNN\\n\",\n",
    "        \"model = Sequential([\\n\",\n",
    "        \"    # Première couche de convolution\\n\",\n",
    "        \"    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), name='conv1'),\\n\",\n",
    "        \"    MaxPooling2D((2, 2), name='pool1'),\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Deuxième couche de convolution\\n\",\n",
    "        \"    Conv2D(64, (3, 3), activation='relu', name='conv2'),\\n\",\n",
    "        \"    MaxPooling2D((2, 2), name='pool2'),\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Aplatissement pour passer aux couches denses\\n\",\n",
    "        \"    Flatten(name='flatten'),\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Couches denses (fully connected)\\n\",\n",
    "        \"    Dense(128, activation='relu', name='dense1'),\\n\",\n",
    "        \"    Dropout(0.5, name='dropout1'),  # Évite le surapprentissage\\n\",\n",
    "        \"    Dense(10, activation='softmax', name='output')  # 10 classes (chiffres 0-9)\\n\",\n",
    "        \"])\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Compiler le modèle\\n\",\n",
    "        \"model.compile(\\n\",\n",
    "        \"    optimizer='adam',\\n\",\n",
    "        \"    loss='categorical_crossentropy',\\n\",\n",
    "        \"    metrics=['accuracy']\\n\",\n",
    "        \")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Afficher le résumé de l'architecture\\n\",\n",
    "        \"model.summary()\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 4. Entraînement du modèle\\n\",\n",
    "        \"\\n\",\n",
    "        \"Entraînons maintenant notre CNN sur les données MNIST.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Entraînement du modèle\\n\",\n",
    "        \"start_time = time.time()\\n\",\n",
    "        \"\\n\",\n",
    "        \"history = model.fit(\\n\",\n",
    "        \"    X_train, \\n\",\n",
    "        \"    y_train_onehot, \\n\",\n",
    "        \"    batch_size=128, \\n\",\n",
    "        \"    epochs=5,  # Nombre réduit d'époques pour la démonstration\\n\",\n",
    "        \"    validation_split=0.2,  # 20% des données d'entraînement pour la validation\\n\",\n",
    "        \"    verbose=1\\n\",\n",
    "        \")\\n\",\n",
    "        \"\\n\",\n",
    "        \"training_time = time.time() - start_time\\n\",\n",
    "        \"print(f\\\"\\\\nTemps d'entraînement: {training_time:.2f} secondes\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### Visualisation de l'évolution de l'entraînement\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"plt.figure(figsize=(12, 4))\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Graphique de précision\\n\",\n",
    "        \"plt.subplot(1, 2, 1)\\n\",\n",
    "        \"plt.plot(history.history['accuracy'], label='Entraînement')\\n\",\n",
    "        \"plt.plot(history.history['val_accuracy'], label='Validation')\\n\",\n",
    "        \"plt.title('Évolution de la précision')\\n\",\n",
    "        \"plt.xlabel('Époque')\\n\",\n",
    "        \"plt.ylabel('Précision')\\n\",\n",
    "        \"plt.legend()\\n\",\n",
    "        \"plt.grid(True, linestyle='--', alpha=0.6)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Graphique de perte\\n\",\n",
    "        \"plt.subplot(1, 2, 2)\\n\",\n",
    "        \"plt.plot(history.history['loss'], label='Entraînement')\\n\",\n",
    "        \"plt.plot(history.history['val_loss'], label='Validation')\\n\",\n",
    "        \"plt.title('Évolution de la perte')\\n\",\n",
    "        \"plt.xlabel('Époque')\\n\",\n",
    "        \"plt.ylabel('Perte')\\n\",\n",
    "        \"plt.legend()\\n\",\n",
    "        \"plt.grid(True, linestyle='--', alpha=0.6)\\n\",\n",
    "        \"\\n\",\n",
    "        \"plt.tight_layout()\\n\",\n",
    "        \"plt.show()\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 5. Évaluation du modèle\\n\",\n",
    "        \"\\n\",\n",
    "        \"Évaluons notre modèle sur l'ensemble de test.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Évaluation sur l'ensemble de test\\n\",\n",
    "        \"test_loss, test_acc = model.evaluate(X_test, y_test_onehot, verbose=1)\\n\",\n",
    "        \"print(f\\\"Précision sur l'ensemble de test: {test_acc*100:.2f}%\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Prédictions\\n\",\n",
    "        \"y_pred = model.predict(X_test)\\n\",\n",
    "        \"y_pred_classes = np.argmax(y_pred, axis=1)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Matrice de confusion\\n\",\n",
    "        \"conf_mat = confusion_matrix(y_test, y_pred_classes)\\n\",\n",
    "        \"plt.figure(figsize=(10, 8))\\n\",\n",
    "        \"sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues')\\n\",\n",
    "        \"plt.xlabel('Prédit')\\n\",\n",
    "        \"plt.ylabel('Réel')\\n\",\n",
    "        \"plt.title('Matrice de confusion')\\n\",\n",
    "        \"plt.show()\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### Visualisation des exemples mal classifiés\\n\",\n",
    "        \"\\n\",\n",
    "        \"Explorons quelques exemples que notre modèle a mal classifiés.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Identifier les erreurs\\n\",\n",
    "        \"misclassified_indices = np.where(y_pred_classes != y_test)[0]\\n\",\n",
    "        \"misclassified_count = len(misclassified_indices)\\n\",\n",
    "        \"print(f\\\"Nombre total d'erreurs: {misclassified_count} sur {len(y_test)} images de test\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Afficher quelques exemples mal classifiés\\n\",\n",
    "        \"num_examples = min(10, misclassified_count)\\n\",\n",
    "        \"plt.figure(figsize=(15, 6))\\n\",\n",
    "        \"\\n\",\n",
    "        \"for i, idx in enumerate(misclassified_indices[:num_examples]):\\n\",\n",
    "        \"    plt.subplot(2, 5, i+1)\\n\",\n",
    "        \"    plt.imshow(X_test[idx].reshape(28, 28), cmap='gray')\\n\",\n",
    "        \"    plt.title(f\\\"Réel: {y_test[idx]}\\\\nPrédit: {y_pred_classes[idx]}\\\")\\n\",\n",
    "        \"    plt.axis('off')\\n\",\n",
    "        \"    \\n\",\n",
    "        \"plt.tight_layout()\\n\",\n",
    "        \"plt.show()\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### 🧠 Réflexion sur les erreurs\\n\",\n",
    "        \"\\n\",\n",
    "        \"**Question**: En observant les exemples mal classifiés, quelles pourraient être les raisons de ces erreurs? Notez vos observations et hypothèses ci-dessous.\\n\",\n",
    "        \"\\n\",\n",
    "        \"**Points à considérer:**\\n\",\n",
    "        \"- Certains chiffres sont-ils plus souvent confondus que d'autres?\\n\",\n",
    "        \"- Quelles caractéristiques visuelles communes peuvent expliquer les erreurs?\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"*Écrivez vos observations ici...*\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 6. Visualisation des filtres et feature maps\\n\",\n",
    "        \"\\n\",\n",
    "        \"Une des grandes forces des CNNs est leur interprétabilité visuelle. Explorons ce que le réseau \\\"voit\\\" réellement.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Fonction pour visualiser les filtres de convolution\\n\",\n",
    "        \"def visualize_filters(model, layer_name, num_filters=8):\\n\",\n",
    "        \"    \\\"\\\"\\\"Visualise les filtres d'une couche de convolution\\\"\\\"\\\"\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Récupérer les poids du filtre de la couche spécifiée\\n\",\n",
    "        \"    filters, biases = model.get_layer(layer_name).get_weights()\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Normaliser les filtres pour une meilleure visualisation\\n\",\n",
    "        \"    f_min, f_max = filters.min(), filters.max()\\n\",\n",
    "        \"    filters = (filters - f_min) / (f_max - f_min)\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Afficher les premiers filtres\\n\",\n",
    "        \"    plt.figure(figsize=(12, 4))\\n\",\n",
    "        \"    for i in range(num_filters):\\n\",\n",
    "        \"        plt.subplot(2, 4, i+1)\\n\",\n",
    "        \"        # Pour la première couche de convolution, les filtres sont 3D (hauteur, largeur, canaux)\\n\",\n",
    "        \"        # Nous affichons le filtre pour le premier canal (0)\\n\",\n",
    "        \"        plt.imshow(filters[:, :, 0, i], cmap='viridis')\\n\",\n",
    "        \"        plt.title(f'Filtre {i+1}')\\n\",\n",
    "        \"        plt.axis('off')\\n\",\n",
    "        \"    plt.suptitle(f'Filtres de la couche {layer_name}')\\n\",\n",
    "        \"    plt.tight_layout()\\n\",\n",
    "        \"    plt.show()\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Visualiser les filtres de la première couche de convolution\\n\",\n",
    "        \"visualize_filters(model, 'conv1')\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### Visualisation des feature maps (cartes d'activation)\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"def visualize_feature_maps(model, image, layer_name, num_features=8):\\n\",\n",
    "        \"    \\\"\\\"\\\"Visualise les feature maps (activations) d'une couche pour une image donnée\\\"\\\"\\\"\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Créer un modèle qui renvoie les activations de la couche spécifiée\\n\",\n",
    "        \"    layer_model = tf.keras.Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Obtenir les activations pour une image\\n\",\n",
    "        \"    feature_maps = layer_model.predict(image.reshape(1, 28, 28, 1))\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Afficher les premières cartes d'activation\\n\",\n",
    "        \"    plt.figure(figsize=(12, 4))\\n\",\n",
    "        \"    for i in range(min(num_features, feature_maps.shape[3])):\\n\",\n",
    "        \"        plt.subplot(2, 4, i+1)\\n\",\n",
    "        \"        plt.imshow(feature_maps[0, :, :, i], cmap='viridis')\\n\",\n",
    "        \"        plt.title(f'Feature {i+1}')\\n\",\n",
    "        \"        plt.axis('off')\\n\",\n",
    "        \"    plt.suptitle(f'Feature Maps de la couche {layer_name}')\\n\",\n",
    "        \"    plt.tight_layout()\\n\",\n",
    "        \"    plt.show()\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Choisir une image de test\\n\",\n",
    "        \"sample_idx = 12  # Vous pouvez essayer avec différents indices\\n\",\n",
    "        \"sample_image = X_test[sample_idx]\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Afficher l'image originale\\n\",\n",
    "        \"plt.figure(figsize=(3, 3))\\n\",\n",
    "        \"plt.imshow(sample_image.reshape(28, 28), cmap='gray')\\n\",\n",
    "        \"plt.title(f\\\"Chiffre: {y_test[sample_idx]}\\\")\\n\",\n",
    "        \"plt.axis('off')\\n\",\n",
    "        \"plt.show()\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Visualiser les feature maps pour chaque couche de convolution\\n\",\n",
    "        \"print(\\\"Feature maps de la première couche de convolution:\\\")\\n\",\n",
    "        \"visualize_feature_maps(model, sample_image, 'conv1')\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(\\\"Feature maps de la deuxième couche de convolution:\\\")\\n\",\n",
    "        \"visualize_feature_maps(model, sample_image, 'conv2')\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### 💡 Interprétation des feature maps\\n\",\n",
    "        \"\\n\",\n",
    "        \"Les feature maps nous montrent ce que \\\"voit\\\" chaque filtre de convolution :\\n\",\n",
    "        \"\\n\",\n",
    "        \"- **Première couche** : Détecte principalement des caractéristiques de base comme les bords et les contours\\n\",\n",
    "        \"- **Deuxième couche** : Combine ces caractéristiques de base pour détecter des formes plus complexes\\n\",\n",
    "        \"\\n\",\n",
    "        \"Cette hiérarchie de représentations est ce qui rend les CNNs si puissants pour la vision par ordinateur.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 7. Test avec des images bruitées\\n\",\n",
    "        \"\\n\",\n",
    "        \"Testons la robustesse de notre modèle face à des perturbations.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Fonction pour ajouter du bruit aux images\\n\",\n",
    "        \"def add_noise(images, noise_level=0.2):\\n\",\n",
    "        \"    \\\"\\\"\\\"Ajoute du bruit gaussien aux images\\\"\\\"\\\"\\n\",\n",
    "        \"    noisy_images = images.copy()\\n\",\n",
    "        \"    noise = np.random.normal(0, noise_level, images.shape)\\n\",\n",
    "        \"    noisy_images = noisy_images + noise\\n\",\n",
    "        \"    # Assurer que les valeurs restent entre 0 et 1\\n\",\n",
    "        \"    return np.clip(noisy_images, 0, 1)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Créer des versions bruitées de quelques images de test\\n\",\n",
    "        \"num_test_images = 5\\n\",\n",
    "        \"test_samples = X_test[:num_test_images]\\n\",\n",
    "        \"noisy_samples = add_noise(test_samples, noise_level=0.3)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Visualiser les images originales et bruitées\\n\",\n",
    "        \"plt.figure(figsize=(12, 4))\\n\",\n",
    "        \"for i in range(num_test_images):\\n\",\n",
    "        \"    # Image originale\\n\",\n",
    "        \"    plt.subplot(2, num_test_images, i+1)\\n\",\n",
    "        \"    plt.imshow(test_samples[i].reshape(28, 28), cmap='gray')\\n\",\n",
    "        \"    plt.title(f\\\"Original: {y_test[i]}\\\")\\n\",\n",
    "        \"    plt.axis('off')\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Image bruitée\\n\",\n",
    "        \"    plt.subplot(2, num_test_images, i+num_test_images+1)\\n\",\n",
    "        \"    plt.imshow(noisy_samples[i].reshape(28, 28), cmap='gray')\\n\",\n",
    "        \"    plt.axis('off')\\n\",\n",
    "        \"    \\n\",\n",
    "        \"plt.tight_layout()\\n\",\n",
    "        \"plt.show()\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Prédire sur les images bruitées\\n\",\n",
    "        \"noisy_predictions = model.predict(noisy_samples)\\n\",\n",
    "        \"noisy_pred_classes = np.argmax(noisy_predictions, axis=1)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Afficher les résultats\\n\",\n",
    "        \"print(\\\"Résultats des prédictions sur les images bruitées:\\\")\\n\",\n",
    "        \"for i in range(num_test_images):\\n\",\n",
    "        \"    status = \\\"✓\\\" if noisy_pred_classes[i] == y_test[i] else \\\"✗\\\"\\n\",\n",
    "        \"    print(f\\\"Image {i+1} - Réel: {y_test[i]}, Prédit: {noisy_pred_classes[i]} {status}\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Calculer la précision sur les images bruitées\\n\",\n",
    "        \"accuracy_on_noisy = np.mean(noisy_pred_classes == y_test[:num_test_images]) * 100\\n\",\n",
    "        \"print(f\\\"\\\\nPrécision sur les images bruitées: {accuracy_on_noisy:.1f}%\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 8. Exercice : Amélioration du modèle\\n\",\n",
    "        \"\\n\",\n",
    "        \"À vous de jouer ! Essayez de modifier l'architecture du modèle pour améliorer ses performances. Voici quelques suggestions :\\n\",\n",
    "        \"\\n\",\n",
    "        \"1. Ajouter plus de couches de convolution\\n\",\n",
    "        \"2. Modifier le nombre de filtres\\n\",\n",
    "        \"3. Changer la taille des filtres\\n\",\n",
    "        \"4. Ajuster les paramètres d'entraînement (epochs, batch_size)\\n\",\n",
    "        \"\\n\",\n",
    "        \"Copiez le code de création du modèle ci-dessous et modifiez-le :\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# VOTRE CODE ICI\\n\",\n",
    "        \"# Créez votre propre modèle amélioré\\n\",\n",
    "        \"\\n\",\n",
    "        \"improved_model = Sequential([\\n\",\n",
    "        \"    # Modifiez l'architecture ici\\n\",\n",
    "        \"    \\n\",\n",
    "        \"])\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Compiler le modèle\\n\",\n",
    "        \"improved_model.compile(\\n\",\n",
    "        \"    optimizer='adam',\\n\",\n",
    "        \"    loss='categorical_crossentropy',\\n\",\n",
    "        \"    metrics=['accuracy']\\n\",\n",
    "        \")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Afficher le résumé\\n\",\n",
    "        \"improved_model.summary()\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Entraînez votre modèle amélioré\\n\",\n",
    "        \"# history = improved_model.fit(...)\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 9. Conclusion\\n\",\n",
    "        \"\\n\",\n",
    "        \"Dans ce notebook, vous avez :\\n\",\n",
    "        \"- Créé et entraîné un réseau de neurones convolutif (CNN) pour la classification d'images\\n\",\n",
    "        \"- Visualisé les filtres et les feature maps pour comprendre ce que \\\"voit\\\" le réseau\\n\",\n",
    "        \"- Évalué les performances du modèle et sa robustesse face au bruit\\n\",\n",
    "        \"\\n\",\n",
    "        \"Les CNN sont la base de nombreuses applications modernes de vision par ordinateur comme la reconnaissance faciale, la détection d'objets, et bien d'autres.\"\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"metadata\": {\n",
    "    \"kernelspec\": {\n",
    "      \"display_name\": \"Python 3\",\n",
    "      \"language\": \"python\",\n",
    "      \"name\": \"python3\"\n",
    "    },\n",
    "    \"language_info\": {\n",
    "      \"codemirror_mode\": {\n",
    "        \"name\": \"ipython\",\n",
    "        \"version\": 3\n",
    "      },\n",
    "      \"file_extension\": \".py\",\n",
    "      \"mimetype\": \"text/x-python\",\n",
    "      \"name\": \"python\",\n",
    "      \"nbconvert_exporter\": \"python\",\n",
    "      \"pygments_lexer\": \"ipython3\",\n",
    "      \"version\": \"3.8.5\"\n",
    "    }\n",
    "  },\n",
    "  \"nbformat\": 4,\n",
    "  \"nbformat_minor\": 4\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
