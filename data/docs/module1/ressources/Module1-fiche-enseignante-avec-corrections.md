# üìã Fiche Enseignante - Module 1 : Fondamentaux du Deep Learning

## Pr√©sentation g√©n√©rale du module

**Dur√©e totale** : 4 heures  
**Public cible** : √âtudiants BTS SIO (d√©butants en Deep Learning)  
**Pr√©requis** : Bases en programmation Python, compte Google pour Colab

**Approche p√©dagogique** : Apprentissage par la pratique d'abord, conceptualisation ensuite

## Objectifs d'apprentissage

√Ä l'issue de ce module, les √©tudiants seront capables de :
1. Manipuler un r√©seau de neurones simple via TensorFlow/Keras
2. Distinguer les diff√©rences entre Machine Learning classique et Deep Learning
3. Comprendre les concepts fondamentaux (couches, fonction d'activation, propagation)
4. Modifier et analyser un mod√®le de Deep Learning simple

## Organisation du module

### Phase 1 : Introduction pratique (1h)

#### Objectifs sp√©cifiques
- Cr√©er un premier contact positif avec le Deep Learning
- Manipuler un r√©seau de neurones sans barri√®re th√©orique pr√©alable
- Observer concr√®tement le fonctionnement et les performances d'un mod√®le

#### D√©roulement et conseils d'animation

| Dur√©e | Activit√© | Conseils pour l'enseignant |
|-------|----------|----------------------------|
| 15 min | **D√©monstrations** applications DL<br>‚Ä¢ GitHub Copilot<br>‚Ä¢ Reconnaissance d'objets<br>‚Ä¢ G√©n√©ration de texte | ‚Ä¢ Choisir des exemples spectaculaires et accessibles<br>‚Ä¢ Associer les √©tudiants via questions<br>‚Ä¢ √âtablir des liens avec des applications r√©elles |
| 30 min | **Manipulation guid√©e** notebook "Hello World"<br>‚Ä¢ Ex√©cution pas √† pas<br>‚Ä¢ Observation des performances<br>‚Ä¢ Premi√®res exp√©rimentations | ‚Ä¢ S'assurer que tous les √©tudiants ont acc√®s √† Colab<br>‚Ä¢ Circuler pour aider aux probl√®mes techniques<br>‚Ä¢ Encourager l'observation et le questionnement |
| 15 min | **Remplissage** fiche d'observations<br>‚Ä¢ Analyse des r√©sultats<br>‚Ä¢ Documentation des observations | ‚Ä¢ Insister sur l'importance de la documentation<br>‚Ä¢ Encourager la pr√©cision des observations<br>‚Ä¢ Pr√©voir un temps de mise en commun |

#### √âl√©ments de correction pour la Fiche d'observations - Phase 1

| Question | √âl√©ments de r√©ponse attendus |
|----------|------------------------------|
| Version de TensorFlow d√©tect√©e | TensorFlow 2.x (la version exacte d√©pend de Colab) |
| GPU disponible ? | Oui (g√©n√©ralement dans Colab) |
| Importance du GPU pour le Deep Learning | Acc√©l√©ration consid√©rable de l'entra√Ænement gr√¢ce au calcul parall√®le des GPU, permettant de traiter des mod√®les plus grands et plus complexes |
| Nombre d'exemples d'entra√Ænement | 60 000 images |
| Nombre d'exemples de test | 10 000 images |
| Dimension des images | 28√ó28 pixels (784 pixels au total) |
| Pourquoi normalise-t-on les valeurs? | Pour ramener toutes les valeurs entre 0 et 1, ce qui facilite la convergence du mod√®le et √©vite les probl√®mes num√©riques |
| Difficult√©s potentielles | Variabilit√© des styles d'√©criture, similarit√© entre certains chiffres (ex: 1 et 7, 3 et 8), qualit√© variable du trac√©, positionnement non centr√© |
| Nombre de couches du mod√®le | 4-5 couches (entr√©e, convolutions, pooling, dense, sortie) |
| Nombre total de param√®tres | Entre 500 000 et 1 500 000 selon l'architecture exacte |
| R√¥le des couches de convolution | Extraction de caract√©ristiques locales (bords, contours, motifs) ind√©pendamment de leur position |
| R√¥le des couches de pooling | R√©duction de dimension, invariance aux petites translations, abstraction des caract√©ristiques |
| Pourquoi utiliser 'softmax'? | Pour obtenir une distribution de probabilit√©s sur les 10 classes (somme = 1) |
| Nombre d'√©poques | 5-10 g√©n√©ralement |
| Pr√©cision sur donn√©es d'entra√Ænement | ~99% |
| Pr√©cision sur donn√©es de validation | ~98% |
| Pr√©cision sur l'ensemble de test | ~97-98% |
| Signes de surapprentissage | √âcart entre pr√©cision d'entra√Ænement et de validation qui se creuse au fil des √©poques |
| La courbe de pr√©cision est-elle croissante? | Oui, avec une augmentation rapide au d√©but puis une stabilisation |
| La courbe de perte est-elle d√©croissante? | Oui, avec une diminution rapide au d√©but puis une stabilisation |
| √âcart entre courbes d'entra√Ænement et validation | Faible √† mod√©r√©, ce qui indique une bonne g√©n√©ralisation |
| Entra√Ænement suffisant? | Oui, si les courbes se stabilisent. Plus d'√©poques n'apporterait que peu d'am√©lioration |

### Phase 2 : Concepts fondamentaux (1h30)

#### Objectifs sp√©cifiques
- Comprendre les diff√©rences entre ML classique et Deep Learning
- Explorer l'anatomie d'un r√©seau de neurones
- Saisir les concepts de forward/backward propagation

#### D√©roulement et conseils d'animation

| Dur√©e | Activit√© | Conseils pour l'enseignant |
|-------|----------|----------------------------|
| 30 min | **Comparaison** ML/DL<br>‚Ä¢ Exploration des deux approches<br>‚Ä¢ Test sur donn√©es normales/bruit√©es | ‚Ä¢ Insister sur les diff√©rences fondamentales (features engineered vs learned)<br>‚Ä¢ Laisser les √©tudiants d√©couvrir par eux-m√™mes les forces/faiblesses |
| 45 min | **Exploration interactive**<br>‚Ä¢ Neurone unique<br>‚Ä¢ R√©seau simple<br>‚Ä¢ Visualisation de l'entra√Ænement | ‚Ä¢ Utiliser des analogies concr√®tes (ex: neurone comme d√©tecteur de motifs)<br>‚Ä¢ Favoriser la manipulation et l'exp√©rimentation<br>‚Ä¢ Poser des questions guid√©es pour la r√©flexion |
| 15 min | **Sch√©ma conceptuel**<br>‚Ä¢ Compl√©tion collaborative<br>‚Ä¢ Discussion des concepts | ‚Ä¢ Synth√©tiser les observations<br>‚Ä¢ Formaliser progressivement les concepts<br>‚Ä¢ V√©rifier la compr√©hension par des questions |

#### √âl√©ments de correction pour la Fiche d'observations - Phase 2

**Comparaison Machine Learning vs Deep Learning**

| Aspect observ√© | Machine Learning (Random Forest) | Deep Learning (CNN) |
|----------------|----------------------------------|---------------------|
| Pr√©paration des donn√©es | N√©cessite un pr√©traitement important et une extraction manuelle de caract√©ristiques | Travaille directement sur les donn√©es brutes (pixels) |
| Extraction de caract√©ristiques | Manuelle, n√©cessite expertise du domaine | Automatique, apprend les caract√©ristiques pertinentes |
| Temps d'entra√Ænement | Relativement rapide (quelques secondes √† minutes) | Plus long (minutes √† heures), n√©cessite souvent un GPU |
| Pr√©cision globale | Bonne (~95-96%) | Excellente (~98-99%) |
| Pr√©cision sur donn√©es bruit√©es | Faible √† moyenne, sensible au bruit | Bonne, plus robuste aux variations |
| Pr√©cision sur donn√©es avec rotation | Tr√®s faible, ne g√®re pas les rotations | Mod√©r√©e √† bonne selon l'entra√Ænement |
| Facilit√© d'impl√©mentation | Plus simple, moins de param√®tres √† r√©gler | Plus complexe, plus d'hyperparam√®tres |
| Interpr√©tabilit√© | Plus interpr√©table (r√®gles de d√©cision explicites) | Moins interpr√©table ("bo√Æte noire") |
| Capacit√© de g√©n√©ralisation | Limit√©e aux caract√©ristiques explicites | Meilleure sur des motifs complexes et variations |

**Sch√©ma conceptuel du r√©seau de neurones**

1. Couche d'entr√©e (Input Layer)
2. Premi√®re couche cach√©e (Hidden Layer 1)
3. Deuxi√®me couche cach√©e (Hidden Layer 2)
4. Couche de sortie (Output Layer)
5. Pr√©diction (Prediction)
6. Calcul de l'erreur (Loss Calculation)
7. Donn√©es r√©elles (Ground Truth)

**Structure du r√©seau**

1. Type de r√©seau repr√©sent√©: R√©seau de neurones multicouche (MLP) ou perceptron multicouche

2. Nombre de neurones pour MNIST:
   - Couche d'entr√©e: 784 (28√ó28 pixels)
   - Premi√®re couche cach√©e: 128-512 (variable selon architecture)
   - Deuxi√®me couche cach√©e: 64-256 (variable selon architecture)
   - Couche de sortie: 10 (un neurone par chiffre 0-9)

3. Fonctions d'activation appropri√©es:
   - Couches cach√©es: ReLU (Rectified Linear Unit)
   - Couche de sortie: Softmax (pour obtenir des probabilit√©s)

**Processus d'apprentissage**

1. Forward propagation: Les donn√©es d'entr√©e sont propag√©es √† travers le r√©seau pour produire une pr√©diction
2. Calcul de l'erreur: Comparaison entre la pr√©diction et la valeur r√©elle (ground truth)
3. Backward propagation: L'erreur est propag√©e en arri√®re pour calculer les gradients
4. Mise √† jour des poids: Les param√®tres du r√©seau sont ajust√©s pour minimiser l'erreur

### Phase 3 : Mini-projet individuel (1h)

#### Objectifs sp√©cifiques
- Appliquer les connaissances acquises
- D√©velopper une d√©marche d'am√©lioration m√©thodique
- Analyser l'impact des modifications

#### D√©roulement et conseils d'animation

| Dur√©e | Activit√© | Conseils pour l'enseignant |
|-------|----------|----------------------------|
| 15 min | **Pr√©paration** mod√®le de base<br>‚Ä¢ Configuration notebook<br>‚Ä¢ Analyse du mod√®le initial | ‚Ä¢ Fournir le code de base pr√™t √† l'emploi<br>‚Ä¢ Expliquer clairement la structure du projet<br>‚Ä¢ Rappeler les points d'observation importants |
| 30 min | **Exp√©rimentation** modifications<br>‚Ä¢ Impl√©mentation des changements<br>‚Ä¢ Tests et comparaisons | ‚Ä¢ Sugg√©rer des modifications adapt√©es au niveau<br>‚Ä¢ Encourager la d√©marche scientifique (hypoth√®se‚Üítest‚Üíanalyse)<br>‚Ä¢ Circuler pour guider sans trop diriger |
| 15 min | **Analyse** des r√©sultats<br>‚Ä¢ Documentation des observations<br>‚Ä¢ R√©flexion sur les am√©liorations | ‚Ä¢ Rappeler l'importance de l'analyse critique<br>‚Ä¢ Encourager la comparaison entre √©tudiants<br>‚Ä¢ Valoriser les d√©marches originales |

#### √âl√©ments de correction pour la Fiche d'observations - Phase 3

**Mod√®le de base**

| √âl√©ment | R√©ponse attendue |
|---------|------------------|
| Architecture | CNN simple avec 1-2 couches de convolution, 1-2 couches de pooling, 1 couche dense et 1 couche de sortie |
| Nombre de param√®tres | ~500 000 pour le mod√®le de base propos√© |
| Fonction d'activation | ReLU pour les couches interm√©diaires, Softmax pour la sortie |
| Optimiseur | Adam |
| Pr√©cision du mod√®le de base | ~97-98% sur l'ensemble de test |

**Modifications et leurs impacts**

| Modification | Impact attendu |
|--------------|----------------|
| Ajout d'une couche de convolution | Augmentation de la capacit√© √† d√©tecter des motifs plus complexes. Am√©lioration potentielle de ~0.5-1% de pr√©cision. |
| Augmentation du nombre de neurones | Augmentation de la capacit√© du mod√®le mais risque de surapprentissage. Am√©lioration variable selon le niveau de r√©gularisation. |
| Ajout de Dropout | R√©duction du surapprentissage, possiblement plus robuste. Peut r√©duire l√©g√®rement la performance sur les donn√©es d'entra√Ænement mais am√©liorer sur les donn√©es de test. |
| Changement d'optimiseur | SGD plus lent √† converger qu'Adam mais parfois plus stable. Performance finale similaire mais n√©cessite plus d'√©poques. |
| Augmentation du nombre d'√©poques | Am√©lioration des performances jusqu'√† un plateau. Au-del√†, risque de surapprentissage. |

**Test sur donn√©es bruit√©es**

| Version | Performances attendues |
|---------|------------------------|
| Mod√®le de base | ~70-80% sur donn√©es bruit√©es |
| Mod√®le avec plus de filtres | ~75-85% sur donn√©es bruit√©es |
| Mod√®le avec Dropout | ~80-90% sur donn√©es bruit√©es (g√©n√©ralement plus robuste) |

**√âvaluation des analyses**

Une bonne analyse devrait inclure:
- Identification correcte des modifications ayant le plus d'impact positif
- Compr√©hension de l'effet du Dropout sur la robustesse
- Observation pertinente des types d'erreurs (ex: confusion entre 3/8, 4/9, etc.)
- R√©flexion sur les compromis entre complexit√© du mod√®le et performances

## √âvaluation et suivi

### Livrables √† r√©cup√©rer
- Fiche d'observations Phase 1 : "Hello World du Deep Learning"
- Fiche d'observations Phase 2 : "Concepts fondamentaux"
- Fiche d'observations Phase 3 : "Mini-projet d'am√©lioration"

### Crit√®res d'√©valuation

| Crit√®re | Indicateurs de r√©ussite |
|---------|------------------------|
| **Manipulation technique** | ‚Ä¢ Notebook fonctionnel<br>‚Ä¢ Modifications correctement impl√©ment√©es |
| **Compr√©hension des concepts** | ‚Ä¢ Explication correcte des √©l√©ments du r√©seau<br>‚Ä¢ Sch√©ma conceptuel bien compl√©t√© |
| **Analyse critique** | ‚Ä¢ Observations pertinentes sur les performances<br>‚Ä¢ Identification correcte des forces/faiblesses |
| **Documentation** | ‚Ä¢ Fiches d'observations compl√®tes et pr√©cises<br>‚Ä¢ Justification des choix techniques |

### Bar√®me sugg√©r√© (sur 20 points)

| Livrable | Points | √âl√©ments √©valu√©s |
|----------|--------|------------------|
| Fiche Phase 1 | 6 pts | ‚Ä¢ Compl√©tude (3 pts)<br>‚Ä¢ Pertinence des observations (3 pts) |
| Fiche Phase 2 | 8 pts | ‚Ä¢ Tableau comparatif ML/DL (3 pts)<br>‚Ä¢ Sch√©ma conceptuel correct (3 pts)<br>‚Ä¢ Processus d'apprentissage (2 pts) |
| Fiche Phase 3 | 6 pts | ‚Ä¢ Modifications impl√©ment√©es (2 pts)<br>‚Ä¢ Analyse des r√©sultats (2 pts)<br>‚Ä¢ Pertinence des conclusions (2 pts) |

## Ressources et mat√©riel

### Pour l'enseignant
- Pr√©sentations des concepts cl√©s (neurones, couches, fonctions d'activation)
- Solutions compl√®tes des notebooks
- Exemples d'am√©liorations possibles avec impact attendu
- Glossaire des erreurs courantes et leurs solutions

### Pour les √©tudiants
- Notebooks pr√©-configur√©s
- Fiches d'observations √† compl√©ter
- Guide d'utilisation de Google Colab
- Glossaire des termes techniques

## Adaptations possibles

### Pour les √©tudiants avanc√©s
- Proposer des architectures plus complexes √† explorer
- Sugg√©rer des d√©fis suppl√©mentaires (ex: atteindre une pr√©cision cible)
- Encourager l'exploration de datasets alternatifs

### Pour les √©tudiants en difficult√©
- Fournir des mod√®les pr√©-configur√©s avec modifications √† choisir
- Proposer un travail en bin√¥me
- Simplifier les fiches d'observations avec plus de guidage

## Points de vigilance et conseils

### Difficult√©s techniques courantes
- Probl√®mes d'acc√®s √† Google Colab ‚Üí Pr√©parer un environnement de secours
- Temps d'ex√©cution trop long ‚Üí R√©duire taille du dataset ou nombre d'√©poques
- Erreurs dans le code ‚Üí Pr√©voir des checkpoints de code fonctionnel

### Difficult√©s conceptuelles courantes
- Confusion entre les types de couches ‚Üí Utiliser des analogies visuelles
- Difficult√© √† comprendre la backpropagation ‚Üí Simplifier avec des exemples concrets
- Interpr√©tation des m√©triques ‚Üí Fournir des r√©f√©rences de comparaison

### Gestion du temps
- Pr√©voir une marge pour les probl√®mes techniques
- Adapter le nombre de modifications √† tester selon l'avancement
- Pr√©parer des activit√©s "tampons" pour les plus rapides

## Prolongements possibles

- QCM d'auto-√©valuation pour v√©rifier les acquis
- Exercices compl√©mentaires pour renforcer la compr√©hension
- Suggestions de projets personnels pour prolonger l'apprentissage

---

## Annexe : Concepts cl√©s √† aborder

### Vocabulaire essentiel
- Neurone artificiel
- Poids et biais
- Couches (entr√©e, cach√©e, sortie)
- Fonction d'activation
- Forward propagation
- Backpropagation
- Gradient descent
- Epoch (√©poque)
- Batch
- Loss function

### Diff√©rences ML vs DL √† souligner
- Extraction manuelle vs automatique des caract√©ristiques
- Processus d'entra√Ænement
- Besoins en donn√©es et en calcul
- Domaines d'application privil√©gi√©s

### Architectures √† pr√©senter
- Perceptron multicouche (MLP)
- R√©seau convolutif (CNN) - introduction
- R√©seau r√©current (RNN) - mention

## Annexe : FAQ anticip√©es

**Q: Pourquoi utiliser le Deep Learning plut√¥t que le Machine Learning classique ?**  
R: Le Deep Learning excelle pour les donn√©es complexes (images, texte, son) o√π l'extraction manuelle de caract√©ristiques est difficile. Il peut apprendre des repr√©sentations hi√©rarchiques des donn√©es.

**Q: Comment choisir le nombre de couches et de neurones ?**  
R: C'est souvent empirique. Plus le probl√®me est complexe, plus le r√©seau doit √™tre profond. On commence g√©n√©ralement avec des architectures standard puis on ajuste.

**Q: Pourquoi normaliser les donn√©es d'entr√©e ?**  
R: Pour homog√©n√©iser les √©chelles et faciliter la convergence de l'entra√Ænement. Des valeurs trop disparates peuvent causer des instabilit√©s num√©riques.

**Q: Comment √©viter le surapprentissage (overfitting) ?**  
R: Techniques de r√©gularisation comme le dropout, l'augmentation de donn√©es, l'arr√™t pr√©coce (early stopping).

**Q: Quel mat√©riel est n√©cessaire pour faire du Deep Learning ?**  
R: Pour l'apprentissage, un GPU est souvent n√©cessaire. Pour ce TP, Google Colab fournit gratuitement l'acc√®s √† des GPUs.