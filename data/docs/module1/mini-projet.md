# üõ†Ô∏è Phase 3 : Mini-projet individuel (1h)

![Mini-projet](../images/banner-mini-projet.svg)

## üéØ Objectifs

Ce mini-projet individuel vous permettra de :

- Appliquer les connaissances acquises sur les r√©seaux de neurones
- Exp√©rimenter avec diff√©rentes architectures et hyperparam√®tres 
- Comprendre l'impact des modifications sur les performances
- Documenter vos observations dans une fiche structur√©e

## üìã Fiche d'observations √† compl√©ter

> **IMPORTANT** : Tout au long de ce mini-projet, vous devrez compl√©ter la **Fiche d'observations** disponible ci-dessous. Ce document sera votre livrable principal.
>
> üì• **T√©l√©chargez et consultez la üìã [fiche d'observations](ressources/Partie1-Phase3-fiche-observations.md) d√®s maintenant** pour comprendre les √©l√©ments √† observer et √† documenter pendant le mini-projet.

## üìù D√©roulement du mini-projet

### √âtape 1 : Mod√®le de base (15 min)

1. **Cr√©ez un nouveau notebook** dans Google Colab
2. **Copiez-collez le code du mod√®le de base** ci-dessous
3. **Ex√©cutez le code** pour voir la performance initiale

```python
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D
from tensorflow.keras.utils import to_categorical

# Charger les donn√©es
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# Pr√©traiter les donn√©es
X_train = X_train.reshape(-1, 28, 28, 1) / 255.0
X_test = X_test.reshape(-1, 28, 28, 1) / 255.0
y_train_cat = to_categorical(y_train, 10)
y_test_cat = to_categorical(y_test, 10)

# Cr√©er le mod√®le de base
model = Sequential([
    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),
    MaxPooling2D(pool_size=(2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])

# Compiler le mod√®le
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Afficher le r√©sum√© du mod√®le
model.summary()

# Entra√Æner le mod√®le
history = model.fit(
    X_train, y_train_cat,
    epochs=3,  # Peu d'√©poques pour aller vite
    batch_size=128,
    validation_split=0.2,
    verbose=1
)

# √âvaluer le mod√®le
test_loss, test_acc = model.evaluate(X_test, y_test_cat)
print(f"Pr√©cision sur les donn√©es de test : {test_acc*100:.2f}%")

# Visualiser l'√©volution de l'apprentissage
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Entra√Ænement')
plt.plot(history.history['val_accuracy'], label='Validation')
plt.title('Pr√©cision du mod√®le')
plt.xlabel('√âpoque')
plt.ylabel('Pr√©cision')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Entra√Ænement')
plt.plot(history.history['val_loss'], label='Validation')
plt.title('Perte (loss)')
plt.xlabel('√âpoque')
plt.ylabel('Perte')
plt.legend()
plt.tight_layout()
plt.show()
```

### √âtape 2 : Am√©liorations du mod√®le (30 min)

Choisissez **au moins 2 modifications** parmi les propositions suivantes et notez vos observations sur votre fiche :

#### Modification A : Ajouter une couche de convolution
```python
model = Sequential([
    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),
    MaxPooling2D(pool_size=(2, 2)),
    Conv2D(64, kernel_size=(3, 3), activation='relu'),  # Couche ajout√©e
    MaxPooling2D(pool_size=(2, 2)),  # Pooling ajout√©
    Flatten(),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])
```

#### Modification B : Ajouter plus de neurones
```python
model = Sequential([
    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),
    MaxPooling2D(pool_size=(2, 2)),
    Flatten(),
    Dense(256, activation='relu'),  # 256 au lieu de 128
    Dense(10, activation='softmax')
])
```

#### Modification C : Ajouter du Dropout pour r√©duire le surapprentissage
```python
from tensorflow.keras.layers import Dropout

model = Sequential([
    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),
    MaxPooling2D(pool_size=(2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),  # Ajout d'une couche de Dropout
    Dense(10, activation='softmax')
])
```

#### Modification D : Changer l'optimiseur
```python
from tensorflow.keras.optimizers import SGD

# Compiler le mod√®le avec SGD au lieu d'Adam
model.compile(
    optimizer=SGD(learning_rate=0.01),  # Utilisation de SGD
    loss='categorical_crossentropy',
    metrics=['accuracy']
)
```

#### Modification E : Augmenter le nombre d'√©poques d'entra√Ænement
```python
# Entra√Æner le mod√®le plus longtemps
history = model.fit(
    X_train, y_train_cat,
    epochs=5,  # 5 au lieu de 3
    batch_size=128,
    validation_split=0.2,
    verbose=1
)
```

### √âtape 3 : Analyse des r√©sultats (15 min)

Pour analyser l'impact de vos modifications, ajoutez ce code √† votre notebook :

```python
# Visualiser quelques pr√©dictions
def plot_predictions(model, X, y_true, n=10):
    predictions = model.predict(X[:n])
    pred_classes = np.argmax(predictions, axis=1)
    true_classes = np.argmax(y_true[:n], axis=1)
    
    plt.figure(figsize=(15, 3))
    for i in range(n):
        plt.subplot(1, n, i+1)
        plt.imshow(X[i].reshape(28, 28), cmap='gray')
        
        if pred_classes[i] == true_classes[i]:
            color = 'green'
        else:
            color = 'red'
            
        plt.title(f"Vrai: {true_classes[i]}\nPr√©dit: {pred_classes[i]}", color=color)
        plt.axis('off')
    plt.tight_layout()
    plt.show()

# Tester avec des donn√©es normales
plot_predictions(model, X_test, y_test_cat)

# Tester avec des donn√©es bruit√©es
X_test_noisy = X_test + np.random.normal(0, 0.1, X_test.shape)
X_test_noisy = np.clip(X_test_noisy, 0, 1)
plot_predictions(model, X_test_noisy, y_test_cat)
```

## üìä √âl√©ments √† documenter dans votre fiche d'observations

Sur votre fiche d'observations (√† t√©l√©charger au d√©but du TP), vous devrez remplir :

1. **Mod√®le de base**
   - Architecture (nombre de couches et de neurones)
   - Performance obtenue (pr√©cision sur les donn√©es de test)
   - Analyse des courbes d'entra√Ænement

2. **Modifications effectu√©es**
   - Description de chaque modification
   - Justification de votre choix

3. **R√©sultats obtenus**
   - Performance apr√®s chaque modification
   - Comparaison avec le mod√®le de base
   - Comportement avec les donn√©es bruit√©es

4. **Analyse et observations**
   - Impact de chaque modification
   - Types d'erreurs observ√©es
   - Votre interpr√©tation des r√©sultats

## üí° Conseils

- Testez les modifications une par une
- Prenez des notes sur chaque modification dans votre fiche d'observations
- Observez attentivement les courbes d'apprentissage et les pr√©dictions

[Retour au Module 1](index.md){ .md-button }
[Continuer vers l'Auto-√©valuation](qcm-evaluation-module1.md){ .md-button .md-button--primary }