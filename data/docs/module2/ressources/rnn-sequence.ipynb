{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "  \"cells\": [\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"# RNN/LSTM pour l'analyse de sentiment\\n\",\n",
    "        \"\\n\",\n",
    "        \"##  S√©ance 2: Types de r√©seaux de neurones\\n\",\n",
    "        \"\\n\",\n",
    "        \"Ce notebook vous guidera √† travers l'impl√©mentation d'un mod√®le LSTM (Long Short-Term Memory) pour l'analyse de sentiment. Vous d√©couvrirez comment les r√©seaux r√©currents peuvent √™tre utilis√©s pour comprendre et classifier du texte.\\n\",\n",
    "        \"\\n\",\n",
    "        \"### Objectifs d'apprentissage:\\n\",\n",
    "        \"- Comprendre le pr√©traitement du texte pour les mod√®les de Deep Learning\\n\",\n",
    "        \"- D√©couvrir l'architecture et le fonctionnement des r√©seaux LSTM\\n\",\n",
    "        \"- Apprendre √† √©valuer un mod√®le d'analyse de sentiment\\n\",\n",
    "        \"- Visualiser et interpr√©ter les embeddings de mots\\n\",\n",
    "        \"\\n\",\n",
    "        \"### Pr√©requis:\\n\",\n",
    "        \"- Connaissances de base en Python\\n\",\n",
    "        \"- Notions fondamentales de r√©seaux de neurones\\n\",\n",
    "        \"- Avoir suivi la s√©ance 1 d'introduction au Deep Learning\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 1. Configuration de l'environnement\\n\",\n",
    "        \"\\n\",\n",
    "        \"Commen√ßons par importer les biblioth√®ques n√©cessaires et configurer notre environnement.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"import numpy as np\\n\",\n",
    "        \"import matplotlib.pyplot as plt\\n\",\n",
    "        \"import tensorflow as tf\\n\",\n",
    "        \"from tensorflow.keras.models import Sequential\\n\",\n",
    "        \"from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\\n\",\n",
    "        \"from tensorflow.keras.preprocessing.text import Tokenizer\\n\",\n",
    "        \"from tensorflow.keras.preprocessing.sequence import pad_sequences\\n\",\n",
    "        \"from tensorflow.keras.callbacks import EarlyStopping\\n\",\n",
    "        \"import pandas as pd\\n\",\n",
    "        \"import re\\n\",\n",
    "        \"import time\\n\",\n",
    "        \"import seaborn as sns\\n\",\n",
    "        \"from sklearn.metrics import confusion_matrix, classification_report\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Configuration pour reproductibilit√©\\n\",\n",
    "        \"np.random.seed(42)\\n\",\n",
    "        \"tf.random.set_seed(42)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# V√©rifier la version de TensorFlow\\n\",\n",
    "        \"print(f\\\"TensorFlow version: {tf.__version__}\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 2. Pr√©paration des donn√©es\\n\",\n",
    "        \"\\n\",\n",
    "        \"Pour ce TP, nous allons utiliser un petit dataset simul√© d'avis sur des films. Chaque avis sera class√© comme positif, n√©gatif ou neutre.\\n\",\n",
    "        \"\\n\",\n",
    "        \"Dans un projet r√©el, vous pourriez utiliser des datasets plus importants comme IMDB, Amazon Reviews, etc.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Cr√©er un petit jeu de donn√©es d'avis sur les films (simul√©)\\n\",\n",
    "        \"reviews = [\\n\",\n",
    "        \"    \\\"Ce film √©tait excellent, j'ai vraiment ador√© les performances des acteurs.\\\",\\n\",\n",
    "        \"    \\\"Une exp√©rience cin√©matographique incroyable, absolument √† voir !\\\",\\n\",\n",
    "        \"    \\\"Un chef-d'≈ìuvre du cin√©ma, magnifiquement r√©alis√©.\\\",\\n\",\n",
    "        \"    \\\"J'ai beaucoup appr√©ci√© l'histoire et les personnages √©taient bien d√©velopp√©s.\\\",\\n\",\n",
    "        \"    \\\"Visuellement √©poustouflant avec une histoire captivante.\\\",\\n\",\n",
    "        \"    \\\"Un film d√©cevant avec un sc√©nario plein de trous.\\\",\\n\",\n",
    "        \"    \\\"Vraiment terrible, je n'ai pas aim√© du tout.\\\",\\n\",\n",
    "        \"    \\\"Un g√¢chis complet de temps et d'argent, √©vitez √† tout prix.\\\",\\n\",\n",
    "        \"    \\\"Ennuyeux et pr√©visible, les acteurs semblaient d√©sint√©ress√©s.\\\",\\n\",\n",
    "        \"    \\\"Une d√©ception totale, l'intrigue ne fait aucun sens.\\\",\\n\",\n",
    "        \"    \\\"C'√©tait correct, ni bon ni mauvais.\\\",\\n\",\n",
    "        \"    \\\"Un film moyen avec quelques bons moments.\\\",\\n\",\n",
    "        \"    \\\"Certaines sc√®nes √©taient bonnes, mais dans l'ensemble assez moyen.\\\",\\n\",\n",
    "        \"    \\\"Pas aussi bon que je l'esp√©rais, mais pas horrible non plus.\\\",\\n\",\n",
    "        \"    \\\"Une histoire int√©ressante mais mal ex√©cut√©e.\\\",\\n\",\n",
    "        \"    \\\"Un film brillant qui m'a fait r√©fl√©chir pendant des jours.\\\",\\n\",\n",
    "        \"    \\\"Absolument sublime, l'un des meilleurs films que j'ai jamais vus.\\\",\\n\",\n",
    "        \"    \\\"Un d√©sastre total, je me suis endormi au milieu.\\\",\\n\",\n",
    "        \"    \\\"Pas du tout ce √† quoi je m'attendais, tr√®s d√©√ßu.\\\",\\n\",\n",
    "        \"    \\\"Le jeu d'acteur √©tait fantastique, mais l'histoire √©tait faible.\\\"\\n\",\n",
    "        \"]\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Attribuer des sentiments (0 = n√©gatif, 1 = neutre, 2 = positif)\\n\",\n",
    "        \"sentiments = [2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 0, 0, 1]\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Convertir en DataFrame pour faciliter la manipulation\\n\",\n",
    "        \"df = pd.DataFrame({\\n\",\n",
    "        \"    'review': reviews,\\n\",\n",
    "        \"    'sentiment': sentiments\\n\",\n",
    "        \"})\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Afficher quelques informations sur le dataset\\n\",\n",
    "        \"print(f\\\"Nombre total d'avis: {len(df)}\\\")\\n\",\n",
    "        \"print(f\\\"R√©partition des sentiments: {df['sentiment'].value_counts().sort_index()}\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Afficher quelques exemples\\n\",\n",
    "        \"print(\\\"\\\\nExemples d'avis:\\\")\\n\",\n",
    "        \"for sentiment in [0, 1, 2]:\\n\",\n",
    "        \"    sample = df[df['sentiment'] == sentiment].iloc[0]\\n\",\n",
    "        \"    print(f\\\"Sentiment {sentiment}: '{sample['review']}'\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### Visualisation de la distribution des sentiments\\n\",\n",
    "        \"\\n\",\n",
    "        \"V√©rifions que notre jeu de donn√©es est √©quilibr√© entre les diff√©rentes classes.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Visualiser la distribution des sentiments\\n\",\n",
    "        \"plt.figure(figsize=(8, 5))\\n\",\n",
    "        \"ax = sns.countplot(x='sentiment', data=df)\\n\",\n",
    "        \"plt.title('Distribution des sentiments')\\n\",\n",
    "        \"plt.xlabel('Sentiment (0=n√©gatif, 1=neutre, 2=positif)')\\n\",\n",
    "        \"plt.ylabel('Nombre d\\\\'avis')\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Ajouter les valeurs sur les barres\\n\",\n",
    "        \"for p in ax.patches:\\n\",\n",
    "        \"    ax.annotate(f\\\"{p.get_height()}\\\", (p.get_x() + p.get_width()/2., p.get_height()),\\n\",\n",
    "        \"                ha='center', va='bottom')\\n\",\n",
    "        \"\\n\",\n",
    "        \"plt.show()\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 3. Pr√©traitement du texte\\n\",\n",
    "        \"\\n\",\n",
    "        \"Avant de pouvoir utiliser le texte avec notre mod√®le LSTM, nous devons le pr√©traiter. Cela implique plusieurs √©tapes:\\n\",\n",
    "        \"1. Nettoyage (minuscules, suppression de ponctuation, etc.)\\n\",\n",
    "        \"2. Tokenisation (conversion du texte en s√©quences de nombres)\\n\",\n",
    "        \"3. Padding (uniformisation de la longueur des s√©quences)\\n\",\n",
    "        \"\\n\",\n",
    "        \"Commen√ßons par le nettoyage de texte:\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"def preprocess_text(text):\\n\",\n",
    "        \"    \\\"\\\"\\\"Fonction pour nettoyer et normaliser le texte\\\"\\\"\\\"\\n\",\n",
    "        \"    # Convertir en minuscules\\n\",\n",
    "        \"    text = text.lower()\\n\",\n",
    "        \"    # Supprimer la ponctuation et les caract√®res sp√©ciaux\\n\",\n",
    "        \"    text = re.sub(r'[^\\\\w\\\\s]', '', text)\\n\",\n",
    "        \"    # Supprimer les chiffres\\n\",\n",
    "        \"    text = re.sub(r'\\\\d+', '', text)\\n\",\n",
    "        \"    # Supprimer les espaces multiples\\n\",\n",
    "        \"    text = re.sub(r'\\\\s+', ' ', text).strip()\\n\",\n",
    "        \"    return text\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Appliquer le pr√©traitement √† nos avis\\n\",\n",
    "        \"df['processed_review'] = df['review'].apply(preprocess_text)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Afficher un exemple avant et apr√®s pr√©traitement\\n\",\n",
    "        \"example_idx = 0\\n\",\n",
    "        \"print(f\\\"Avant: {df['review'][example_idx]}\\\")\\n\",\n",
    "        \"print(f\\\"Apr√®s: {df['processed_review'][example_idx]}\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### Tokenisation du texte\\n\",\n",
    "        \"\\n\",\n",
    "        \"La tokenisation convertit le texte en s√©quences num√©riques que notre r√©seau de neurones peut traiter.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Configuration pour la tokenisation\\n\",\n",
    "        \"max_words = 1000  # Taille du vocabulaire\\n\",\n",
    "        \"max_len = 100     # Longueur maximale des s√©quences\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Cr√©er et configurer le tokenizer\\n\",\n",
    "        \"tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\\n\",\n",
    "        \"tokenizer.fit_on_texts(df['processed_review'])\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Convertir les textes en s√©quences de tokens\\n\",\n",
    "        \"sequences = tokenizer.texts_to_sequences(df['processed_review'])\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Appliquer le padding pour uniformiser la longueur des s√©quences\\n\",\n",
    "        \"padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(f\\\"Taille du vocabulaire: {len(tokenizer.word_index)}\\\")\\n\",\n",
    "        \"print(f\\\"Forme des s√©quences apr√®s padding: {padded_sequences.shape}\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Afficher le mapping de quelques mots vers leurs tokens\\n\",\n",
    "        \"print(\\\"\\\\nExemples de mapping mot -> token:\\\")\\n\",\n",
    "        \"sample_words = ['film', 'bon', 'mauvais', 'excellent', 'terrible']\\n\",\n",
    "        \"for word in sample_words:\\n\",\n",
    "        \"    if word in tokenizer.word_index:\\n\",\n",
    "        \"        print(f\\\"{word} -> {tokenizer.word_index[word]}\\\")\\n\",\n",
    "        \"    else:\\n\",\n",
    "        \"        print(f\\\"{word} -> Non trouv√© dans le vocabulaire\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### Visualisation d'une s√©quence tokenis√©e\\n\",\n",
    "        \"\\n\",\n",
    "        \"Pour mieux comprendre la tokenisation, visualisons comment un avis est converti en s√©quence de tokens.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"def visualize_tokenized_sequence(text, tokens):\\n\",\n",
    "        \"    \\\"\\\"\\\"Visualise la correspondance entre mots et tokens\\\"\\\"\\\"\\n\",\n",
    "        \"    words = text.split()\\n\",\n",
    "        \"    plt.figure(figsize=(15, 3))\\n\",\n",
    "        \"    plt.bar(range(len(tokens)), tokens)\\n\",\n",
    "        \"    plt.xticks(range(len(tokens)), words, rotation=45, ha='right')\\n\",\n",
    "        \"    plt.ylabel('Token ID')\\n\",\n",
    "        \"    plt.title('Repr√©sentation tokenis√©e d\\\\'un avis')\\n\",\n",
    "        \"    plt.tight_layout()\\n\",\n",
    "        \"    plt.show()\\n\",\n",
    "        \"\\n\",\n",
    "        \"sample_idx = 0\\n\",\n",
    "        \"sample_text = df['processed_review'][sample_idx].split()[:15]  # Limiter √† 15 mots pour lisibilit√©\\n\",\n",
    "        \"sample_tokens = sequences[sample_idx][:15]\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(f\\\"Exemple d'avis: {' '.join(sample_text)}\\\")\\n\",\n",
    "        \"print(f\\\"Tokens correspondants: {sample_tokens}\\\")\\n\",\n",
    "        \"visualize_tokenized_sequence(' '.join(sample_text), sample_tokens)\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 4. Division en ensembles d'entra√Ænement et de test\\n\",\n",
    "        \"\\n\",\n",
    "        \"Avant de cr√©er notre mod√®le, divisons nos donn√©es en ensembles d'entra√Ænement et de test pour √©valuer ses performances.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"from sklearn.model_selection import train_test_split\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Division 70-30 avec stratification pour conserver la distribution des classes\\n\",\n",
    "        \"X_train, X_test, y_train, y_test = train_test_split(\\n\",\n",
    "        \"    padded_sequences, \\n\",\n",
    "        \"    df['sentiment'],\\n\",\n",
    "        \"    test_size=0.3,\\n\",\n",
    "        \"    random_state=42,\\n\",\n",
    "        \"    stratify=df['sentiment']  # Assurer une r√©partition √©quilibr√©e des classes\\n\",\n",
    "        \")\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(f\\\"Forme des donn√©es d'entra√Ænement: {X_train.shape}\\\")\\n\",\n",
    "        \"print(f\\\"Forme des donn√©es de test: {X_test.shape}\\\")\\n\",\n",
    "        \"print(f\\\"Distribution des classes (entra√Ænement): {pd.Series(y_train).value_counts().sort_index()}\\\")\\n\",\n",
    "        \"print(f\\\"Distribution des classes (test): {pd.Series(y_test).value_counts().sort_index()}\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 5. Cr√©ation du mod√®le LSTM\\n\",\n",
    "        \"\\n\",\n",
    "        \"Nous allons maintenant cr√©er notre mod√®le d'analyse de sentiment en utilisant une architecture LSTM bidirectionnelle.\\n\",\n",
    "        \"\\n\",\n",
    "        \"### Architecture du mod√®le\\n\",\n",
    "        \"- **Couche d'embedding**: Convertit les tokens en vecteurs denses\\n\",\n",
    "        \"- **Couches LSTM bidirectionnelles**: Capture les d√©pendances √† long terme dans les deux directions\\n\",\n",
    "        \"- **Dropout**: √âvite le surapprentissage\\n\",\n",
    "        \"- **Couche dense finale**: Classification en 3 cat√©gories (n√©gatif, neutre, positif)\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Param√®tres du mod√®le\\n\",\n",
    "        \"embedding_dim = 32  # Dimension de l'espace d'embedding\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Cr√©ation du mod√®le\\n\",\n",
    "        \"model = Sequential([\\n\",\n",
    "        \"    # Couche d'embedding pour convertir les tokens en vecteurs denses\\n\",\n",
    "        \"    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len),\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Couche LSTM bidirectionnelle\\n\",\n",
    "        \"    Bidirectional(LSTM(64, return_sequences=True)),\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Deuxi√®me couche LSTM suivie de dropout pour r√©gularisation\\n\",\n",
    "        \"    Bidirectional(LSTM(32)),\\n\",\n",
    "        \"    Dropout(0.5),\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Couche de classification (3 classes: n√©gatif, neutre, positif)\\n\",\n",
    "        \"    Dense(3, activation='softmax')\\n\",\n",
    "        \"])\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Afficher un r√©sum√© du mod√®le\\n\",\n",
    "        \"model.summary()\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### üí° Points cl√©s √† observer dans l'architecture\\n\",\n",
    "        \"\\n\",\n",
    "        \"- **LSTM bidirectionnel** : Lit le texte de gauche √† droite ET de droite √† gauche, capturant mieux le contexte\\n\",\n",
    "        \"- **return_sequences=True** : Permet d'empiler plusieurs couches LSTM\\n\",\n",
    "        \"- **Dropout** : D√©sactive al√©atoirement 50% des neurones pendant l'entra√Ænement pour √©viter le surapprentissage\\n\",\n",
    "        \"- **Activation softmax** : G√©n√®re une distribution de probabilit√© sur les 3 classes\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 6. Compilation et entra√Ænement du mod√®le\\n\",\n",
    "        \"\\n\",\n",
    "        \"Maintenant, compilons et entra√Ænons notre mod√®le LSTM.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Compiler le mod√®le\\n\",\n",
    "        \"model.compile(\\n\",\n",
    "        \"    optimizer='adam',\\n\",\n",
    "        \"    loss='sparse_categorical_crossentropy',  # Pour les √©tiquettes sous forme d'entiers (non one-hot)\\n\",\n",
    "        \"    metrics=['accuracy']\\n\",\n",
    "        \")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Early stopping pour √©viter le surapprentissage\\n\",\n",
    "        \"early_stopping = EarlyStopping(\\n\",\n",
    "        \"    monitor='val_loss',\\n\",\n",
    "        \"    patience=3,\\n\",\n",
    "        \"    restore_best_weights=True\\n\",\n",
    "        \")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Mesure du temps d'entra√Ænement\\n\",\n",
    "        \"start_time = time.time()\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Entra√Ænement du mod√®le\\n\",\n",
    "        \"history = model.fit(\\n\",\n",
    "        \"    X_train, \\n\",\n",
    "        \"    y_train, \\n\",\n",
    "        \"    epochs=20,\\n\",\n",
    "        \"    batch_size=4,  # Petit batch size en raison de la petite taille du dataset\\n\",\n",
    "        \"    validation_split=0.2,  # 20% des donn√©es d'entra√Ænement serviront √† la validation\\n\",\n",
    "        \"    callbacks=[early_stopping],\\n\",\n",
    "        \"    verbose=1\\n\",\n",
    "        \")\\n\",\n",
    "        \"\\n\",\n",
    "        \"training_time = time.time() - start_time\\n\",\n",
    "        \"print(f\\\"\\\\nTemps d'entra√Ænement: {training_time:.2f} secondes\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### Visualisation de l'√©volution de l'entra√Ænement\\n\",\n",
    "        \"\\n\",\n",
    "        \"Observons comment la pr√©cision et la perte ont √©volu√© au cours de l'entra√Ænement.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Visualisation de l'entra√Ænement\\n\",\n",
    "        \"plt.figure(figsize=(12, 5))\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Graphique de pr√©cision\\n\",\n",
    "        \"plt.subplot(1, 2, 1)\\n\",\n",
    "        \"plt.plot(history.history['accuracy'], label='Entra√Ænement')\\n\",\n",
    "        \"plt.plot(history.history['val_accuracy'], label='Validation')\\n\",\n",
    "        \"plt.title('√âvolution de la pr√©cision')\\n\",\n",
    "        \"plt.xlabel('√âpoque')\\n\",\n",
    "        \"plt.ylabel('Pr√©cision')\\n\",\n",
    "        \"plt.legend()\\n\",\n",
    "        \"plt.grid(True, linestyle='--', alpha=0.6)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Graphique de perte\\n\",\n",
    "        \"plt.subplot(1, 2, 2)\\n\",\n",
    "        \"plt.plot(history.history['loss'], label='Entra√Ænement')\\n\",\n",
    "        \"plt.plot(history.history['val_loss'], label='Validation')\\n\",\n",
    "        \"plt.title('√âvolution de la perte')\\n\",\n",
    "        \"plt.xlabel('√âpoque')\\n\",\n",
    "        \"plt.ylabel('Perte')\\n\",\n",
    "        \"plt.legend()\\n\",\n",
    "        \"plt.grid(True, linestyle='--', alpha=0.6)\\n\",\n",
    "        \"\\n\",\n",
    "        \"plt.tight_layout()\\n\",\n",
    "        \"plt.show()\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 7. √âvaluation du mod√®le\\n\",\n",
    "        \"\\n\",\n",
    "        \"Maintenant, √©valuons les performances de notre mod√®le sur l'ensemble de test.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# √âvaluation sur l'ensemble de test\\n\",\n",
    "        \"test_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)\\n\",\n",
    "        \"print(f\\\"Pr√©cision sur l'ensemble de test: {test_acc*100:.2f}%\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# G√©n√©rer les pr√©dictions\\n\",\n",
    "        \"y_pred_proba = model.predict(X_test)\\n\",\n",
    "        \"y_pred_classes = np.argmax(y_pred_proba, axis=1)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Matrice de confusion\\n\",\n",
    "        \"conf_mat = confusion_matrix(y_test, y_pred_classes)\\n\",\n",
    "        \"plt.figure(figsize=(8, 6))\\n\",\n",
    "        \"sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', \\n\",\n",
    "        \"            xticklabels=['N√©gatif', 'Neutre', 'Positif'],\\n\",\n",
    "        \"            yticklabels=['N√©gatif', 'Neutre', 'Positif'])\\n\",\n",
    "        \"plt.xlabel('Pr√©dit')\\n\",\n",
    "        \"plt.ylabel('R√©el')\\n\",\n",
    "        \"plt.title('Matrice de confusion')\\n\",\n",
    "        \"plt.tight_layout()\\n\",\n",
    "        \"plt.show()\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Rapport de classification\\n\",\n",
    "        \"print(\\\"\\\\nRapport de classification d√©taill√©:\\\")\\n\",\n",
    "        \"target_names = ['N√©gatif', 'Neutre', 'Positif']\\n\",\n",
    "        \"print(classification_report(y_test, y_pred_classes, target_names=target_names))\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### üß† R√©flexions sur les r√©sultats\\n\",\n",
    "        \"\\n\",\n",
    "        \"- **Analysez la matrice de confusion**: Quelles classes sont le mieux reconnues? Y a-t-il des confusions particuli√®res?\\n\",\n",
    "        \"- **Pr√©cision vs Rappel**: Y a-t-il un d√©s√©quilibre? Quelle m√©trique privil√©gier selon le contexte?\\n\",\n",
    "        \"- **Taille du dataset**: Comment les r√©sultats pourraient-ils √™tre affect√©s par la petite taille de notre jeu de donn√©es?\\n\",\n",
    "        \"\\n\",\n",
    "        \"üëâ **Discussion**: Notez vos observations ci-dessous:\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"*√âcrivez vos observations ici...*\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 8. Test avec de nouveaux avis\\n\",\n",
    "        \"\\n\",\n",
    "        \"Testons maintenant notre mod√®le avec quelques nouveaux avis qui n'ont pas √©t√© utilis√©s pour l'entra√Ænement.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Nouveaux avis √† tester\\n\",\n",
    "        \"new_reviews = [\\n\",\n",
    "        \"    \\\"Ce film √©tait vraiment fantastique, j'ai ador√© chaque minute.\\\",\\n\",\n",
    "        \"    \\\"Je n'ai pas du tout aim√© ce film, c'√©tait une perte de temps compl√®te.\\\",\\n\",\n",
    "        \"    \\\"C'√©tait un film correct, ni bon ni mauvais.\\\"\\n\",\n",
    "        \"]\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Pr√©traitement des nouveaux avis\\n\",\n",
    "        \"processed_new_reviews = [preprocess_text(review) for review in new_reviews]\\n\",\n",
    "        \"sequences_new = tokenizer.texts_to_sequences(processed_new_reviews)\\n\",\n",
    "        \"padded_new = pad_sequences(sequences_new, maxlen=max_len, padding='post', truncating='post')\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Pr√©dictions\\n\",\n",
    "        \"predictions = model.predict(padded_new)\\n\",\n",
    "        \"predicted_classes = np.argmax(predictions, axis=1)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Afficher les r√©sultats\\n\",\n",
    "        \"sentiment_labels = {0: \\\"N√©gatif\\\", 1: \\\"Neutre\\\", 2: \\\"Positif\\\"}\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(\\\"Pr√©dictions pour les nouveaux avis:\\\\n\\\")\\n\",\n",
    "        \"for i, review in enumerate(new_reviews):\\n\",\n",
    "        \"    pred_class = predicted_classes[i]\\n\",\n",
    "        \"    confidence = predictions[i][pred_class] * 100\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    print(f\\\"Avis: {review}\\\")\\n\",\n",
    "        \"    print(f\\\"Sentiment pr√©dit: {sentiment_labels[pred_class]} (confiance: {confidence:.2f}%)\\\")\\n\",\n",
    "        \"    print(\\\"Probabilit√©s pour chaque classe:\\\")\\n\",\n",
    "        \"    for j, label in sentiment_labels.items():\\n\",\n",
    "        \"        print(f\\\"  {label}: {predictions[i][j]*100:.2f}%\\\")\\n\",\n",
    "        \"    print()\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### Visualisation graphique des pr√©dictions\\n\",\n",
    "        \"\\n\",\n",
    "        \"Visualisons les probabilit√©s pour chaque classe pour les nouveaux avis.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Visualisation des probabilit√©s pour chaque avis\\n\",\n",
    "        \"plt.figure(figsize=(15, 5))\\n\",\n",
    "        \"labels = ['N√©gatif', 'Neutre', 'Positif']\\n\",\n",
    "        \"\\n\",\n",
    "        \"for i, review in enumerate(new_reviews):\\n\",\n",
    "        \"    plt.subplot(1, 3, i+1)\\n\",\n",
    "        \"    plt.bar(labels, predictions[i], color=['red', 'gray', 'green'])\\n\",\n",
    "        \"    plt.title(f\\\"Avis {i+1}\\\")\\n\",\n",
    "        \"    plt.ylim(0, 1)\\n\",\n",
    "        \"    plt.ylabel('Probabilit√©')\\n\",\n",
    "        \"    plt.xticks(rotation=45)\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Ajouter les valeurs sur les barres\\n\",\n",
    "        \"    for j, p in enumerate(predictions[i]):\\n\",\n",
    "        \"        plt.text(j, p + 0.02, f\\\"{p*100:.1f}%\\\", ha='center')\\n\",\n",
    "        \"        \\n\",\n",
    "        \"plt.tight_layout()\\n\",\n",
    "        \"plt.show()\"\n",
    "      ]\n",
    "    },"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
