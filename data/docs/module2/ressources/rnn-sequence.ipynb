{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "  \"cells\": [\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"# RNN/LSTM pour l'analyse de sentiment\\n\",\n",
    "        \"\\n\",\n",
    "        \"##  Séance 2: Types de réseaux de neurones\\n\",\n",
    "        \"\\n\",\n",
    "        \"Ce notebook vous guidera à travers l'implémentation d'un modèle LSTM (Long Short-Term Memory) pour l'analyse de sentiment. Vous découvrirez comment les réseaux récurrents peuvent être utilisés pour comprendre et classifier du texte.\\n\",\n",
    "        \"\\n\",\n",
    "        \"### Objectifs d'apprentissage:\\n\",\n",
    "        \"- Comprendre le prétraitement du texte pour les modèles de Deep Learning\\n\",\n",
    "        \"- Découvrir l'architecture et le fonctionnement des réseaux LSTM\\n\",\n",
    "        \"- Apprendre à évaluer un modèle d'analyse de sentiment\\n\",\n",
    "        \"- Visualiser et interpréter les embeddings de mots\\n\",\n",
    "        \"\\n\",\n",
    "        \"### Prérequis:\\n\",\n",
    "        \"- Connaissances de base en Python\\n\",\n",
    "        \"- Notions fondamentales de réseaux de neurones\\n\",\n",
    "        \"- Avoir suivi la séance 1 d'introduction au Deep Learning\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 1. Configuration de l'environnement\\n\",\n",
    "        \"\\n\",\n",
    "        \"Commençons par importer les bibliothèques nécessaires et configurer notre environnement.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"import numpy as np\\n\",\n",
    "        \"import matplotlib.pyplot as plt\\n\",\n",
    "        \"import tensorflow as tf\\n\",\n",
    "        \"from tensorflow.keras.models import Sequential\\n\",\n",
    "        \"from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\\n\",\n",
    "        \"from tensorflow.keras.preprocessing.text import Tokenizer\\n\",\n",
    "        \"from tensorflow.keras.preprocessing.sequence import pad_sequences\\n\",\n",
    "        \"from tensorflow.keras.callbacks import EarlyStopping\\n\",\n",
    "        \"import pandas as pd\\n\",\n",
    "        \"import re\\n\",\n",
    "        \"import time\\n\",\n",
    "        \"import seaborn as sns\\n\",\n",
    "        \"from sklearn.metrics import confusion_matrix, classification_report\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Configuration pour reproductibilité\\n\",\n",
    "        \"np.random.seed(42)\\n\",\n",
    "        \"tf.random.set_seed(42)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Vérifier la version de TensorFlow\\n\",\n",
    "        \"print(f\\\"TensorFlow version: {tf.__version__}\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 2. Préparation des données\\n\",\n",
    "        \"\\n\",\n",
    "        \"Pour ce TP, nous allons utiliser un petit dataset simulé d'avis sur des films. Chaque avis sera classé comme positif, négatif ou neutre.\\n\",\n",
    "        \"\\n\",\n",
    "        \"Dans un projet réel, vous pourriez utiliser des datasets plus importants comme IMDB, Amazon Reviews, etc.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Créer un petit jeu de données d'avis sur les films (simulé)\\n\",\n",
    "        \"reviews = [\\n\",\n",
    "        \"    \\\"Ce film était excellent, j'ai vraiment adoré les performances des acteurs.\\\",\\n\",\n",
    "        \"    \\\"Une expérience cinématographique incroyable, absolument à voir !\\\",\\n\",\n",
    "        \"    \\\"Un chef-d'œuvre du cinéma, magnifiquement réalisé.\\\",\\n\",\n",
    "        \"    \\\"J'ai beaucoup apprécié l'histoire et les personnages étaient bien développés.\\\",\\n\",\n",
    "        \"    \\\"Visuellement époustouflant avec une histoire captivante.\\\",\\n\",\n",
    "        \"    \\\"Un film décevant avec un scénario plein de trous.\\\",\\n\",\n",
    "        \"    \\\"Vraiment terrible, je n'ai pas aimé du tout.\\\",\\n\",\n",
    "        \"    \\\"Un gâchis complet de temps et d'argent, évitez à tout prix.\\\",\\n\",\n",
    "        \"    \\\"Ennuyeux et prévisible, les acteurs semblaient désintéressés.\\\",\\n\",\n",
    "        \"    \\\"Une déception totale, l'intrigue ne fait aucun sens.\\\",\\n\",\n",
    "        \"    \\\"C'était correct, ni bon ni mauvais.\\\",\\n\",\n",
    "        \"    \\\"Un film moyen avec quelques bons moments.\\\",\\n\",\n",
    "        \"    \\\"Certaines scènes étaient bonnes, mais dans l'ensemble assez moyen.\\\",\\n\",\n",
    "        \"    \\\"Pas aussi bon que je l'espérais, mais pas horrible non plus.\\\",\\n\",\n",
    "        \"    \\\"Une histoire intéressante mais mal exécutée.\\\",\\n\",\n",
    "        \"    \\\"Un film brillant qui m'a fait réfléchir pendant des jours.\\\",\\n\",\n",
    "        \"    \\\"Absolument sublime, l'un des meilleurs films que j'ai jamais vus.\\\",\\n\",\n",
    "        \"    \\\"Un désastre total, je me suis endormi au milieu.\\\",\\n\",\n",
    "        \"    \\\"Pas du tout ce à quoi je m'attendais, très déçu.\\\",\\n\",\n",
    "        \"    \\\"Le jeu d'acteur était fantastique, mais l'histoire était faible.\\\"\\n\",\n",
    "        \"]\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Attribuer des sentiments (0 = négatif, 1 = neutre, 2 = positif)\\n\",\n",
    "        \"sentiments = [2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 0, 0, 1]\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Convertir en DataFrame pour faciliter la manipulation\\n\",\n",
    "        \"df = pd.DataFrame({\\n\",\n",
    "        \"    'review': reviews,\\n\",\n",
    "        \"    'sentiment': sentiments\\n\",\n",
    "        \"})\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Afficher quelques informations sur le dataset\\n\",\n",
    "        \"print(f\\\"Nombre total d'avis: {len(df)}\\\")\\n\",\n",
    "        \"print(f\\\"Répartition des sentiments: {df['sentiment'].value_counts().sort_index()}\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Afficher quelques exemples\\n\",\n",
    "        \"print(\\\"\\\\nExemples d'avis:\\\")\\n\",\n",
    "        \"for sentiment in [0, 1, 2]:\\n\",\n",
    "        \"    sample = df[df['sentiment'] == sentiment].iloc[0]\\n\",\n",
    "        \"    print(f\\\"Sentiment {sentiment}: '{sample['review']}'\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### Visualisation de la distribution des sentiments\\n\",\n",
    "        \"\\n\",\n",
    "        \"Vérifions que notre jeu de données est équilibré entre les différentes classes.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Visualiser la distribution des sentiments\\n\",\n",
    "        \"plt.figure(figsize=(8, 5))\\n\",\n",
    "        \"ax = sns.countplot(x='sentiment', data=df)\\n\",\n",
    "        \"plt.title('Distribution des sentiments')\\n\",\n",
    "        \"plt.xlabel('Sentiment (0=négatif, 1=neutre, 2=positif)')\\n\",\n",
    "        \"plt.ylabel('Nombre d\\\\'avis')\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Ajouter les valeurs sur les barres\\n\",\n",
    "        \"for p in ax.patches:\\n\",\n",
    "        \"    ax.annotate(f\\\"{p.get_height()}\\\", (p.get_x() + p.get_width()/2., p.get_height()),\\n\",\n",
    "        \"                ha='center', va='bottom')\\n\",\n",
    "        \"\\n\",\n",
    "        \"plt.show()\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 3. Prétraitement du texte\\n\",\n",
    "        \"\\n\",\n",
    "        \"Avant de pouvoir utiliser le texte avec notre modèle LSTM, nous devons le prétraiter. Cela implique plusieurs étapes:\\n\",\n",
    "        \"1. Nettoyage (minuscules, suppression de ponctuation, etc.)\\n\",\n",
    "        \"2. Tokenisation (conversion du texte en séquences de nombres)\\n\",\n",
    "        \"3. Padding (uniformisation de la longueur des séquences)\\n\",\n",
    "        \"\\n\",\n",
    "        \"Commençons par le nettoyage de texte:\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"def preprocess_text(text):\\n\",\n",
    "        \"    \\\"\\\"\\\"Fonction pour nettoyer et normaliser le texte\\\"\\\"\\\"\\n\",\n",
    "        \"    # Convertir en minuscules\\n\",\n",
    "        \"    text = text.lower()\\n\",\n",
    "        \"    # Supprimer la ponctuation et les caractères spéciaux\\n\",\n",
    "        \"    text = re.sub(r'[^\\\\w\\\\s]', '', text)\\n\",\n",
    "        \"    # Supprimer les chiffres\\n\",\n",
    "        \"    text = re.sub(r'\\\\d+', '', text)\\n\",\n",
    "        \"    # Supprimer les espaces multiples\\n\",\n",
    "        \"    text = re.sub(r'\\\\s+', ' ', text).strip()\\n\",\n",
    "        \"    return text\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Appliquer le prétraitement à nos avis\\n\",\n",
    "        \"df['processed_review'] = df['review'].apply(preprocess_text)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Afficher un exemple avant et après prétraitement\\n\",\n",
    "        \"example_idx = 0\\n\",\n",
    "        \"print(f\\\"Avant: {df['review'][example_idx]}\\\")\\n\",\n",
    "        \"print(f\\\"Après: {df['processed_review'][example_idx]}\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### Tokenisation du texte\\n\",\n",
    "        \"\\n\",\n",
    "        \"La tokenisation convertit le texte en séquences numériques que notre réseau de neurones peut traiter.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Configuration pour la tokenisation\\n\",\n",
    "        \"max_words = 1000  # Taille du vocabulaire\\n\",\n",
    "        \"max_len = 100     # Longueur maximale des séquences\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Créer et configurer le tokenizer\\n\",\n",
    "        \"tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\\n\",\n",
    "        \"tokenizer.fit_on_texts(df['processed_review'])\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Convertir les textes en séquences de tokens\\n\",\n",
    "        \"sequences = tokenizer.texts_to_sequences(df['processed_review'])\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Appliquer le padding pour uniformiser la longueur des séquences\\n\",\n",
    "        \"padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(f\\\"Taille du vocabulaire: {len(tokenizer.word_index)}\\\")\\n\",\n",
    "        \"print(f\\\"Forme des séquences après padding: {padded_sequences.shape}\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Afficher le mapping de quelques mots vers leurs tokens\\n\",\n",
    "        \"print(\\\"\\\\nExemples de mapping mot -> token:\\\")\\n\",\n",
    "        \"sample_words = ['film', 'bon', 'mauvais', 'excellent', 'terrible']\\n\",\n",
    "        \"for word in sample_words:\\n\",\n",
    "        \"    if word in tokenizer.word_index:\\n\",\n",
    "        \"        print(f\\\"{word} -> {tokenizer.word_index[word]}\\\")\\n\",\n",
    "        \"    else:\\n\",\n",
    "        \"        print(f\\\"{word} -> Non trouvé dans le vocabulaire\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### Visualisation d'une séquence tokenisée\\n\",\n",
    "        \"\\n\",\n",
    "        \"Pour mieux comprendre la tokenisation, visualisons comment un avis est converti en séquence de tokens.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"def visualize_tokenized_sequence(text, tokens):\\n\",\n",
    "        \"    \\\"\\\"\\\"Visualise la correspondance entre mots et tokens\\\"\\\"\\\"\\n\",\n",
    "        \"    words = text.split()\\n\",\n",
    "        \"    plt.figure(figsize=(15, 3))\\n\",\n",
    "        \"    plt.bar(range(len(tokens)), tokens)\\n\",\n",
    "        \"    plt.xticks(range(len(tokens)), words, rotation=45, ha='right')\\n\",\n",
    "        \"    plt.ylabel('Token ID')\\n\",\n",
    "        \"    plt.title('Représentation tokenisée d\\\\'un avis')\\n\",\n",
    "        \"    plt.tight_layout()\\n\",\n",
    "        \"    plt.show()\\n\",\n",
    "        \"\\n\",\n",
    "        \"sample_idx = 0\\n\",\n",
    "        \"sample_text = df['processed_review'][sample_idx].split()[:15]  # Limiter à 15 mots pour lisibilité\\n\",\n",
    "        \"sample_tokens = sequences[sample_idx][:15]\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(f\\\"Exemple d'avis: {' '.join(sample_text)}\\\")\\n\",\n",
    "        \"print(f\\\"Tokens correspondants: {sample_tokens}\\\")\\n\",\n",
    "        \"visualize_tokenized_sequence(' '.join(sample_text), sample_tokens)\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 4. Division en ensembles d'entraînement et de test\\n\",\n",
    "        \"\\n\",\n",
    "        \"Avant de créer notre modèle, divisons nos données en ensembles d'entraînement et de test pour évaluer ses performances.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"from sklearn.model_selection import train_test_split\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Division 70-30 avec stratification pour conserver la distribution des classes\\n\",\n",
    "        \"X_train, X_test, y_train, y_test = train_test_split(\\n\",\n",
    "        \"    padded_sequences, \\n\",\n",
    "        \"    df['sentiment'],\\n\",\n",
    "        \"    test_size=0.3,\\n\",\n",
    "        \"    random_state=42,\\n\",\n",
    "        \"    stratify=df['sentiment']  # Assurer une répartition équilibrée des classes\\n\",\n",
    "        \")\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(f\\\"Forme des données d'entraînement: {X_train.shape}\\\")\\n\",\n",
    "        \"print(f\\\"Forme des données de test: {X_test.shape}\\\")\\n\",\n",
    "        \"print(f\\\"Distribution des classes (entraînement): {pd.Series(y_train).value_counts().sort_index()}\\\")\\n\",\n",
    "        \"print(f\\\"Distribution des classes (test): {pd.Series(y_test).value_counts().sort_index()}\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 5. Création du modèle LSTM\\n\",\n",
    "        \"\\n\",\n",
    "        \"Nous allons maintenant créer notre modèle d'analyse de sentiment en utilisant une architecture LSTM bidirectionnelle.\\n\",\n",
    "        \"\\n\",\n",
    "        \"### Architecture du modèle\\n\",\n",
    "        \"- **Couche d'embedding**: Convertit les tokens en vecteurs denses\\n\",\n",
    "        \"- **Couches LSTM bidirectionnelles**: Capture les dépendances à long terme dans les deux directions\\n\",\n",
    "        \"- **Dropout**: Évite le surapprentissage\\n\",\n",
    "        \"- **Couche dense finale**: Classification en 3 catégories (négatif, neutre, positif)\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Paramètres du modèle\\n\",\n",
    "        \"embedding_dim = 32  # Dimension de l'espace d'embedding\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Création du modèle\\n\",\n",
    "        \"model = Sequential([\\n\",\n",
    "        \"    # Couche d'embedding pour convertir les tokens en vecteurs denses\\n\",\n",
    "        \"    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len),\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Couche LSTM bidirectionnelle\\n\",\n",
    "        \"    Bidirectional(LSTM(64, return_sequences=True)),\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Deuxième couche LSTM suivie de dropout pour régularisation\\n\",\n",
    "        \"    Bidirectional(LSTM(32)),\\n\",\n",
    "        \"    Dropout(0.5),\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Couche de classification (3 classes: négatif, neutre, positif)\\n\",\n",
    "        \"    Dense(3, activation='softmax')\\n\",\n",
    "        \"])\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Afficher un résumé du modèle\\n\",\n",
    "        \"model.summary()\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### 💡 Points clés à observer dans l'architecture\\n\",\n",
    "        \"\\n\",\n",
    "        \"- **LSTM bidirectionnel** : Lit le texte de gauche à droite ET de droite à gauche, capturant mieux le contexte\\n\",\n",
    "        \"- **return_sequences=True** : Permet d'empiler plusieurs couches LSTM\\n\",\n",
    "        \"- **Dropout** : Désactive aléatoirement 50% des neurones pendant l'entraînement pour éviter le surapprentissage\\n\",\n",
    "        \"- **Activation softmax** : Génère une distribution de probabilité sur les 3 classes\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 6. Compilation et entraînement du modèle\\n\",\n",
    "        \"\\n\",\n",
    "        \"Maintenant, compilons et entraînons notre modèle LSTM.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Compiler le modèle\\n\",\n",
    "        \"model.compile(\\n\",\n",
    "        \"    optimizer='adam',\\n\",\n",
    "        \"    loss='sparse_categorical_crossentropy',  # Pour les étiquettes sous forme d'entiers (non one-hot)\\n\",\n",
    "        \"    metrics=['accuracy']\\n\",\n",
    "        \")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Early stopping pour éviter le surapprentissage\\n\",\n",
    "        \"early_stopping = EarlyStopping(\\n\",\n",
    "        \"    monitor='val_loss',\\n\",\n",
    "        \"    patience=3,\\n\",\n",
    "        \"    restore_best_weights=True\\n\",\n",
    "        \")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Mesure du temps d'entraînement\\n\",\n",
    "        \"start_time = time.time()\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Entraînement du modèle\\n\",\n",
    "        \"history = model.fit(\\n\",\n",
    "        \"    X_train, \\n\",\n",
    "        \"    y_train, \\n\",\n",
    "        \"    epochs=20,\\n\",\n",
    "        \"    batch_size=4,  # Petit batch size en raison de la petite taille du dataset\\n\",\n",
    "        \"    validation_split=0.2,  # 20% des données d'entraînement serviront à la validation\\n\",\n",
    "        \"    callbacks=[early_stopping],\\n\",\n",
    "        \"    verbose=1\\n\",\n",
    "        \")\\n\",\n",
    "        \"\\n\",\n",
    "        \"training_time = time.time() - start_time\\n\",\n",
    "        \"print(f\\\"\\\\nTemps d'entraînement: {training_time:.2f} secondes\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### Visualisation de l'évolution de l'entraînement\\n\",\n",
    "        \"\\n\",\n",
    "        \"Observons comment la précision et la perte ont évolué au cours de l'entraînement.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Visualisation de l'entraînement\\n\",\n",
    "        \"plt.figure(figsize=(12, 5))\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Graphique de précision\\n\",\n",
    "        \"plt.subplot(1, 2, 1)\\n\",\n",
    "        \"plt.plot(history.history['accuracy'], label='Entraînement')\\n\",\n",
    "        \"plt.plot(history.history['val_accuracy'], label='Validation')\\n\",\n",
    "        \"plt.title('Évolution de la précision')\\n\",\n",
    "        \"plt.xlabel('Époque')\\n\",\n",
    "        \"plt.ylabel('Précision')\\n\",\n",
    "        \"plt.legend()\\n\",\n",
    "        \"plt.grid(True, linestyle='--', alpha=0.6)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Graphique de perte\\n\",\n",
    "        \"plt.subplot(1, 2, 2)\\n\",\n",
    "        \"plt.plot(history.history['loss'], label='Entraînement')\\n\",\n",
    "        \"plt.plot(history.history['val_loss'], label='Validation')\\n\",\n",
    "        \"plt.title('Évolution de la perte')\\n\",\n",
    "        \"plt.xlabel('Époque')\\n\",\n",
    "        \"plt.ylabel('Perte')\\n\",\n",
    "        \"plt.legend()\\n\",\n",
    "        \"plt.grid(True, linestyle='--', alpha=0.6)\\n\",\n",
    "        \"\\n\",\n",
    "        \"plt.tight_layout()\\n\",\n",
    "        \"plt.show()\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 7. Évaluation du modèle\\n\",\n",
    "        \"\\n\",\n",
    "        \"Maintenant, évaluons les performances de notre modèle sur l'ensemble de test.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Évaluation sur l'ensemble de test\\n\",\n",
    "        \"test_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)\\n\",\n",
    "        \"print(f\\\"Précision sur l'ensemble de test: {test_acc*100:.2f}%\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Générer les prédictions\\n\",\n",
    "        \"y_pred_proba = model.predict(X_test)\\n\",\n",
    "        \"y_pred_classes = np.argmax(y_pred_proba, axis=1)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Matrice de confusion\\n\",\n",
    "        \"conf_mat = confusion_matrix(y_test, y_pred_classes)\\n\",\n",
    "        \"plt.figure(figsize=(8, 6))\\n\",\n",
    "        \"sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', \\n\",\n",
    "        \"            xticklabels=['Négatif', 'Neutre', 'Positif'],\\n\",\n",
    "        \"            yticklabels=['Négatif', 'Neutre', 'Positif'])\\n\",\n",
    "        \"plt.xlabel('Prédit')\\n\",\n",
    "        \"plt.ylabel('Réel')\\n\",\n",
    "        \"plt.title('Matrice de confusion')\\n\",\n",
    "        \"plt.tight_layout()\\n\",\n",
    "        \"plt.show()\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Rapport de classification\\n\",\n",
    "        \"print(\\\"\\\\nRapport de classification détaillé:\\\")\\n\",\n",
    "        \"target_names = ['Négatif', 'Neutre', 'Positif']\\n\",\n",
    "        \"print(classification_report(y_test, y_pred_classes, target_names=target_names))\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### 🧠 Réflexions sur les résultats\\n\",\n",
    "        \"\\n\",\n",
    "        \"- **Analysez la matrice de confusion**: Quelles classes sont le mieux reconnues? Y a-t-il des confusions particulières?\\n\",\n",
    "        \"- **Précision vs Rappel**: Y a-t-il un déséquilibre? Quelle métrique privilégier selon le contexte?\\n\",\n",
    "        \"- **Taille du dataset**: Comment les résultats pourraient-ils être affectés par la petite taille de notre jeu de données?\\n\",\n",
    "        \"\\n\",\n",
    "        \"👉 **Discussion**: Notez vos observations ci-dessous:\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"*Écrivez vos observations ici...*\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"## 8. Test avec de nouveaux avis\\n\",\n",
    "        \"\\n\",\n",
    "        \"Testons maintenant notre modèle avec quelques nouveaux avis qui n'ont pas été utilisés pour l'entraînement.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Nouveaux avis à tester\\n\",\n",
    "        \"new_reviews = [\\n\",\n",
    "        \"    \\\"Ce film était vraiment fantastique, j'ai adoré chaque minute.\\\",\\n\",\n",
    "        \"    \\\"Je n'ai pas du tout aimé ce film, c'était une perte de temps complète.\\\",\\n\",\n",
    "        \"    \\\"C'était un film correct, ni bon ni mauvais.\\\"\\n\",\n",
    "        \"]\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Prétraitement des nouveaux avis\\n\",\n",
    "        \"processed_new_reviews = [preprocess_text(review) for review in new_reviews]\\n\",\n",
    "        \"sequences_new = tokenizer.texts_to_sequences(processed_new_reviews)\\n\",\n",
    "        \"padded_new = pad_sequences(sequences_new, maxlen=max_len, padding='post', truncating='post')\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Prédictions\\n\",\n",
    "        \"predictions = model.predict(padded_new)\\n\",\n",
    "        \"predicted_classes = np.argmax(predictions, axis=1)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Afficher les résultats\\n\",\n",
    "        \"sentiment_labels = {0: \\\"Négatif\\\", 1: \\\"Neutre\\\", 2: \\\"Positif\\\"}\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(\\\"Prédictions pour les nouveaux avis:\\\\n\\\")\\n\",\n",
    "        \"for i, review in enumerate(new_reviews):\\n\",\n",
    "        \"    pred_class = predicted_classes[i]\\n\",\n",
    "        \"    confidence = predictions[i][pred_class] * 100\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    print(f\\\"Avis: {review}\\\")\\n\",\n",
    "        \"    print(f\\\"Sentiment prédit: {sentiment_labels[pred_class]} (confiance: {confidence:.2f}%)\\\")\\n\",\n",
    "        \"    print(\\\"Probabilités pour chaque classe:\\\")\\n\",\n",
    "        \"    for j, label in sentiment_labels.items():\\n\",\n",
    "        \"        print(f\\\"  {label}: {predictions[i][j]*100:.2f}%\\\")\\n\",\n",
    "        \"    print()\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### Visualisation graphique des prédictions\\n\",\n",
    "        \"\\n\",\n",
    "        \"Visualisons les probabilités pour chaque classe pour les nouveaux avis.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Visualisation des probabilités pour chaque avis\\n\",\n",
    "        \"plt.figure(figsize=(15, 5))\\n\",\n",
    "        \"labels = ['Négatif', 'Neutre', 'Positif']\\n\",\n",
    "        \"\\n\",\n",
    "        \"for i, review in enumerate(new_reviews):\\n\",\n",
    "        \"    plt.subplot(1, 3, i+1)\\n\",\n",
    "        \"    plt.bar(labels, predictions[i], color=['red', 'gray', 'green'])\\n\",\n",
    "        \"    plt.title(f\\\"Avis {i+1}\\\")\\n\",\n",
    "        \"    plt.ylim(0, 1)\\n\",\n",
    "        \"    plt.ylabel('Probabilité')\\n\",\n",
    "        \"    plt.xticks(rotation=45)\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Ajouter les valeurs sur les barres\\n\",\n",
    "        \"    for j, p in enumerate(predictions[i]):\\n\",\n",
    "        \"        plt.text(j, p + 0.02, f\\\"{p*100:.1f}%\\\", ha='center')\\n\",\n",
    "        \"        \\n\",\n",
    "        \"plt.tight_layout()\\n\",\n",
    "        \"plt.show()\"\n",
    "      ]\n",
    "    },"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
