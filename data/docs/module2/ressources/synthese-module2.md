# SynthÃ¨se - Module 2

# Architectures spÃ©cialisÃ©es de rÃ©seaux de neurones
## Guide de rÃ©fÃ©rence synthÃ©tique

### ğŸ” Architectures spÃ©cialisÃ©es

- **ğŸ—ï¸ Au-delÃ  des rÃ©seaux de neurones simples**  
  Architectures conÃ§ues pour exploiter la structure spÃ©cifique des donnÃ©es (images, texte, sÃ©quences)

- **ğŸ“š Ã‰volution des architectures**  
  Des perceptrons aux modÃ¨les complexes actuels, chaque architecture rÃ©sout des problÃ¨mes spÃ©cifiques

- **ğŸ§© SpÃ©cialisation par type de donnÃ©es**  
  CNN pour les images, RNN pour les sÃ©quences, Transformers pour le texte

- **ğŸš€ Gains de performances considÃ©rables**  
  Les architectures spÃ©cialisÃ©es surpassent largement les modÃ¨les gÃ©nÃ©riques pour leurs tÃ¢ches ciblÃ©es

### ğŸ‘ï¸ RÃ©seaux de neurones convolutifs (CNN)

#### ğŸ§  Principes fondamentaux

- **ğŸ” Convolution**  
  Filtres (kernels) qui parcourent l'image pour dÃ©tecter des motifs locaux

- **ğŸŠ Pooling**  
  RÃ©duction de dimension qui prÃ©serve les informations importantes tout en diminuant la taille des donnÃ©es

- **ğŸ”„ HiÃ©rarchie des caractÃ©ristiques**  
  Extraction progressive de motifs de plus en plus abstraits (bords â†’ formes â†’ objets)

- **ğŸ”— Couches fully connected**  
  Couches finales qui combinent les caractÃ©ristiques extraites pour la classification

#### ğŸ›ï¸ Architecture typique d'un CNN

```
Input â†’ Conv â†’ ReLU â†’ Pool â†’ Conv â†’ ReLU â†’ Pool â†’ Flatten â†’ Dense â†’ Output
```

- **Couches de convolution:** extraction de caractÃ©ristiques
- **Fonctions d'activation (ReLU):** introduction de non-linÃ©aritÃ©
- **Couches de pooling:** rÃ©duction de dimension et invariance aux petites translations
- **Flatten:** transformation des matrices en vecteur
- **Couches denses:** classification finale

#### ğŸ’ª Forces du CNN

- **ğŸ”„ Partage des paramÃ¨tres**  
  Les mÃªmes filtres sont appliquÃ©s sur toute l'image, rÃ©duisant le nombre de paramÃ¨tres

- **ğŸ“ Invariance Ã  la translation**  
  CapacitÃ© Ã  reconnaÃ®tre les objets quelle que soit leur position dans l'image

- **ğŸ§  Extraction automatique des caractÃ©ristiques**  
  Pas besoin d'extraction manuelle des features comme en ML classique

- **ğŸŒ Robustesse aux variations**  
  Bonne gÃ©nÃ©ralisation face aux variations de luminositÃ©, angle, etc.

#### ğŸ“Š Applications principales

- **ğŸ–¼ï¸ Classification d'images**  
  Reconnaissance d'objets, de chiffres, de visages

- **ğŸ¯ DÃ©tection d'objets**  
  Localisation et identification d'objets multiples dans une image

- **ğŸ§© Segmentation**  
  SÃ©paration prÃ©cise des diffÃ©rents Ã©lÃ©ments d'une image

- **ğŸ‘ï¸ Vision par ordinateur**  
  Voitures autonomes, robotique, rÃ©alitÃ© augmentÃ©e

### ğŸ“ RÃ©seaux rÃ©currents (RNN)

#### ğŸ§  Principes fondamentaux

- **ğŸ”„ Boucles de rÃ©currence**  
  Connections qui permettent de transmettre l'information d'une Ã©tape Ã  la suivante

- **ğŸ“š Ã‰tat cachÃ© (hidden state)**  
  MÃ©moire interne qui conserve le contexte des Ã©lÃ©ments prÃ©cÃ©dents

- **â±ï¸ Traitement sÃ©quentiel**  
  Analyse des donnÃ©es une Ã©tape Ã  la fois, en tenant compte du contexte

- **ğŸ”— Partage des paramÃ¨tres dans le temps**  
  Les mÃªmes poids sont utilisÃ©s Ã  chaque Ã©tape, permettant de traiter des sÃ©quences de longueur variable

#### ğŸ”„ ProblÃ¨me du gradient qui s'Ã©vanouit

- **ğŸ“‰ DifficultÃ© Ã  capturer les dÃ©pendances Ã  long terme**  
  L'information se dilue progressivement lors de la backpropagation

- **ğŸ’¡ Solutions: LSTM et GRU**  
  Architectures qui permettent de mieux conserver l'information sur de longues sÃ©quences

#### ğŸ§© Long Short-Term Memory (LSTM)

- **ğŸšª SystÃ¨me de portes (gates)**  
  â€¢ Porte d'oubli (forget gate): dÃ©cide quelle information oublier  
  â€¢ Porte d'entrÃ©e (input gate): dÃ©cide quelle information nouvelle stocker  
  â€¢ Porte de sortie (output gate): dÃ©cide quelle information transmettre

- **ğŸ“‹ Cellule de mÃ©moire**  
  Permet de conserver l'information importante sur de longues sÃ©quences

- **ğŸ§® Flux d'information contrÃ´lÃ©**  
  MÃ©canismes sÃ©lectifs qui gÃ¨rent l'ajout et la suppression d'information

#### ğŸ“Š Applications principales

- **ğŸ“ Traitement du langage naturel**  
  Analyse de sentiment, traduction automatique, rÃ©sumÃ© de texte

- **â±ï¸ SÃ©ries temporelles**  
  PrÃ©diction de valeurs futures, dÃ©tection d'anomalies

- **ğŸµ Traitement audio**  
  Reconnaissance vocale, gÃ©nÃ©ration de musique

- **ğŸ“Š DonnÃ©es sÃ©quentielles**  
  Toute donnÃ©e oÃ¹ l'ordre importe (gÃ©nomique, logs, etc.)

### ğŸ“Š Comparaison des architectures

| CaractÃ©ristique | CNN | RNN/LSTM |
|-----------------|-----|----------|
| **Type de donnÃ©es idÃ©al** | Images, donnÃ©es en grille | SÃ©quences, texte, sÃ©rie temporelles |
| **Force principale** | DÃ©tection de patterns spatiaux | Capture des dÃ©pendances temporelles |
| **Structure de l'information** | HiÃ©rarchie spatiale | Flux sÃ©quentiel avec mÃ©moire |
| **ParallÃ©lisation** | Hautement parallÃ©lisable | Moins parallÃ©lisable (sÃ©quentiel) |
| **Taille de contexte** | LimitÃ©e par la taille des filtres | ThÃ©oriquement illimitÃ©e (LSTM) |
| **ParamÃ¨tres** | Relativement peu nombreux (partage) | Plus nombreux pour LSTM/GRU |
| **Applications types** | Vision par ordinateur | NLP, prÃ©diction de sÃ©ries |

### ğŸ’¡ Bonnes pratiques pour les architectures spÃ©cialisÃ©es

#### ğŸ› ï¸ Conception du CNN

- **ğŸ” Commencer simple**  
  DÃ©buter avec une architecture Ã©prouvÃ©e (ex: LeNet, mini-VGG)

- **ğŸ“Š Augmenter progressivement la profondeur**  
  Plus de filtres dans les couches profondes, moins dans les premiÃ¨res

- **ğŸ“‰ RÃ©duire graduellement la dimension spatiale**  
  Diminuer la hauteur/largeur tout en augmentant le nombre de filtres

- **ğŸ§ª Dropout entre les couches denses**  
  Ajouter du dropout aprÃ¨s la mise Ã  plat pour Ã©viter le surapprentissage

- **ğŸ”§ Batch normalization pour stabiliser**  
  Normaliser les activations pour accÃ©lÃ©rer l'entraÃ®nement

#### ğŸ”„ Optimisation des RNN/LSTM

- **ğŸ“š Attention Ã  la longueur des sÃ©quences**  
  Les sÃ©quences trop longues peuvent causer des problÃ¨mes de mÃ©moire et d'entraÃ®nement

- **ğŸ“Š BidirectionnalitÃ© pour plus de contexte**  
  Les LSTM bidirectionnels analysent la sÃ©quence dans les deux sens

- **ğŸ§ª Empilement de couches LSTM**  
  Plusieurs couches pour capturer diffÃ©rents niveaux d'abstraction

- **âš–ï¸ GRU vs LSTM**  
  GRU plus lÃ©ger et plus rapide, LSTM potentiellement plus puissant pour les trÃ¨s longues sÃ©quences

### ğŸ”„ Ã‰volution vers les Transformers

- **âš ï¸ Limitations des RNN/LSTM**  
  Traitement sÃ©quentiel, difficultÃ©s avec les trÃ¨s longues sÃ©quences

- **ğŸ§  MÃ©canisme d'attention**  
  Permet de se concentrer sur les parties pertinentes de la sÃ©quence

- **ğŸš€ Architecture Transformer**  
  Traitement parallÃ¨le, meilleure capture des dÃ©pendances Ã  long terme

- **ğŸ“š ModÃ¨les fondÃ©s sur les Transformers**  
  BERT, GPT, T5 qui rÃ©volutionnent le NLP et au-delÃ 

### ğŸ› ï¸ Conseils pratiques d'implÃ©mentation

- **ğŸ“Š Gestion des donnÃ©es**  
  â€¢ CNN: redimensionnement, normalisation, augmentation de donnÃ©es  
  â€¢ RNN: padding, troncation, tokenisation, embeddings

- **ğŸ§ª Visualisation pour comprendre**  
  â€¢ CNN: visualiser filtres et feature maps  
  â€¢ RNN: analyser les Ã©tats cachÃ©s et l'Ã©volution des embeddings

- **âš™ï¸ HyperparamÃ¨tres clÃ©s**  
  â€¢ CNN: taille et nombre de filtres, pas de convolution, type de pooling  
  â€¢ RNN: taille des Ã©tats cachÃ©s, nombre de couches, dropout

- **ğŸ“ˆ Transfer learning**  
  RÃ©utiliser des modÃ¨les prÃ©-entraÃ®nÃ©s (VGG, ResNet, etc.) pour gagner en temps et performance
