# üìù QCM d'auto-√©valuation - Module 2 : Architectures sp√©cialis√©es

Ce QCM vous permettra d'√©valuer votre compr√©hension des r√©seaux convolutifs (CNN) et r√©currents (RNN) √©tudi√©s dans ce module.

## ‚úÖ Instructions
- Cochez la ou les r√©ponses correctes pour chaque question
- Certaines questions peuvent avoir plusieurs r√©ponses correctes
- Pour les questions √† choix multiples, 0,5 point est attribu√© par r√©ponse correcte (maximum 1 point par question)
- √Ä la fin du questionnaire, calculez votre score gr√¢ce au corrig√© fourni
- Dur√©e recommand√©e : 15 minutes

## üîç Partie A : R√©seaux Convolutifs (CNN)

### 1. Dans un r√©seau convolutif, √† quoi sert principalement l'op√©ration de convolution ?
- [ ] √Ä r√©duire la dimension des donn√©es
- [ ] √Ä d√©tecter des caract√©ristiques locales dans les donn√©es d'entr√©e
- [ ] √Ä connecter tous les neurones entre eux
- [ ] √Ä acc√©l√©rer le temps d'entra√Ænement

### 2. Qu'est-ce qu'un filtre (ou noyau) dans un CNN ?
- [ ] Une fonction qui supprime les pixels ind√©sirables de l'image
- [ ] Une matrice de poids qui s'applique localement sur les donn√©es d'entr√©e
- [ ] Un seuil qui √©limine les valeurs en dessous d'un certain niveau
- [ ] Une technique pour s√©lectionner les meilleures images d'entra√Ænement

### 3. Quel est le r√¥le principal de l'op√©ration de pooling dans un CNN ?
- [ ] Augmenter la taille des feature maps
- [ ] R√©duire la dimensionnalit√© tout en pr√©servant les informations importantes
- [ ] Ajouter de la non-lin√©arit√© au r√©seau
- [ ] Connecter les diff√©rentes couches de convolution

### 4. Quels sont les avantages des CNN pour le traitement d'images ? (plusieurs r√©ponses possibles)
- [ ] Partage des param√®tres entre diff√©rentes positions spatiales
- [ ] Invariance √† la translation
- [ ] R√©duction significative du nombre de param√®tres par rapport aux r√©seaux enti√®rement connect√©s
- [ ] Capacit√© √† traiter des images de n'importe quelle taille sans redimensionnement

### 5. Dans quelle couche d'un CNN typique se trouvent g√©n√©ralement le plus grand nombre de param√®tres ?
- [ ] Couches de convolution
- [ ] Couches de pooling
- [ ] Couches enti√®rement connect√©es (fully connected)
- [ ] Couches de normalisation par lots (batch normalization)

### 6. Qu'est-ce qu'une feature map dans un CNN ?
- [ ] Une carte qui indique les r√©gions d'int√©r√™t dans l'image originale
- [ ] Le r√©sultat de l'application d'un filtre de convolution sur une entr√©e
- [ ] Un graphique montrant la progression de l'entra√Ænement
- [ ] La liste des caract√©ristiques extraites manuellement avant l'entra√Ænement

### 7. Comment √©voluent les caract√©ristiques d√©tect√©es √† mesure qu'on avance dans les couches d'un CNN ?
- [ ] Elles deviennent de plus en plus simples et √©l√©mentaires
- [ ] Elles restent de m√™me nature mais deviennent plus pr√©cises
- [ ] Elles deviennent de plus en plus abstraites et complexes
- [ ] Elles concernent des r√©gions de plus en plus petites de l'image

## üß© Partie B : R√©seaux R√©currents (RNN)

### 8. Quelle est la principale caract√©ristique des r√©seaux de neurones r√©currents (RNN) ?
- [ ] Ils utilisent des op√©rations de convolution pour traiter les donn√©es
- [ ] Ils contiennent des connexions formant des boucles permettant de m√©moriser les informations
- [ ] Ils traitent chaque √©l√©ment d'une s√©quence de mani√®re compl√®tement ind√©pendante
- [ ] Ils sont sp√©cialis√©s dans le traitement d'images

### 9. Pour quels types de donn√©es les RNN sont-ils particuli√®rement adapt√©s ?
- [ ] Images 2D
- [ ] Donn√©es tabulaires (comme des tableaux Excel)
- [ ] Donn√©es s√©quentielles (texte, s√©ries temporelles, audio)
- [ ] Nuages de points 3D

### 10. Quel probl√®me majeur affecte les RNN classiques lors du traitement de s√©quences longues ?
- [ ] Surconsommation de m√©moire
- [ ] Temps de traitement exponentiel
- [ ] Probl√®me de disparition ou d'explosion du gradient
- [ ] Incapacit√© √† parall√©liser les calculs

### 11. Quelle est la principale innovation des cellules LSTM par rapport aux RNN classiques ?
- [ ] Elles utilisent des op√©rations de convolution
- [ ] Elles poss√®dent des m√©canismes de portes contr√¥lant le flux d'information
- [ ] Elles peuvent traiter plusieurs s√©quences en parall√®le
- [ ] Elles ne n√©cessitent pas d'entra√Ænement

### 12. Dans un r√©seau LSTM, √† quoi sert la "porte d'oubli" (forget gate) ?
- [ ] √Ä d√©terminer quelles informations de l'√©tat pr√©c√©dent doivent √™tre conserv√©es ou supprim√©es
- [ ] √Ä r√©initialiser compl√®tement le r√©seau quand la s√©quence est trop longue
- [ ] √Ä sauter certaines √©tapes de calcul pour acc√©l√©rer le traitement
- [ ] √Ä ignorer les donn√©es d'entr√©e corrompues ou bruit√©es

### 13. Quelles applications typiques utilisent les RNN/LSTM ? (plusieurs r√©ponses possibles)
- [ ] Reconnaissance de caract√®res manuscrits
- [ ] Traduction automatique
- [ ] Pr√©diction de s√©ries temporelles
- [ ] G√©n√©ration de texte
- [ ] Segmentation d'images

### 14. Qu'est-ce qui diff√©rencie principalement les GRU (Gated Recurrent Units) des LSTM ?
- [ ] Les GRU n'ont aucune forme de m√©moire
- [ ] Les GRU ont une architecture plus simple avec moins de portes
- [ ] Les GRU sont sp√©cifiquement con√ßus pour les donn√©es non s√©quentielles
- [ ] Les GRU ne peuvent pas √™tre entra√Æn√©s par r√©tropropagation

## üìä Partie C : Comparaison et applications

### 15. Dans quel contexte choisiriez-vous un CNN plut√¥t qu'un RNN ?
- [ ] Pour l'analyse de sentiment dans des avis clients
- [ ] Pour la pr√©diction de cours boursiers
- [ ] Pour la d√©tection de visages dans des photos
- [ ] Pour la traduction automatique de texte

### 16. Quelles sont les principales diff√©rences entre CNN et RNN ? (plusieurs r√©ponses possibles)
- [ ] Les CNN traitent mieux les relations spatiales, les RNN les relations temporelles
- [ ] Les CNN ont g√©n√©ralement plus de param√®tres que les RNN
- [ ] Les CNN sont plus faciles √† parall√©liser que les RNN
- [ ] Les CNN utilisent le concept d'√©tat cach√©, contrairement aux RNN

### 17. Quelle architecture serait la plus adapt√©e pour l'analyse de logs syst√®me √† des fins de s√©curit√© ?
- [ ] Un CNN simple
- [ ] Un LSTM ou un GRU
- [ ] Un r√©seau dens√©ment connect√© (MLP)
- [ ] Un r√©seau de neurones √† convolution 1D

### 18. Quel mod√®le a-t-il tendance √† √™tre le plus efficace en termes de temps d'entra√Ænement ?
- [ ] Les CNN sont g√©n√©ralement plus rapides √† entra√Æner que les RNN
- [ ] Les RNN sont g√©n√©ralement plus rapides √† entra√Æner que les CNN
- [ ] Les LSTM sont plus rapides √† entra√Æner que les GRU
- [ ] Le temps d'entra√Ænement est similaire pour toutes ces architectures

## Auto-√©valuation

Une fois le QCM compl√©t√©, v√©rifiez vos r√©ponses avec le corrig√© ci-dessous et calculez votre score.

### Corrig√© avec explications

1. **b - √Ä d√©tecter des caract√©ristiques locales dans les donn√©es d'entr√©e**  
   *L'op√©ration de convolution permet de d√©tecter des motifs locaux dans l'image comme des bords, des textures ou des formes, gr√¢ce √† des filtres qui balaient l'image.*

2. **b - Une matrice de poids qui s'applique localement sur les donn√©es d'entr√©e**  
   *Un filtre (ou noyau) est une petite matrice de poids qui est d√©plac√©e sur l'image pour d√©tecter des motifs sp√©cifiques par le calcul du produit de convolution.*

3. **b - R√©duire la dimensionnalit√© tout en pr√©servant les informations importantes**  
   *Le pooling r√©duit la taille des feature maps en conservant les informations les plus pertinentes, ce qui diminue le temps de calcul et rend le mod√®le plus robuste aux variations.*

4. **a, b, c - Partage des param√®tres entre diff√©rentes positions spatiales, Invariance √† la translation, R√©duction significative du nombre de param√®tres**  
   *Les CNN partagent les m√™mes poids sur toute l'image (a), peuvent reconna√Ætre des objets quelle que soit leur position (b) et utilisent beaucoup moins de param√®tres que les r√©seaux enti√®rement connect√©s (c). Ils n√©cessitent cependant que les images soient redimensionn√©es √† une taille fixe (d est incorrect).*

5. **c - Couches enti√®rement connect√©es (fully connected)**  
   *Dans un CNN typique, ce sont les couches enti√®rement connect√©es qui contiennent le plus grand nombre de param√®tres, car chaque neurone est connect√© √† tous les neurones de la couche pr√©c√©dente.*

6. **b - Le r√©sultat de l'application d'un filtre de convolution sur une entr√©e**  
   *Une feature map est la sortie obtenue apr√®s l'application d'un filtre de convolution sur les donn√©es d'entr√©e, repr√©sentant l'activation du filtre √† chaque position.*

7. **c - Elles deviennent de plus en plus abstraites et complexes**  
   *Dans les premi√®res couches, les CNN d√©tectent des √©l√©ments simples comme des bords et des contours. Plus on avance dans le r√©seau, plus les caract√©ristiques d√©tect√©es deviennent abstraites et complexes (formes, parties d'objets, objets entiers).*

8. **b - Ils contiennent des connexions formant des boucles permettant de m√©moriser les informations**  
   *La particularit√© des RNN est leur structure avec des connexions cycliques, ce qui permet √† l'information de persister d'une √©tape de traitement √† l'autre.*

9. **c - Donn√©es s√©quentielles (texte, s√©ries temporelles, audio)**  
   *Les RNN sont particuli√®rement adapt√©s aux donn√©es o√π l'ordre et le contexte sont importants, comme le texte, l'audio ou les s√©ries temporelles.*

10. **c - Probl√®me de disparition ou d'explosion du gradient**  
    *Lors de l'entra√Ænement sur des s√©quences longues, les RNN souffrent du probl√®me du gradient qui s'√©vanouit (ou explose), ce qui les emp√™che d'apprendre les d√©pendances √† long terme.*

11. **b - Elles poss√®dent des m√©canismes de portes contr√¥lant le flux d'information**  
    *Les LSTM introduisent un syst√®me de portes (forget, input, output) qui contr√¥le quelles informations sont conserv√©es, mises √† jour ou transmises, r√©solvant ainsi le probl√®me du gradient qui s'√©vanouit.*

12. **a - √Ä d√©terminer quelles informations de l'√©tat pr√©c√©dent doivent √™tre conserv√©es ou supprim√©es**  
    *La porte d'oubli (forget gate) d√©cide quelles informations de l'√©tat pr√©c√©dent sont pertinentes et doivent √™tre conserv√©es, et lesquelles peuvent √™tre √©cart√©es.*

13. **b, c, d - Traduction automatique, Pr√©diction de s√©ries temporelles, G√©n√©ration de texte**  
    *Les RNN/LSTM excellent dans les t√¢ches s√©quentielles comme la traduction, la pr√©diction de s√©ries temporelles et la g√©n√©ration de texte. La reconnaissance de caract√®res et la segmentation d'images sont plut√¥t des domaines de pr√©dilection des CNN.*

14. **b - Les GRU ont une architecture plus simple avec moins de portes**  
    *Les GRU sont une version simplifi√©e des LSTM avec seulement deux portes au lieu de trois, ce qui les rend g√©n√©ralement plus rapides √† entra√Æner mais potentiellement moins puissants sur certaines t√¢ches.*

15. **c - Pour la d√©tection de visages dans des photos**  
    *La d√©tection d'objets dans des images est une t√¢che id√©ale pour un CNN. Les autres options concernent des donn√©es s√©quentielles, mieux trait√©es par les RNN.*

16. **a, c - Les CNN traitent mieux les relations spatiales, les RNN les relations temporelles; Les CNN sont plus faciles √† parall√©liser que les RNN**  
    *Les CNN sont sp√©cialis√©s dans les motifs spatiaux, les RNN dans les s√©quences temporelles (a). Les CNN peuvent traiter une image enti√®re en parall√®le, tandis que les RNN sont intrins√®quement s√©quentiels (c). Les RNN n'ont g√©n√©ralement pas plus de param√®tres que les CNN (b est incorrect). Ce sont les RNN qui utilisent un √©tat cach√©, pas les CNN (d est incorrect).*

17. **b - Un LSTM ou un GRU**  
    *L'analyse de logs syst√®me implique des s√©quences d'√©v√©nements o√π l'ordre et le contexte temporel sont cruciaux, ce qui correspond parfaitement aux capacit√©s des LSTM/GRU.*

18. **a - Les CNN sont g√©n√©ralement plus rapides √† entra√Æner que les RNN**  
    *Les CNN peuvent √™tre hautement parall√©lis√©s, contrairement aux RNN qui sont s√©quentiels par nature, ce qui rend g√©n√©ralement l'entra√Ænement des CNN plus rapide pour des volumes de donn√©es comparables.*

### Calcul de votre score
- Questions √† choix unique (1-3, 5-12, 14-15, 17-18) : 1 point par r√©ponse correcte
- Questions √† choix multiples (4, 13, 16) : 0,5 point par r√©ponse correcte et -0,25 par r√©ponse incorrecte (minimum 0, maximum 1 point par question)

**Total des points possibles : 18**

### Interpr√©tation
- **14-18 points** : Excellente ma√Ætrise des architectures sp√©cialis√©es de Deep Learning
- **10-13 points** : Bonne compr√©hension, quelques points √† clarifier
- **6-9 points** : Compr√©hension de base, r√©vision n√©cessaire de certains concepts
- **0-5 points** : R√©vision approfondie recommand√©e avant de poursuivre

## Pour approfondir
Si vous avez obtenu moins de 14 points, nous vous recommandons de revoir les concepts sur lesquels vous avez fait des erreurs. Consultez les ressources suivantes :

  - Les notebooks [CNN pour classification](ressources/cnn-classification.md) et [RNN pour analyse de sentiment](ressources/rnn-sequence.md)
  - Les sections explicatives sur les [r√©seaux convolutifs](reseaux-convolutifs.md) et les [r√©seaux r√©currents](reseaux-recurrents.md)
  - La [synth√®se des architectures sp√©cialis√©es](ressources/synthese-module2.md)
  - Le [glossaire des termes du Deep Learning](../module1/ressources/glossaire-dl.md)

[Retour au Module 2](index.md){ .md-button }
[Continuer vers le Module 3](../module3/index.md){ .md-button .md-button--primary }