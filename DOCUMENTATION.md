# üìö Documentation - Chatbot IA avec Base Documentaire

> **Projet BTS SIO SLAM** - Assistant intelligent utilisant RAG (Retrieval-Augmented Generation)

## üìã Table des mati√®res

- [Pr√©sentation du projet](#-pr√©sentation-du-projet)
- [Architecture technique](#-architecture-technique)
- [Installation et configuration](#-installation-et-configuration)
- [Guide d'utilisation](#-guide-dutilisation)
- [API et fonctionnalit√©s](#-api-et-fonctionnalit√©s)
- [D√©veloppement](#-d√©veloppement)
- [D√©ploiement](#-d√©ploiement)
- [D√©pannage](#-d√©pannage)

---

## üéØ Pr√©sentation du projet

### Objectif
D√©velopper un chatbot intelligent capable d'analyser et de r√©pondre aux questions bas√©es sur une base documentaire personnalis√©e, en utilisant les technologies d'IA modernes.

### Fonctionnalit√©s principales
- **üìñ Analyse de documents** : Support multiple formats (TXT, MD, PDF, code source)
- **ü§ñ IA conversationnelle** : R√©ponses contextuelles avec historique
- **üîç Recherche vectorielle** : Indexation et recherche s√©mantique
- **üìä Interface moderne** : Dashboard Streamlit intuitif
- **üîß Multi-providers** : Support OpenAI et Mistral AI

### Technologies utilis√©es
- **Backend** : Python 3.11+, LangChain, FAISS
- **Frontend** : Streamlit
- **IA/ML** : OpenAI GPT, Mistral AI, embeddings vectoriels
- **Traitement documents** : PyMuPDF, python-docx
- **D√©ploiement** : GitHub Codespaces, Docker

---

## üèóÔ∏è Architecture technique

### Structure du projet
```
Projet_IA_BTS-SIO/
‚îú‚îÄ‚îÄ üìÅ .devcontainer/           # Configuration GitHub Codespaces
‚îÇ   ‚îî‚îÄ‚îÄ devcontainer.json
‚îú‚îÄ‚îÄ üìÅ app/                     # Application principale
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ config.py              # Configuration et variables
‚îÇ   ‚îú‚îÄ‚îÄ ingest.py              # Ingestion des documents
‚îÇ   ‚îú‚îÄ‚îÄ qa_chain.py            # Cha√Æne QA et logique IA
‚îÇ   ‚îî‚îÄ‚îÄ main.py                # Interface Streamlit
‚îú‚îÄ‚îÄ üìÅ data/                   # Documents sources (auto-g√©n√©r√©)
‚îú‚îÄ‚îÄ üìÅ vectorstore/            # Base vectorielle FAISS (auto-g√©n√©r√©)
‚îú‚îÄ‚îÄ üìÑ requirements.txt        # D√©pendances Python
‚îú‚îÄ‚îÄ üìÑ .env                    # Variables d'environnement
‚îú‚îÄ‚îÄ üìÑ .gitignore
‚îî‚îÄ‚îÄ üìÑ README.md
```

### Flux de donn√©es
```mermaid
graph TD
    A[Documents sources] --> B[Ingestion]
    B --> C[D√©coupage en chunks]
    C --> D[Cr√©ation embeddings]
    D --> E[Stockage FAISS]
    
    F[Question utilisateur] --> G[Recherche vectorielle]
    E --> G
    G --> H[R√©cup√©ration contexte]
    H --> I[LLM G√©n√©ration]
    I --> J[R√©ponse + Sources]
```

### Composants principaux

#### 1. **DocumentIngestor** (`ingest.py`)
- Chargement multi-formats
- D√©coupage intelligent des documents
- Cr√©ation d'embeddings vectoriels
- Stockage dans FAISS

#### 2. **QAChain** (`qa_chain.py`)
- Interface avec les LLMs (OpenAI/Mistral)
- Gestion de l'historique conversationnel
- Recherche et r√©cup√©ration de contexte
- Scoring de confiance

#### 3. **Interface Streamlit** (`main.py`)
- Dashboard principal
- Upload et gestion de fichiers
- Chat interactif
- Visualisation des r√©sultats

---

## ‚öôÔ∏è Installation et configuration

### Pr√©requis syst√®me
- **Python** : 3.11 ou sup√©rieur
- **Git** : Pour le clonage de repositories
- **Cl√© API** : OpenAI ou Mistral AI

### Installation locale

#### 1. Cloner le projet
```bash
git clone https://github.com/votre-username/Projet_IA_BTS-SIO.git
cd Projet_IA_BTS-SIO
```

#### 2. Cr√©er un environnement virtuel
```bash
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/Mac
source venv/bin/activate
```

#### 3. Installer les d√©pendances
```bash
pip install -r requirements.txt
```

#### 4. Configuration des variables d'environnement
```bash
cp .env.example .env
# √âditer le fichier .env avec vos cl√©s API
```

### Configuration avec GitHub Codespaces

1. **Fork** le repository
2. Cliquer sur **Code > Codespaces > Create codespace**
3. Attendre l'initialisation automatique
4. Configurer le fichier `.env`
5. Lancer l'application

### Variables d'environnement

#### Configuration obligatoire
```bash
# Provider principal (openai ou mistral)
PROVIDER=mistral
MISTRAL_API_KEY=votre_cle_mistral_ici

# Pour les embeddings (si pas d'OpenAI, utiliser embeddings gratuits)
OPENAI_API_KEY=votre_cle_openai_ici
```

#### Configuration avanc√©e
```bash
# Mod√®les
MODEL_NAME=mistral-small              # ou gpt-3.5-turbo
EMBEDDING_MODEL=text-embedding-ada-002

# Traitement des documents
CHUNK_SIZE=1000                       # Taille des segments
CHUNK_OVERLAP=200                     # Chevauchement

# Recherche
MAX_RESULTS=4                         # Nombre de r√©sultats max
SIMILARITY_THRESHOLD=0.7              # Seuil de similarit√©

# Repository par d√©faut (optionnel)
REPO_URL=https://github.com/user/docs.git
```

---

## üìñ Guide d'utilisation

### Premi√®re utilisation

#### 1. Lancer l'application
```bash
streamlit run app/main.py
```
L'interface s'ouvre automatiquement sur `http://localhost:8501`

#### 2. Indexer des documents

**Option A : Repository Git**
- Sidebar > Onglet "Repository"
- Entrer l'URL du repository
- Cliquer "Cloner et indexer"

**Option B : Upload de fichiers**
- Sidebar > Onglet "Upload"
- S√©lectionner les fichiers
- Cliquer "Uploader et indexer"

#### 3. Commencer √† converser
- Utiliser la zone de chat principale
- Poser des questions sur les documents
- Explorer les sources des r√©ponses

### Interface utilisateur

#### Sidebar (panneau lat√©ral)
- **‚öôÔ∏è Configuration** : Informations sur le provider et mod√®le
- **üìÅ Gestion des documents** : Upload et indexation
- **üìä Statistiques** : M√©triques du vectorstore

#### Zone principale
- **üí¨ Conversation** : Chat interactif avec l'IA
- **üîç Outils d'analyse** : Recherche et exploration

#### Fonctionnalit√©s avanc√©es
- **Historique contextuel** : Maintien du contexte sur plusieurs √©changes
- **Sources tra√ßables** : R√©f√©rences exactes aux documents
- **Export conversations** : Sauvegarde en Markdown
- **Questions sugg√©r√©es** : Exemples pour commencer

### Formats de documents support√©s

| Format | Extension | Description |
|--------|-----------|-------------|
| **Texte** | `.txt`, `.md` | Fichiers texte et Markdown |
| **Code** | `.py`, `.js`, `.html`, `.css` | Code source |
| **Donn√©es** | `.json`, `.xml` | Fichiers structur√©s |
| **PDF** | `.pdf` | Documents PDF (version avanc√©e) |

---

## üîß API et fonctionnalit√©s

### Classes principales

#### `DocumentIngestor`
```python
from app.ingest import SimpleDocumentIngestor

# Initialisation
ingestor = SimpleDocumentIngestor()

# M√©thodes principales
ingestor.clone_repository(repo_url)           # Cloner un repo
ingestor.load_documents_from_directory()      # Charger documents locaux
ingestor.upload_documents(uploaded_files)     # Traiter uploads
ingestor.create_vectorstore(documents)        # Cr√©er l'index
```

#### `QAChain`
```python
from app.qa_chain import SimpleQAChain

# Initialisation
qa = SimpleQAChain()

# M√©thodes principales
qa.load_vectorstore()                         # Charger l'index
qa.ask_question(question, use_history=True)   # Poser une question
qa.search_similar_documents(query, k=5)      # Recherche libre
qa.export_conversation()                     # Exporter l'historique
```

### Configuration des mod√®les

#### OpenAI
```python
# Dans .env
PROVIDER=openai
OPENAI_API_KEY=sk-...
MODEL_NAME=gpt-3.5-turbo
```

#### Mistral AI
```python
# Dans .env
PROVIDER=mistral
MISTRAL_API_KEY=...
MODEL_NAME=mistral-small
```

### Personnalisation des prompts

```python
def create_custom_prompt(self):
    template = """Vous √™tes un assistant IA sp√©cialis√©...
    
    Contexte: {context}
    Question: {question}
    
    Instructions:
    - R√©pondez pr√©cis√©ment
    - Citez vos sources
    
    R√©ponse:"""
    
    return PromptTemplate(template=template, input_variables=["context", "question"])
```

---

## üë®‚Äçüíª D√©veloppement

### Structure de d√©veloppement

#### Installation pour d√©veloppeurs
```bash
# D√©pendances compl√®tes
pip install -r requirements.txt

# Outils de d√©veloppement
pip install pytest black flake8

# Tests
pytest tests/

# Formatage
black app/

# Linting
flake8 app/
```

#### Ajout de nouvelles fonctionnalit√©s

**1. Nouveaux formats de documents**
```python
# Dans DocumentIngestor
def load_new_format(self, file_path):
    # Impl√©menter le loader
    loader = NewFormatLoader(file_path)
    return loader.load()
```

**2. Nouveaux providers LLM**
```python
# Dans config.py
def get_model_config():
    if PROVIDER == "nouveau_provider":
        return {"api_key": NEW_API_KEY, "model": NEW_MODEL}
```

**3. Interface personnalis√©e**
```python
# Dans main.py
def custom_component():
    st.custom_widget("Mon composant")
```

### Variables de session Streamlit

| Variable | Usage | Type |
|----------|--------|------|
| `st.session_state.qa_chain` | Instance QA principale | `SimpleQAChain` |
| `st.session_state.messages` | Historique chat | `List[Dict]` |
| `st.session_state.example_question` | Question s√©lectionn√©e | `str` |

### Gestion des erreurs

```python
try:
    result = qa_chain.ask_question(question)
except Exception as e:
    st.error(f"Erreur: {e}")
    # Fallback ou message d'aide
```

---

## üöÄ D√©ploiement

### D√©ploiement local
```bash
# Production locale
streamlit run app/main.py --server.port 8501
```

### D√©ploiement Streamlit Cloud
1. Push sur GitHub
2. Connecter √† Streamlit Cloud
3. Configurer les secrets (variables .env)
4. D√©ployer automatiquement

### D√©ploiement Docker
```dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
EXPOSE 8501

CMD ["streamlit", "run", "app/main.py"]
```

### Variables d'environnement en production
```bash
# Exemple pour Streamlit Cloud
PROVIDER=mistral
MISTRAL_API_KEY=xxxx
OPENAI_API_KEY=xxxx
```

---

## üîç D√©pannage

### Probl√®mes courants

#### 1. Erreur de cl√© API
```
Error: Incorrect API key provided
```
**Solution** : V√©rifier la cl√© dans le fichier `.env`

#### 2. Documents non trouv√©s
```
Aucun document index√©
```
**Solutions** :
- R√©indexer les documents
- V√©rifier les formats support√©s
- V√©rifier les permissions de fichiers

#### 3. R√©ponses de faible qualit√©
**Solutions** :
- Ajuster `CHUNK_SIZE` (essayer 500-2000)
- Modifier `SIMILARITY_THRESHOLD` (0.5-0.8)
- Am√©liorer la qualit√© des documents sources

#### 4. Erreurs d'import
```
ImportError: cannot import name 'X'
```
**Solutions** :
- V√©rifier l'installation : `pip install -r requirements.txt`
- Mettre √† jour les d√©pendances
- V√©rifier la version de Python (3.11+)

#### 5. Probl√®mes de m√©moire
**Solutions** :
- R√©duire `CHUNK_SIZE`
- Limiter `MAX_RESULTS`
- Nettoyer le vectorstore r√©guli√®rement

### Logs et debugging

#### Activer les logs d√©taill√©s
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

#### Streamlit debugging
```bash
streamlit run app/main.py --logger.level debug
```

### Performance et optimisation

#### M√©triques √† surveiller
- **Temps de r√©ponse** : < 3 secondes id√©alement
- **Utilisation m√©moire** : Monitor avec `htop`
- **Qualit√© des r√©ponses** : Score de confiance > 0.7

#### Optimisations
```python
# Cache pour les embeddings
@st.cache_data
def create_embeddings(texts):
    return embeddings.embed_documents(texts)

# Pagination des r√©sultats
def paginate_results(results, page_size=10):
    return results[start:start+page_size]
```

---

## üìö Ressources et r√©f√©rences

### Documentation officielle
- [LangChain](https://python.langchain.com/)
- [Streamlit](https://docs.streamlit.io/)
- [FAISS](https://faiss.ai/)
- [OpenAI API](https://platform.openai.com/docs)
- [Mistral AI](https://docs.mistral.ai/)

### Tutoriels et guides
- [RAG avec LangChain](https://python.langchain.com/docs/use_cases/question_answering)
- [Streamlit pour l'IA](https://docs.streamlit.io/knowledge-base/tutorials)
- [Vector databases](https://www.pinecone.io/learn/vector-database/)

### Communaut√© et support
- **GitHub Issues** : Rapporter les bugs
- **Discussions** : Poser des questions
- **Stack Overflow** : Tag `langchain`, `streamlit`

---

## üìù Licence et contribution

### Licence
Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour les d√©tails.

### Contribution
1. **Fork** le repository
2. Cr√©er une **branche** pour votre fonctionnalit√©
3. **Commit** vos changements
4. **Push** vers la branche
5. Ouvrir une **Pull Request**

### Code de conduite
- Code propre et document√©
- Tests unitaires pour les nouvelles fonctionnalit√©s
- Respect des conventions Python (PEP 8)

---

## üë• √âquipe et contact

**D√©velopp√© par** : G.G  


---

*Documentation mise √† jour le : {{ date actuelle }}*